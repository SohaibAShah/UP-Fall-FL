{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c3a569",
   "metadata": {},
   "source": [
    "Let's convert the entire script to use Flower, a popular framework for federated learning. Flower will handle all the complex server-client communication, so our code will become much simpler and more organized.\n",
    "\n",
    "We'll follow the same step-by-step process, keeping your original model, data loading functions, and variable names so you can easily see what's changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221b1e3",
   "metadata": {},
   "source": [
    "## Step 1: The Foundation (Imports and Setup)\n",
    "Every Python script starts with importing the necessary libraries and setting up the environment. This is like laying the foundation for a house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "696a41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import OrderedDict\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from flwr.common import Context # <-- ADD THIS LINE\n",
    "\n",
    "\n",
    "# --- Set Device ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b56c2",
   "metadata": {},
   "source": [
    "### Define dataset loader\n",
    "\n",
    "PyTorch uses a Dataset object to handle data loading. Since our model will take three different kinds of input (sensor data, image 1, and image 2), we need to create a special class that tells PyTorch how to retrieve one sample of each, along with its corresponding label.\n",
    "\n",
    "This class will have three essential methods:\n",
    "\n",
    "__init__: Initializes the dataset by storing our feature and label arrays.\n",
    "\n",
    "__len__: Returns the total number of samples in the dataset.\n",
    "\n",
    "__getitem__: Fetches a single data sample at a given index.\n",
    "\n",
    "Here is the code for it. Add this to your script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ebd8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset loader\n",
    "class CustomDatasetRes(Dataset):\n",
    "    def __init__(self, features1, features2, features3, labels):\n",
    "        self.features1 = features1\n",
    "        self.features2 = features2\n",
    "        self.features3 = features3\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.features1[index], self.features2[index], self.features3[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03702557",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "Next, we'll add a few helper functions. These functions will perform common tasks that we'll need later, like displaying results, scaling data, and ensuring our experiments are reproducible.\n",
    "\n",
    "1. display_result\n",
    "\n",
    "This function takes the true labels (y_test) and the model's predicted labels (y_pred) and prints out standard performance metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e6414e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result(y_test, y_pred):\n",
    "    print('Accuracy score : ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision score : ', precision_score(y_test, y_pred, average='weighted'))\n",
    "    print('Recall score : ', recall_score(y_test, y_pred, average='weighted'))\n",
    "    print('F1 score : ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a5531",
   "metadata": {},
   "source": [
    "2. scaled_data\n",
    "\n",
    "This function uses Scikit-learn's StandardScaler to normalize the sensor (CSV) data. Scaling is crucial because it ensures that features with larger value ranges don't dominate the learning process. Notice there are two functions with the same name in the original code. In Python, the last definition of a function is the one that gets used. We will add both for completeness, but just know that the first one is effectively overwritten by the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adc0678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def scaled_data(X_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    return X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d51f72",
   "metadata": {},
   "source": [
    "3. set_seed\n",
    "\n",
    "This is a very important function for reproducibility. Machine learning involves a lot of randomness (e.g., initializing model weights, shuffling data). By setting a \"seed,\" we ensure that the sequence of random numbers is the same every time we run the code, which means we'll get the exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93f4630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    # Sets the environment variable for Python's hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # Sets the seed for NumPy's random number generator\n",
    "    np.random.seed(seed)\n",
    "    # Sets the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    # Sets the seed for PyTorch's random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    # If using a GPU, sets the seed for all CUDA devices\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "    # Ensures deterministic behavior in cuDNN (CUDA Deep Neural Network library)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf1683",
   "metadata": {},
   "source": [
    "### loading and preprocessing the data.\n",
    "\n",
    "The function loadClientsData is designed for a federated learning scenario. It reads data from separate files for each participant (or \"client\"), cleans it, aligns the different data types (sensor vs. image), and splits it into training and testing sets for each client.\n",
    "\n",
    "Because this function is quite long, we'll build it in a few parts.\n",
    "\n",
    "#### Part 1: Initializing and Processing Training Data\n",
    "First, we'll define the function, list the subject IDs we want to load, and create empty dictionaries to store each client's data. Then, we'll start a loop to process each subject one by one. Inside the loop, we'll begin by loading and cleaning the training data.\n",
    "\n",
    "This involves:\n",
    "\n",
    "Reading the sensor data from a CSV file.\n",
    "\n",
    "Removing rows with missing values and any duplicate rows.\n",
    "\n",
    "Dropping columns that we don't need (like the 'Infrared' sensor readings).\n",
    "\n",
    "Loading the corresponding image, label, and timestamp data from .npy files.\n",
    "\n",
    "#### Part 2: Aligning and Preparing Training Data\n",
    "After loading the raw data, we face a common problem: the datasets don't perfectly match. Because we dropped rows with missing values from the sensor (CSV) data, there are now timestamps in our image data that no longer have a corresponding entry in the sensor data.\n",
    "\n",
    "We need to align them by removing the image samples that don't have a matching sensor reading.\n",
    "\n",
    "After alignment, we'll prepare the data for the model:\n",
    "\n",
    "Set the seed for reproducibility.\n",
    "\n",
    "Separate features from labels.\n",
    "\n",
    "One-hot encode the labels, converting them into a format suitable for the model's output layer (e.g., class 3 becomes [0, 0, 0, 1, 0, ...]).\n",
    "\n",
    "Scale the numeric sensor data and the image pixel values.\n",
    "\n",
    "Reshape the images to the format expected by the convolutional layers.\n",
    "\n",
    "#### Part 3: Processing the Test Data and Finalizing the Function\n",
    "The logic here is identical to what we just did for the training data:\n",
    "\n",
    "Load the test sensor data (_test.csv) and test image data (_test.npy).\n",
    "\n",
    "Clean the sensor data by removing missing values and unnecessary columns.\n",
    "\n",
    "Align the test image data with the cleaned test sensor data.\n",
    "\n",
    "Prepare the aligned test data (one-hot encode labels, scale features, reshape images).\n",
    "\n",
    "Store all the processed training and test arrays into our dictionaries.\n",
    "\n",
    "Increment the clint_index and repeat the process for the next subject.\n",
    "\n",
    "After the loop finishes, the function returns all the dictionaries containing the data for every client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81f6095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadClientsData():\n",
    "    #subs = [1, 3, 4, 7, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "    subs = [1, 4, 7, 11, 12, 14, 15, 17]\n",
    "    X_train_csv_scaled_splits = {}\n",
    "    X_test_csv_scaled_splits = {}\n",
    "    Y_train_csv_splits = {}\n",
    "    Y_test_csv_splits = {}\n",
    "    X_train_1_scaled_splits = {}\n",
    "    X_test_1_scaled_splits = {}\n",
    "    Y_train_1_splits = {}\n",
    "    Y_test_1_splits = {}\n",
    "    X_train_2_scaled_splits = {}\n",
    "    X_test_2_scaled_splits = {}\n",
    "    Y_train_2_splits = {}\n",
    "    Y_test_2_splits = {}\n",
    "    clint_index = 0\n",
    "    for sub_ in subs:\n",
    "        # --- Load and clean TRAINING sensor data (CSV) ---\n",
    "        SUB_train = pd.read_csv('./dataset/Sensor + Image/{}_sensor_train.csv'.format(sub_), skiprows=1)\n",
    "        SUB_train.head()\n",
    "        \n",
    "        SUB_train.isnull().sum()\n",
    "        NA_cols = SUB_train.columns[SUB_train.isnull().any()]\n",
    "        SUB_train.dropna(inplace=True)\n",
    "        SUB_train.drop_duplicates(inplace=True)\n",
    "        \n",
    "        times_train = SUB_train['Time']\n",
    "        list_DROP = ['Infrared 1',\n",
    "                     'Infrared 2',\n",
    "                     'Infrared 3',\n",
    "                     'Infrared 4',\n",
    "                     'Infrared 5',\n",
    "                     'Infrared 6']\n",
    "        SUB_train.drop(list_DROP, axis=1, inplace=True)\n",
    "        SUB_train.drop(NA_cols, axis=1, inplace=True)  # drop NAN COLS\n",
    "\n",
    "        SUB_train.set_index('Time', inplace=True)\n",
    "        SUB_train.head()\n",
    "\n",
    "        # --- Load TRAINING image data from both cameras ---\n",
    "        cam = '1'\n",
    "        image_train = './dataset/Sensor + Image' + '/' + '{}_image_1_train.npy'.format(sub_)\n",
    "        name_train = './dataset/Sensor + Image' + '/' + '{}_name_1_train.npy'.format(sub_)\n",
    "        label_train = './dataset/Sensor + Image' + '/' + '{}_label_1_train.npy'.format(sub_)\n",
    "\n",
    "        img_1_train = np.load(image_train)\n",
    "        label_1_train = np.load(label_train)\n",
    "        name_1_train = np.load(name_train)\n",
    "\n",
    "        cam = '2'\n",
    "        image_train = './dataset/Sensor + Image' + '/' + '{}_image_2_train.npy'.format(sub_)\n",
    "        name_train = './dataset/Sensor + Image' + '/' + '{}_name_2_train.npy'.format(sub_)\n",
    "        label_train = './dataset/Sensor + Image' + '/' + '{}_label_2_train.npy'.format(sub_)\n",
    "\n",
    "        img_2_train = np.load(image_train)\n",
    "        label_2_train = np.load(label_train)\n",
    "        name_2_train = np.load(name_train)\n",
    "\n",
    "        # --- Align the training data by removing samples not present in the cleaned CSV ---\n",
    "        redundant_1 = list(set(name_1_train) - set(times_train))\n",
    "        redundant_2 = list(set(name_2_train) - set(times_train))\n",
    "        \n",
    "        ind = np.arange(0, len(img_1_train))\n",
    "\n",
    "        red_in1 = ind[np.isin(name_1_train, redundant_1)]\n",
    "        name_1_train = np.delete(name_1_train, red_in1)\n",
    "        img_1_train = np.delete(img_1_train, red_in1, axis=0)\n",
    "        label_1_train = np.delete(label_1_train, red_in1)\n",
    "\n",
    "        red_in2 = ind[np.isin(name_2_train, redundant_2)]\n",
    "        name_2_train = np.delete(name_2_train, red_in2)\n",
    "        img_2_train = np.delete(img_2_train, red_in2, axis=0)\n",
    "        label_2_train = np.delete(label_2_train, red_in2)\n",
    "        \n",
    "        # --- Prepare the final aligned training data ---\n",
    "        data_train = SUB_train.loc[name_1_train].values\n",
    "\n",
    "        set_seed()\n",
    "        X_csv_train, y_csv_train = data_train[:, :-1], data_train[:, -1]\n",
    "        \n",
    "        # Remap label 20 to 0 for consistency\n",
    "        y_csv_train = np.where(y_csv_train == 20, 0, y_csv_train)\n",
    "        label_1_train = np.where(label_1_train == 20, 0, label_1_train)\n",
    "        label_2_train = np.where(label_2_train == 20, 0, label_2_train)\n",
    "\n",
    "        # One-hot encode the labels for PyTorch\n",
    "        Y_csv_train = torch.nn.functional.one_hot(torch.from_numpy(y_csv_train).long(), 12).float()\n",
    "        Y_train_1 = torch.nn.functional.one_hot(torch.from_numpy(label_1_train).long(), 12).float()\n",
    "        Y_train_2 = torch.nn.functional.one_hot(torch.from_numpy(label_2_train).long(), 12).float()\n",
    "\n",
    "        \n",
    "\n",
    "        X_train_1 = img_1_train\n",
    "        y_train_1 = label_1_train\n",
    "        \n",
    "        X_train_2 = img_2_train\n",
    "        y_train_2 = label_2_train\n",
    "\n",
    "        # Reshape images to (samples, height, width, channels)\n",
    "        shape1, shape2 = 32, 32\n",
    "        X_train_1 = X_train_1.reshape(X_train_1.shape[0], shape1, shape2, 1)\n",
    "        X_train_2 = X_train_2.reshape(X_train_2.shape[0], shape1, shape2, 1)\n",
    "\n",
    "        # Scale image pixel values to be between 0 and 1\n",
    "        X_train_1_scaled = X_train_1 / 255.0\n",
    "        X_train_2_scaled = X_train_2 / 255.0\n",
    "\n",
    "        # --- Load and clean TEST sensor data (CSV) ---\n",
    "        SUB_test = pd.read_csv('./dataset/Sensor + Image/{}_sensor_test.csv'.format(sub_), skiprows=1)\n",
    "        SUB_test.head()\n",
    "        \n",
    "        SUB_test.isnull().sum()\n",
    "        NA_cols = SUB_test.columns[SUB_test.isnull().any()]\n",
    "        SUB_test.dropna(inplace=True)\n",
    "        SUB_test.drop_duplicates(inplace=True)\n",
    "\n",
    "        times_test = SUB_test['Time']\n",
    "        SUB_test.drop(list_DROP, axis=1, inplace=True)\n",
    "        SUB_test.drop(NA_cols, axis=1, inplace=True)\n",
    "\n",
    "        SUB_test.set_index('Time', inplace=True)\n",
    "        SUB_test.head()\n",
    "\n",
    "        # --- Load TEST image data from both cameras ---\n",
    "        image_test = './dataset/Sensor + Image' + '/' + '{}_image_1_test.npy'.format(sub_)\n",
    "        name_test = './dataset/Sensor + Image' + '/' + '{}_name_1_test.npy'.format(sub_)\n",
    "        label_test = './dataset/Sensor + Image' + '/' + '{}_label_1_test.npy'.format(sub_)\n",
    "        img_1_test = np.load(image_test)\n",
    "        label_1_test = np.load(label_test)\n",
    "        name_1_test = np.load(name_test)\n",
    "\n",
    "        image_test = './dataset/Sensor + Image' + '/' + '{}_image_2_test.npy'.format(sub_)\n",
    "        name_test = './dataset/Sensor + Image' + '/' + '{}_name_2_test.npy'.format(sub_)\n",
    "        label_test = './dataset/Sensor + Image' + '/' + '{}_label_2_test.npy'.format(sub_)\n",
    "        img_2_test = np.load(image_test)\n",
    "        label_2_test = np.load(label_test)\n",
    "        name_2_test = np.load(name_test)\n",
    "\n",
    "        # --- Align the test data ---\n",
    "        redundant_1 = list(set(name_1_test) - set(times_test))\n",
    "        redundant_2 = list(set(name_2_test) - set(times_test))\n",
    "        \n",
    "        ind = np.arange(0, len(img_1_test))\n",
    "\n",
    "        red_in1 = ind[np.isin(name_1_test, redundant_1)]\n",
    "        name_1_test = np.delete(name_1_test, red_in1)\n",
    "        img_1_test = np.delete(img_1_test, red_in1, axis=0)\n",
    "        label_1_test = np.delete(label_1_test, red_in1)\n",
    "\n",
    "        red_in2 = ind[np.isin(name_2_test, redundant_2)]\n",
    "        name_2_test = np.delete(name_2_test, red_in2)\n",
    "        img_2_test = np.delete(img_2_test, red_in2, axis=0)\n",
    "        label_2_test = np.delete(label_2_test, red_in2)\n",
    "\n",
    "        # --- Prepare the final aligned test data ---\n",
    "        data_test = SUB_test.loc[name_1_test].values\n",
    "\n",
    "        set_seed()\n",
    "        X_csv_test, y_csv_test = data_test[:, :-1], data_test[:, -1]\n",
    "        y_csv_test = np.where(y_csv_test == 20, 0, y_csv_test)\n",
    "        label_1_test = np.where(label_1_test == 20, 0, label_1_test)\n",
    "        label_2_test = np.where(label_2_test == 20, 0, label_2_test)\n",
    "\n",
    "        Y_csv_test = torch.nn.functional.one_hot(torch.from_numpy(y_csv_test).long(), 12).float()\n",
    "        X_csv_train_scaled, X_csv_test_scaled = scale_data(X_csv_train, X_csv_test)\n",
    "\n",
    "        X_test_1 = img_1_test\n",
    "        y_test_1 = label_1_test\n",
    "        Y_test_1 = torch.nn.functional.one_hot(torch.from_numpy(y_test_1).long(), 12).float()\n",
    "\n",
    "        X_test_2 = img_2_test\n",
    "        y_test_2 = label_2_test\n",
    "        Y_test_2 = torch.nn.functional.one_hot(torch.from_numpy(y_test_2).long(), 12).float()\n",
    "\n",
    "        X_test_1 = X_test_1.reshape(X_test_1.shape[0], shape1, shape2, 1)\n",
    "        X_test_2 = X_test_2.reshape(X_test_2.shape[0], shape1, shape2, 1)\n",
    "\n",
    "        X_test_1_scaled = X_test_1 / 255.0\n",
    "        X_test_2_scaled = X_test_2 / 255.0\n",
    "\n",
    "        # --- Store all processed data for the current client ---\n",
    "        X_train_csv_scaled_splits[clint_index] = X_csv_train_scaled\n",
    "        X_test_csv_scaled_splits[clint_index] = X_csv_test_scaled\n",
    "        Y_train_csv_splits[clint_index] = Y_csv_train\n",
    "        Y_test_csv_splits[clint_index] = Y_csv_test\n",
    "        X_train_1_scaled_splits[clint_index] = X_train_1_scaled\n",
    "        X_test_1_scaled_splits[clint_index] = X_test_1_scaled\n",
    "        Y_train_1_splits[clint_index] = Y_train_1\n",
    "        Y_test_1_splits[clint_index] = Y_test_1\n",
    "        X_train_2_scaled_splits[clint_index] = X_train_2_scaled # This line had a bug in the original code\n",
    "        X_test_2_scaled_splits[clint_index] = X_test_2_scaled\n",
    "        Y_train_2_splits[clint_index] = Y_train_2\n",
    "        Y_test_2_splits[clint_index] = Y_test_2\n",
    "        clint_index += 1\n",
    "        \n",
    "    # --- After loop, return all dictionaries ---\n",
    "    return X_train_csv_scaled_splits,X_test_csv_scaled_splits, Y_train_csv_splits,Y_test_csv_splits,X_train_1_scaled_splits,X_test_1_scaled_splits,Y_train_1_splits,Y_test_1_splits,X_train_2_scaled_splits,X_test_2_scaled_splits,Y_train_2_splits,Y_test_2_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e1b50",
   "metadata": {},
   "source": [
    "## Step 2: Client Selection\n",
    "\n",
    "We're making great progress. We've handled all the data loading and preparation. Now, we'll add the functions that form the \"intelligence\" of our federated learning system: client selection.\n",
    "\n",
    "Instead of blindly averaging updates from every client in every round, these methods evaluate each client's performance and contribution. This allows the server to select the most promising or reliable clients to participate in the global model update, potentially leading to faster convergence and a more robust final model.\n",
    "\n",
    "We'll add a series of functions, each calculating a specific metric to judge the clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5173b0f8",
   "metadata": {},
   "source": [
    "### Client Evaluation Metrics\n",
    "Add all the following functions to your script. Each one calculates a different score based on a client's performance.\n",
    "\n",
    "1. Relative Loss Reduction (RF_loss)\n",
    "\n",
    "This measures how much a client's training loss has dropped from the beginning to the end of a local training round, relative to the client with the biggest drop. A higher score means the client is learning effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f48d0a",
   "metadata": {},
   "source": [
    "2. Relative Training Accuracy (RF_ACC_Train)\n",
    "\n",
    "This measures a client's local training accuracy relative to the client with the highest accuracy. It's a straightforward measure of performance on local data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e977c2",
   "metadata": {},
   "source": [
    "3. Global Validation Accuracy (RF_ACC_Global)\n",
    "\n",
    "This is a more sophisticated metric. It rewards clients for high accuracy on a global test set but penalizes them if their global accuracy is much worse than their local training accuracy (which is a sign of overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26f9bf",
   "metadata": {},
   "source": [
    "4. Loss Outliers (P_loss)\n",
    "\n",
    "This function flags clients that are potential negative contributors. If a client's final training loss is significantly higher than the average loss of all clients, it gets a high penalty score. Otherwise, its penalty is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96a2ac",
   "metadata": {},
   "source": [
    "5. Performance Bias (P_bias)\n",
    "\n",
    "This metric calculates the gap between a client's performance on its own validation data versus its performance on the global validation data. A large gap might indicate that the client's local data is not representative of the overall data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f94b9",
   "metadata": {},
   "source": [
    "##### Replace above Metric Helper Functions by below updated version that use disctionaries instead of Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6a847f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Replace ALL old metric helpers with these dictionary-based versions ---\n",
    "\n",
    "def calculate_relative_loss_reduction_as_list(client_losses):\n",
    "    \"\"\"Calculates RF_loss. Returns a DICTIONARY {cid: score}.\"\"\"\n",
    "    loss_reductions = {}\n",
    "    for cid, losses in client_losses.items():\n",
    "        if losses and len(losses) >= 2:\n",
    "            loss_reductions[cid] = losses[0] - losses[-1]\n",
    "\n",
    "    if not loss_reductions: return {cid: 0.0 for cid in client_losses.keys()}\n",
    "    max_loss_reduction = max(loss_reductions.values())\n",
    "    if max_loss_reduction == 0: return {cid: 0.0 for cid in client_losses.keys()}\n",
    "    \n",
    "    return {cid: loss_reductions.get(cid, 0.0) / max_loss_reduction for cid in client_losses.keys()}\n",
    "\n",
    "def calculate_relative_train_accuracy(client_acc):\n",
    "    \"\"\"Calculates RF_ACC_Train. Returns a DICTIONARY {cid: score}.\"\"\"\n",
    "    if not client_acc: return {}\n",
    "    max_acc = max(client_acc.values())\n",
    "    if max_acc == 0: return {cid: 0.0 for cid in client_acc.keys()}\n",
    "    return {cid: acc / max_acc for cid, acc in client_acc.items()}\n",
    "\n",
    "def calculate_global_validation_accuracy(train_acc, global_acc):\n",
    "    \"\"\"Calculates RF_ACC_Global. Returns a DICTIONARY {cid: score}.\"\"\"\n",
    "    if not train_acc or not global_acc: return {}\n",
    "    max_global_acc = max(global_acc.values()) if global_acc else 0\n",
    "    if max_global_acc == 0: max_global_acc = 1.0\n",
    "\n",
    "    global_train_diff = {cid: global_acc.get(cid, 0) - train_acc.get(cid, 0) for cid in train_acc.keys()}\n",
    "    max_global_train_diff = max(global_train_diff.values()) if global_train_diff else 0\n",
    "    if max_global_train_diff == 0: max_global_train_diff = 1.0\n",
    "    \n",
    "    return {cid: (global_acc.get(cid, 0) / max_global_acc) - (diff / max_global_train_diff) for cid, diff in global_train_diff.items()}\n",
    "\n",
    "def calculate_loss_outliers(client_losses, lambda_loss=1.5):\n",
    "    \"\"\"Calculates P_loss. Returns a DICTIONARY {cid: score}.\"\"\"\n",
    "    final_losses = {cid: losses[-1] for cid, losses in client_losses.items() if losses}\n",
    "    if not final_losses: return {cid: 0.0 for cid in client_losses.keys()}\n",
    "\n",
    "    loss_values = np.array(list(final_losses.values()))\n",
    "    mean_loss, std_loss = np.mean(loss_values), np.std(loss_values)\n",
    "    threshold = mean_loss + lambda_loss * std_loss\n",
    "    max_loss = np.max(loss_values)\n",
    "    if max_loss == 0: return {cid: 0.0 for cid in client_losses.keys()}\n",
    "    \n",
    "    all_client_scores = {}\n",
    "    for cid in client_losses.keys():\n",
    "        final_loss = final_losses.get(cid, 0.0)\n",
    "        score = final_loss / max_loss if final_loss > threshold else 0.0\n",
    "        all_client_scores[cid] = score\n",
    "    return all_client_scores\n",
    "\n",
    "def calculate_performance_bias(val_acc, global_acc):\n",
    "    \"\"\"Calculates P_bias. Returns a DICTIONARY {cid: score}.\"\"\"\n",
    "    if not val_acc: return {}\n",
    "    \n",
    "    bias_dict = {}\n",
    "    for cid, val in val_acc.items():\n",
    "        global_val = global_acc.get(cid, 0.0)\n",
    "        max_val = max(val, global_val)\n",
    "        bias = 0.0 if max_val == 0 else abs(val - global_val) / max_val\n",
    "        bias_dict[cid] = bias\n",
    "    return bias_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfd596",
   "metadata": {},
   "source": [
    "Excellent. Now that we have the functions to score each client, we need the final step: the algorithms that use these scores to select which clients will participate in a given round."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cecd1",
   "metadata": {},
   "source": [
    "### Client Selection Algorithms\n",
    "1. Pareto Optimization\n",
    "\n",
    "This is a powerful technique used when you have multiple, often conflicting, objectives. Instead of combining all metrics into one score, it tries to find a set of clients that represent the best possible trade-offs.\n",
    "\n",
    "A client is considered \"Pareto optimal\" if no other client is better than it across all metrics. The algorithm first finds this set of optimal clients.\n",
    "\n",
    "If there are more optimal clients than needed, it selects a random subset.\n",
    "\n",
    "If there are fewer, it fills the remaining spots by picking the clients with the best-combined performance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25f90e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_optimization(rf_loss, rf_acc_train, rf_acc_val, rf_acc_global, p_loss, p_bias, client_num, client_ids):\n",
    "    \"\"\"Implements Pareto optimization to select clients.\"\"\"\n",
    "    # Convert metric dicts to a numpy array, ensuring a consistent order via client_ids\n",
    "    data_points = [\n",
    "        np.array([rf_loss.get(cid, 0), rf_acc_train.get(cid, 0), rf_acc_val.get(cid, 0),\n",
    "                  rf_acc_global.get(cid, 0), -p_loss.get(cid, 0), -p_bias.get(cid, 0)])\n",
    "        for cid in client_ids\n",
    "    ]\n",
    "    data = np.array(data_points)\n",
    "\n",
    "    def is_dominated(point, others):\n",
    "        \"\"\"Checks if a point is dominated by any other point in the set.\"\"\"\n",
    "        return any(np.all(other >= point) and np.any(other > point) for other in others)\n",
    "\n",
    "    pareto_indices = [i for i, point in enumerate(data) if not is_dominated(point, np.delete(data, i, axis=0))]\n",
    "    # Map indices back to the original string client IDs\n",
    "    pareto_cids = [client_ids[i] for i in pareto_indices]\n",
    "\n",
    "    if len(pareto_cids) >= client_num:\n",
    "        return random.sample(pareto_cids, client_num)\n",
    "    \n",
    "    # CORRECTED: Use a dictionary comprehension with string CIDs\n",
    "    pareto_scores = {cid: 0.4 * rf_loss.get(cid, 0) + 0.6 * rf_acc_global.get(cid, 0) for cid in client_ids}\n",
    "    \n",
    "    # Sort the original client IDs based on their scores\n",
    "    sorted_cids = sorted(client_ids, key=lambda cid: pareto_scores.get(cid, 0), reverse=True)\n",
    "    \n",
    "    selected_clients = set(pareto_cids)\n",
    "    for cid in sorted_cids:\n",
    "        if len(selected_clients) >= client_num:\n",
    "            break\n",
    "        selected_clients.add(cid)\n",
    "        \n",
    "    # CORRECTED: Return a list of strings\n",
    "    return list(selected_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebf65c",
   "metadata": {},
   "source": [
    "2. Weighted Sum Method (5RF)\n",
    "\n",
    "This is a more straightforward approach. It calculates a single comprehensive score for each client by taking a weighted sum of all the metrics. Clients with the highest final scores are selected. The weights (0.2, 0.1, 0.3, etc.) determine the importance of each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04b0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee125dce",
   "metadata": {},
   "source": [
    "## Step 2: The AI's Brain (The Model Definition)\n",
    "We have the data pipeline and the client selection logic. Now it's time to build the brain of the operation: the neural network model itself.\n",
    "\n",
    "The model, ModelCSVIMG, is a multi-modal neural network. This means it's designed to accept and process multiple types of data at once. It has three distinct input branches:\n",
    "\n",
    "One for the numerical sensor (CSV) data.\n",
    "\n",
    "One for the images from camera 1.\n",
    "\n",
    "One for the images from camera 2.\n",
    "\n",
    "The features extracted from each branch are then combined (fused) and passed to a final set of layers that perform the classification. The original code contains a few versions of the architecture; we will use the final, most complex one.\n",
    "\n",
    "Add the complete model class to your script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "218bbd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCSVIMG(nn.Module):\n",
    "    def __init__(self, num_csv_features, img_shape1, img_shape2):\n",
    "        super(ModelCSVIMG, self).__init__()\n",
    "\n",
    "        # --- Branch 1: For processing numerical CSV data ---\n",
    "        self.csv_fc_1 = nn.Linear(num_csv_features, 2000)\n",
    "        self.csv_bn_1 = nn.BatchNorm1d(2000)\n",
    "        self.csv_fc_2 = nn.Linear(2000, 600)\n",
    "        self.csv_bn_2 = nn.BatchNorm1d(600)\n",
    "        self.csv_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # --- Branch 2: For processing images from Camera 1 (CNN) ---\n",
    "        self.img1_conv_1 = nn.Conv2d(in_channels=1, out_channels=18, kernel_size=3, stride=1, padding=1)\n",
    "        self.img1_batch_norm = nn.BatchNorm2d(18)\n",
    "        self.img1_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Flattened features from the CNN go into a fully connected layer\n",
    "        self.img1_fc1 = nn.Linear(18 * 16 * 16, 100)\n",
    "        self.img1_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # --- Branch 3: For processing images from Camera 2 (identical to Branch 2) ---\n",
    "        self.img2_conv = nn.Conv2d(in_channels=1, out_channels=18, kernel_size=3, stride=1, padding=1)\n",
    "        self.img2_batch_norm = nn.BatchNorm2d(18)\n",
    "        self.img2_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.img2_fc1 = nn.Linear(18 * 16 * 16, 100)\n",
    "        self.img2_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # --- Fusion and Final Classification Layers ---\n",
    "        # The input size is 600 (from CSV) + 100 (from Image 1) + 100 (from Image 2) = 800\n",
    "        self.fc1 = nn.Linear(800, 1200)\n",
    "        self.dr1 = nn.Dropout(0.2)\n",
    "        # A residual connection is used here: input to fc2 is the original 800 + output of fc1 (1200) = 2000\n",
    "        self.fc2 = nn.Linear(2000, 12) # 12 output classes\n",
    "\n",
    "    def forward(self, x_csv, x_img1, x_img2):\n",
    "        # --- Process CSV data ---\n",
    "        x_csv = F.relu(self.csv_bn_1(self.csv_fc_1(x_csv)))\n",
    "        x_csv = F.relu(self.csv_bn_2(self.csv_fc_2(x_csv)))\n",
    "        x_csv = self.csv_dropout(x_csv)\n",
    "\n",
    "        # --- Process Image 1 data ---\n",
    "        # Reshape image from (batch, height, width, channels) to (batch, channels, height, width)\n",
    "        x_img1 = x_img1.permute(0, 3, 1, 2)\n",
    "        x_img1 = F.relu(self.img1_conv_1(x_img1))\n",
    "        x_img1 = self.img1_batch_norm(x_img1)\n",
    "        x_img1 = self.img1_pool(x_img1)\n",
    "        x_img1 = x_img1.contiguous().view(x_img1.size(0), -1) # Flatten\n",
    "        x_img1 = F.relu(self.img1_fc1(x_img1))\n",
    "        x_img1 = self.img1_dropout(x_img1)\n",
    "\n",
    "        # --- Process Image 2 data ---\n",
    "        x_img2 = x_img2.permute(0, 3, 1, 2)\n",
    "        x_img2 = F.relu(self.img2_conv(x_img2))\n",
    "        x_img2 = self.img2_batch_norm(x_img2)\n",
    "        x_img2 = self.img2_pool(x_img2)\n",
    "        x_img2 = x_img2.contiguous().view(x_img2.size(0), -1) # Flatten\n",
    "        x_img2 = F.relu(self.img2_fc1(x_img2))\n",
    "        x_img2 = self.img2_dropout(x_img2)\n",
    "\n",
    "        # --- Fusion ---\n",
    "        x = torch.cat((x_csv, x_img1, x_img2), dim=1)\n",
    "        residual = x # Keep a copy for the residual connection\n",
    "        \n",
    "        # --- Final layers ---\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dr1(x)\n",
    "        # Concatenate the residual connection\n",
    "        x = torch.cat((residual, x), dim=1)\n",
    "        # Final output with softmax for classification\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed243d7",
   "metadata": {},
   "source": [
    "## Step 3: Define Training and Testing Functions\n",
    "Instead of methods inside a Client class, we'll create standalone train and test functions. This makes the code cleaner. This logic is taken directly from your train_one_epoch and validate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24327736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs):\n",
    "    \"\"\"Train the model and return the list of losses.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    net.train()\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in trainloader:\n",
    "            data1, data2, data3, target = batch\n",
    "            data1, data2, data3, target = data1.to(DEVICE).float(), data2.to(DEVICE).float(), data3.to(DEVICE).float(), target.to(DEVICE).float()\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(net(data1, data2, data3), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss / len(trainloader))\n",
    "    return losses # Return the history of losses\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Validate the model on the test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(testloader, \"Testing\"):\n",
    "            data1, data2, data3, target = batch\n",
    "            data1, data2, data3, target = data1.to(DEVICE).float(), data2.to(DEVICE).float(), data3.to(DEVICE).float(), target.to(DEVICE).float()\n",
    "            outputs = net(data1, data2, data3)\n",
    "            loss += criterion(outputs, target).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == torch.max(target, 1)[1]).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return loss / len(testloader), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1123f",
   "metadata": {},
   "source": [
    "## Step 4: Create the Flower Client\n",
    "This is where Flower really comes in. We'll replace your custom Client class with a FlowerClient that inherits from Flower's NumPyClient. This class tells Flower how each client should handle parameters received from the server and how to perform local training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e0218cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = zip(self.net.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        self.net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        # NEW: Print when a client starts training\n",
    "        print(f\"✅ [Client {self.cid}] Starting training for round {config['server_round']}...\")\n",
    "\n",
    "        self.set_parameters(parameters)\n",
    "        \n",
    "        # Train and get the list of losses\n",
    "        losses = train(self.net, self.trainloader, epochs=3)\n",
    "        \n",
    "        # Also get the local validation accuracy\n",
    "        _, val_acc = test(self.net, self.valloader)\n",
    "\n",
    "        # NEW: Announce completion of training\n",
    "        print(f\"✅ [Client {self.cid}] Finished training.\")\n",
    "        \n",
    "        # Return parameters, dataset size, and our custom metrics dictionary\n",
    "        return self.get_parameters(config={}), len(self.trainloader.dataset), {\n",
    "            \"train_losses\": losses, \n",
    "            \"val_acc\": val_acc\n",
    "        }\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        # NEW: Print when a client starts evaluation\n",
    "        print(f\"[Client {self.cid}] Starting evaluation...\")\n",
    "\n",
    "        self.set_parameters(parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        \n",
    "        # NEW: Print the client's evaluation results\n",
    "        print(f\"✅ [Client {self.cid}] Evaluation results - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        return float(loss), len(self.valloader.dataset), {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da725a3a",
   "metadata": {},
   "source": [
    "## Step 5: Create the ParetoStrategy\n",
    "Now for the main event. We create a new strategy class that collects all the client metrics and uses them to run your Pareto selection logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "424950c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParetoStrategy(fl.server.strategy.FedAvg):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.client_fit_metrics = {}\n",
    "        self.client_eval_metrics = {}\n",
    "\n",
    "        # NEW: Define a function to create the config for clients\n",
    "        def on_fit_config_fn(server_round: int):\n",
    "            \"\"\"Return a configuration dictionary for the clients.\"\"\"\n",
    "            config = {\"server_round\": server_round}\n",
    "            return config\n",
    "        \n",
    "        # NEW: Assign this function to the strategy\n",
    "        self.on_fit_config_fn = on_fit_config_fn\n",
    "\n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        for _, fit_res in results:\n",
    "            if fit_res.metrics:\n",
    "                self.client_fit_metrics[fit_res.client.cid] = fit_res.metrics\n",
    "        return super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "    def aggregate_evaluate(self, server_round, results, failures):\n",
    "        # NEW: Announce that the server has received evaluation results\n",
    "        print(f\"✅ --- Server received {len(results)} evaluation results in round {server_round} ---\")\n",
    "        for client, eval_res in results:\n",
    "            self.client_eval_metrics[client.cid] = {\"accuracy\": eval_res.metrics[\"accuracy\"]}\n",
    "        return super().aggregate_evaluate(server_round, results, failures)\n",
    "\n",
    "    def configure_fit(self, server_round, parameters, client_manager):\n",
    "        if server_round == 1:\n",
    "            print(\"--- Round 1: Selecting clients randomly ---\")\n",
    "            return super().configure_fit(server_round, parameters, client_manager)\n",
    "\n",
    "        all_cids = list(client_manager.clients.keys())\n",
    "        train_losses = {cid: self.client_fit_metrics.get(cid, {}).get(\"train_losses\", []) for cid in all_cids}\n",
    "        local_val_acc = {cid: self.client_fit_metrics.get(cid, {}).get(\"val_acc\", 0.0) for cid in all_cids}\n",
    "        local_train_acc_for_rf = local_val_acc\n",
    "        global_eval_acc = {cid: self.client_eval_metrics.get(cid, {}).get(\"accuracy\", 0.0) for cid in all_cids}\n",
    "        \n",
    "        rf_loss = calculate_relative_loss_reduction_as_list(train_losses)\n",
    "        rf_acc_train = calculate_relative_train_accuracy(local_train_acc_for_rf)\n",
    "        rf_acc_val = calculate_relative_train_accuracy(local_val_acc)\n",
    "        rf_acc_global = calculate_global_validation_accuracy(local_train_acc_for_rf, global_eval_acc)\n",
    "        p_loss = calculate_loss_outliers(train_losses)\n",
    "        p_bias = calculate_performance_bias(local_val_acc, global_eval_acc)\n",
    "        \n",
    "        num_to_select = int(self.fraction_fit * len(all_cids))\n",
    "        selected_cids = pareto_optimization(\n",
    "            rf_loss, rf_acc_train, rf_acc_val, rf_acc_global, p_loss, p_bias,\n",
    "            num_to_select, all_cids\n",
    "        )\n",
    "        \n",
    "        # NEW: Announce which clients were selected by the strategy\n",
    "        print(f\"✅ --- Pareto selection for round {server_round} chose clients: {selected_cids} ---\")\n",
    "\n",
    "        clients = [client_manager.clients[cid] for cid in selected_cids]\n",
    "        config = self.on_fit_config_fn(server_round)\n",
    "        return [(client, config) for client in clients]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee359c6b",
   "metadata": {},
   "source": [
    "## Step 5: The main Function - Bringing It All Together\n",
    "Finally, we rewrite the main function. This is where we will:\n",
    "\n",
    "Load all the client data just once.\n",
    "\n",
    "Define a client_fn. Flower uses this function to create clients on demand for the simulation.\n",
    "\n",
    "Define a server-side evaluation function (get_evaluate_fn) so the server can test the global model's performance on a held-out test set after each round.\n",
    "\n",
    "Configure and start the Flower simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "619bbfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loading data for all clients...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
      "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
      "\n",
      "\t\t$ flwr new  # Create a new Flower app from a template\n",
      "\n",
      "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
      "\n",
      "\tUsing `start_simulation()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=10, no round_timeout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data for 8 clients loaded successfully.\n",
      "✅ Starting Flower simulation with Pareto Strategy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 08:37:29,954\tINFO worker.py:1771 -- Started a local Ray instance.\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'node:172.30.170.62': 1.0, 'CPU': 16.0, 'object_store_memory': 4869376819.0, 'memory': 9738753639.0, 'GPU': 1.0}\n",
      "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 4, 'num_gpus': 0.25}\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 4 actors\n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m         \n",
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "Testing: 100%|██████████| 181/181 [00:00<00:00, 236.88it/s]\n",
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 2.4836522531772847, {'accuracy': 0.21077242812608243}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ ROUND 0 SUMMARY - Global Model Performance:\n",
      "   Loss: 2.4837 | Accuracy: 0.2108\n",
      "============================================================\n",
      "--- Round 1: Selecting clients randomly ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m ✅ [Client 1] Starting training for round 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m /home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:285: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n",
      "\u001b[36m(ClientAppActor pid=7163)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=7163)\u001b[0m         \n",
      "\u001b[36m(ClientAppActor pid=7163)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=7163)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=7163)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 2273 MiB, 1 objects, write throughput 219 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001b[36m(ClientAppActor pid=7163)\u001b[0m /home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:285: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "\u001b[36m(ClientAppActor pid=7163)\u001b[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n",
      "Testing:   0%|          | 0/186 [00:00<?, ?it/s]\n",
      "Testing:   2%|▏         | 4/186 [00:00<00:04, 36.76it/s]\n",
      "Testing:   9%|▊         | 16/186 [00:00<00:02, 82.56it/s]\n",
      "Testing:  15%|█▌        | 28/186 [00:00<00:01, 97.51it/s]\n",
      "Testing:  22%|██▏       | 40/186 [00:00<00:01, 103.03it/s]\n",
      "Testing:  28%|██▊       | 52/186 [00:00<00:01, 106.53it/s]\n",
      "Testing:  35%|███▍      | 65/186 [00:00<00:01, 113.69it/s]\n",
      "Testing:  42%|████▏     | 78/186 [00:00<00:00, 117.59it/s]\n",
      "Testing:  49%|████▉     | 92/186 [00:00<00:00, 123.27it/s]\n",
      "Testing:  57%|█████▋    | 106/186 [00:00<00:00, 125.21it/s]\n",
      "Testing:  65%|██████▍   | 120/186 [00:01<00:00, 128.61it/s]\n",
      "Testing:  72%|███████▏  | 134/186 [00:01<00:00, 131.14it/s]\n",
      "Testing:  81%|████████  | 150/186 [00:01<00:00, 137.68it/s]\n",
      "Testing:  88%|████████▊ | 164/186 [00:01<00:00, 138.26it/s]\n",
      "Testing:  96%|█████████▌| 178/186 [00:01<00:00, 138.49it/s]\n",
      "Testing: 100%|██████████| 186/186 [00:01<00:00, 121.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=7165)\u001b[0m ✅ [Client 1] Finished training.\n",
      "\u001b[36m(ClientAppActor pid=7163)\u001b[0m ✅ [Client 5] Starting training for round 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=7164)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=7164)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=7164)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=7164)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=7164)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=7164)\u001b[0m ✅ [Client 3] Starting training for round 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=7164)\u001b[0m /home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:285: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "\u001b[36m(ClientAppActor pid=7164)\u001b[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n",
      "Testing: 100%|██████████| 182/182 [00:00<00:00, 218.75it/s]\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 4546 MiB, 2 objects, write throughput 246 MiB/s.\n",
      "Testing:   0%|          | 0/182 [00:00<?, ?it/s]\n",
      "Testing:  85%|████████▍ | 154/182 [00:00<00:00, 229.60it/s]\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=7162)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
      "\u001b[36m(ClientAppActor pid=7162)\u001b[0m \n",
      "\u001b[36m(ClientAppActor pid=7162)\u001b[0m             This is a deprecated feature. It will be removed\n",
      "\u001b[36m(ClientAppActor pid=7162)\u001b[0m             entirely in future versions of Flower.\n",
      "\u001b[36m(ClientAppActor pid=7162)\u001b[0m         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=7163)\u001b[0m ✅ [Client 5] Finished training.\n",
      "\u001b[36m(ClientAppActor pid=7162)\u001b[0m ✅ [Client 0] Starting training for round 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/189 [00:00<?, ?it/s]\n",
      "Testing:   7%|▋         | 13/189 [00:00<00:01, 124.96it/s]\n",
      "Testing:  17%|█▋        | 33/189 [00:00<00:00, 165.02it/s]\n",
      "Testing:  29%|██▊       | 54/189 [00:00<00:00, 183.58it/s]\n",
      "Testing:  39%|███▊      | 73/189 [00:00<00:00, 178.73it/s]\n",
      "Testing:  55%|█████▌    | 104/189 [00:00<00:00, 223.86it/s]\n",
      "Testing:  71%|███████▏  | 135/189 [00:00<00:00, 250.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=7164)\u001b[0m ✅ [Client 3] Finished training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 189/189 [00:00<00:00, 237.42it/s]\n",
      "\u001b[36m(ClientAppActor pid=7162)\u001b[0m /home/syed/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:285: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "\u001b[36m(ClientAppActor pid=7162)\u001b[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n",
      "Testing:   0%|          | 0/185 [00:00<?, ?it/s]\n",
      "Testing:   9%|▊         | 16/185 [00:00<00:01, 155.17it/s]\n",
      "Testing:  23%|██▎       | 42/185 [00:00<00:00, 209.85it/s]\n",
      "Testing:  39%|███▉      | 73/185 [00:00<00:00, 254.11it/s]\n",
      "Testing:  57%|█████▋    | 106/185 [00:00<00:00, 280.79it/s]\n",
      "Testing:  74%|███████▎  | 136/185 [00:00<00:00, 284.96it/s]\n",
      "Testing: 100%|██████████| 185/185 [00:00<00:00, 271.98it/s]\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 4 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=7162)\u001b[0m ✅ [Client 0] Finished training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 181/181 [00:00<00:00, -923.87it/s]\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (1, 2.4836522531772847, {'accuracy': 0.21077242812608243}, 45.437558550000006)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 8)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 4 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ ROUND 1 SUMMARY - Global Model Performance:\n",
      "   Loss: 2.4837 | Accuracy: 0.2108\n",
      "============================================================\n",
      "✅ --- Pareto selection for round 2 chose clients: ['6410545173860501411', '14647438590066615780', '2009433732401043908', '6760040680406013886'] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 181/181 [00:00<00:00, 258.40it/s]\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (2, 2.4836522531772847, {'accuracy': 0.21077242812608243}, 46.199924065999994)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 8)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 4 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ ROUND 2 SUMMARY - Global Model Performance:\n",
      "   Loss: 2.4837 | Accuracy: 0.2108\n",
      "============================================================\n",
      "✅ --- Pareto selection for round 3 chose clients: ['13088511329225947172', '6410545173860501411', '2712072218833966396', '6760040680406013886'] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 181/181 [00:00<00:00, 269.65it/s]\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (3, 2.4836522531772847, {'accuracy': 0.21077242812608243}, 46.92677563400002)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 8)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 4 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ ROUND 3 SUMMARY - Global Model Performance:\n",
      "   Loss: 2.4837 | Accuracy: 0.2108\n",
      "============================================================\n",
      "✅ --- Pareto selection for round 4 chose clients: ['1972966586079314872', '6760040680406013886', '14647438590066615780', '1901063158383376400'] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 181/181 [00:00<00:00, 279.95it/s]\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (4, 2.4836522531772847, {'accuracy': 0.21077242812608243}, 47.622411854000006)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 8)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 4 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ ROUND 4 SUMMARY - Global Model Performance:\n",
      "   Loss: 2.4837 | Accuracy: 0.2108\n",
      "============================================================\n",
      "✅ --- Pareto selection for round 5 chose clients: ['6410545173860501411', '2712072218833966396', '14647438590066615780', '2009433732401043908'] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 181/181 [00:00<00:00, 247.03it/s]\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (5, 2.4836522531772847, {'accuracy': 0.21077242812608243}, 48.41799653600003)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 6]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 8)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 4 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ ROUND 5 SUMMARY - Global Model Performance:\n",
      "   Loss: 2.4837 | Accuracy: 0.2108\n",
      "============================================================\n",
      "✅ --- Pareto selection for round 6 chose clients: ['6410545173860501411', '2712072218833966396', '2009433732401043908', '1901063158383376400'] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 181/181 [00:00<00:00, 215.46it/s]\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (6, 2.4836522531772847, {'accuracy': 0.21077242812608243}, 49.302087975000006)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 7]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 8)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 4 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ ROUND 6 SUMMARY - Global Model Performance:\n",
      "   Loss: 2.4837 | Accuracy: 0.2108\n",
      "============================================================\n",
      "✅ --- Pareto selection for round 7 chose clients: ['1901063158383376400', '13088511329225947172', '2712072218833966396', '6410545173860501411'] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 181/181 [00:00<00:00, 272.68it/s]\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (7, 2.4836522531772847, {'accuracy': 0.21077242812608243}, 50.00824399100003)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 8]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 8)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 4 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ ROUND 7 SUMMARY - Global Model Performance:\n",
      "   Loss: 2.4837 | Accuracy: 0.2108\n",
      "============================================================\n",
      "✅ --- Pareto selection for round 8 chose clients: ['13088511329225947172', '14647438590066615780', '1901063158383376400', '6760040680406013886'] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 181/181 [00:00<00:00, 301.78it/s]\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (8, 2.4836522531772847, {'accuracy': 0.21077242812608243}, 50.65002373300001)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 9]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 8)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 4 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ ROUND 8 SUMMARY - Global Model Performance:\n",
      "   Loss: 2.4837 | Accuracy: 0.2108\n",
      "============================================================\n",
      "✅ --- Pareto selection for round 9 chose clients: ['1901063158383376400', '1972966586079314872', '2712072218833966396', '6410545173860501411'] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 181/181 [00:00<00:00, 306.31it/s]\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (9, 2.4836522531772847, {'accuracy': 0.21077242812608243}, 51.290440776000025)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 10]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 8)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 4 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ ROUND 9 SUMMARY - Global Model Performance:\n",
      "   Loss: 2.4837 | Accuracy: 0.2108\n",
      "============================================================\n",
      "✅ --- Pareto selection for round 10 chose clients: ['14647438590066615780', '2009433732401043908', '1901063158383376400', '6760040680406013886'] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 181/181 [00:00<00:00, 292.39it/s]\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (10, 2.4836522531772847, {'accuracy': 0.21077242812608243}, 51.95566647100003)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 10 round(s) in 51.96s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, centralized):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 0: 2.4836522531772847\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 2.4836522531772847\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 2.4836522531772847\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 3: 2.4836522531772847\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 4: 2.4836522531772847\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 5: 2.4836522531772847\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 6: 2.4836522531772847\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 7: 2.4836522531772847\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 8: 2.4836522531772847\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 9: 2.4836522531772847\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 10: 2.4836522531772847\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, centralized):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(0, 0.21077242812608243),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (1, 0.21077242812608243),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (2, 0.21077242812608243),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (3, 0.21077242812608243),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (4, 0.21077242812608243),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (5, 0.21077242812608243),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (6, 0.21077242812608243),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (7, 0.21077242812608243),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (8, 0.21077242812608243),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (9, 0.21077242812608243),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (10, 0.21077242812608243)]}\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✅ ROUND 10 SUMMARY - Global Model Performance:\n",
      "   Loss: 2.4837 | Accuracy: 0.2108\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "              SIMULATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "📈 Global Model Performance Summary:\n",
      " Round     Loss  Accuracy\n",
      "     0 2.483652  0.210772\n",
      "     1 2.483652  0.210772\n",
      "     2 2.483652  0.210772\n",
      "     3 2.483652  0.210772\n",
      "     4 2.483652  0.210772\n",
      "     5 2.483652  0.210772\n",
      "     6 2.483652  0.210772\n",
      "     7 2.483652  0.210772\n",
      "     8 2.483652  0.210772\n",
      "     9 2.483652  0.210772\n",
      "    10 2.483652  0.210772\n",
      "\n",
      "✅ Summary successfully saved to: simulation_summary.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # --- Load Data ---\n",
    "    print(\"✅ Loading data for all clients...\")\n",
    "    all_data = loadClientsData()\n",
    "    X_train_csv, X_test_csv, Y_train_csv, Y_test_csv, \\\n",
    "    X_train_1, X_test_1, Y_train_1, Y_test_1, \\\n",
    "    X_train_2, X_test_2, Y_train_2, Y_test_2 = all_data\n",
    "    TOTAL_CLIENTS = len(X_train_csv)\n",
    "    print(f\"✅ Data for {TOTAL_CLIENTS} clients loaded successfully.\")\n",
    "\n",
    "    # --- Client Factory ---\n",
    "    # In your main function\n",
    "\n",
    "    def client_fn(cid: str) -> fl.client.Client: # Return type is now fl.client.Client\n",
    "        client_id = int(cid)\n",
    "        net = ModelCSVIMG(num_csv_features=X_train_csv[client_id].shape[1], img_shape1=32, img_shape2=32).to(DEVICE)\n",
    "        train_dataset = CustomDatasetRes(X_train_csv[client_id], X_train_1[client_id], X_train_2[client_id], Y_train_csv[client_id])\n",
    "        val_dataset = CustomDatasetRes(X_test_csv[client_id], X_test_1[client_id], X_test_2[client_id], Y_test_csv[client_id])\n",
    "        trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        valloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "        # Instantiate your NumPyClient\n",
    "        numpy_client = FlowerClient(client_id, net, trainloader, valloader)\n",
    "        \n",
    "        # NEW: Convert it to a Flower Client and return\n",
    "        return numpy_client.to_client()\n",
    "        \n",
    "    # --- Prepare the data for the server's evaluation function ---\n",
    "    # Create a tuple containing all the test set arrays for every client\n",
    "    server_test_data = (X_test_csv, X_test_1, X_test_2, Y_test_csv)\n",
    "    \n",
    "    # --- Server-side Evaluation (optional but good practice) ---\n",
    "    # In your main function, where you define get_evaluate_fn\n",
    "\n",
    "    def get_evaluate_fn(test_data_splits):\n",
    "        \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "        def evaluate(server_round: int, parameters: fl.common.NDArrays, config: dict):\n",
    "            net = ModelCSVIMG(num_csv_features=test_data_splits[0][0].shape[1], img_shape1=32, img_shape2=32).to(DEVICE)\n",
    "\n",
    "            server_test_cid = TOTAL_CLIENTS - 1\n",
    "            params_dict = zip(net.state_dict().keys(), parameters)\n",
    "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "            net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "            test_dataset = CustomDatasetRes(test_data_splits[0][server_test_cid], test_data_splits[1][server_test_cid],\n",
    "                                          test_data_splits[2][server_test_cid], test_data_splits[3][server_test_cid])\n",
    "            testloader = DataLoader(test_dataset, batch_size=32)\n",
    "            loss, accuracy = test(net, testloader)\n",
    "            \n",
    "            # NEW: More prominent end-of-round summary\n",
    "            print(\"=\"*60)\n",
    "            print(f\"✅ ROUND {server_round} SUMMARY - Global Model Performance:\")\n",
    "            print(f\"   Loss: {loss:.4f} | Accuracy: {accuracy:.4f}\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            return loss, {\"accuracy\": accuracy}\n",
    "        return evaluate\n",
    "\n",
    "    # --- Instantiate and use the ParetoStrategy ---\n",
    "    # In your main function\n",
    "    strategy = ParetoStrategy(\n",
    "        fraction_fit=0.5,\n",
    "        min_fit_clients=4,\n",
    "        fraction_evaluate=0.0,      # <-- CORRECTED\n",
    "        min_evaluate_clients=0,     # <-- CORRECTED\n",
    "        min_available_clients=TOTAL_CLIENTS,\n",
    "        evaluate_fn=get_evaluate_fn(server_test_data),\n",
    "    )\n",
    "\n",
    "    # --- Start Simulation ---\n",
    "    print(\"✅ Starting Flower simulation with Pareto Strategy...\")\n",
    "    \n",
    "    # 1. CAPTURE the History object returned by the simulation\n",
    "    history = fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=TOTAL_CLIENTS,\n",
    "        config=fl.server.ServerConfig(num_rounds=10),\n",
    "        strategy=strategy,\n",
    "        client_resources={\"num_cpus\": 4, \"num_gpus\": 0.25 if torch.cuda.is_available() else 0},\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- NEW: Process, Print, and Save the Final Results ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"              SIMULATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 2. EXTRACT loss and accuracy from the History object\n",
    "    # The history object contains metrics from the server-side evaluation (centralized)\n",
    "    try:\n",
    "        # Get accuracy values from history\n",
    "        rounds, accuracies = zip(*history.metrics_centralized[\"accuracy\"])\n",
    "        # Get loss values from history\n",
    "        _, losses = zip(*history.losses_centralized)\n",
    "\n",
    "        # 3. CREATE a pandas DataFrame\n",
    "        summary_df = pd.DataFrame({\n",
    "            \"Round\": rounds,\n",
    "            \"Loss\": losses,\n",
    "            \"Accuracy\": accuracies\n",
    "        })\n",
    "\n",
    "        # 4. PRINT the summary table\n",
    "        print(\"\\n📈 Global Model Performance Summary:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "\n",
    "        # 5. SAVE the summary to a CSV file\n",
    "        csv_filename = \"simulation_summary.csv\"\n",
    "        summary_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"\\n✅ Summary successfully saved to: {csv_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Could not generate summary. No centralized metrics found. Error: {e}\")\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure to copy all the helper functions and classes defined above\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337d49e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
