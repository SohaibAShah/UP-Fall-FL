{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0221b1e3",
   "metadata": {},
   "source": [
    "## Step 1: The Foundation (Imports and Setup)\n",
    "Every Python script starts with importing the necessary libraries and setting up the environment. This is like laying the foundation for a house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696a41f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "# -- Basic Setup --\n",
    "# Set the device to use the GPU if available, otherwise use the CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b56c2",
   "metadata": {},
   "source": [
    "### Define dataset loader\n",
    "\n",
    "PyTorch uses a Dataset object to handle data loading. Since our model will take three different kinds of input (sensor data, image 1, and image 2), we need to create a special class that tells PyTorch how to retrieve one sample of each, along with its corresponding label.\n",
    "\n",
    "This class will have three essential methods:\n",
    "\n",
    "__init__: Initializes the dataset by storing our feature and label arrays.\n",
    "\n",
    "__len__: Returns the total number of samples in the dataset.\n",
    "\n",
    "__getitem__: Fetches a single data sample at a given index.\n",
    "\n",
    "Here is the code for it. Add this to your script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ebd8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset loader\n",
    "class CustomDatasetRes(Dataset):\n",
    "    def __init__(self, features1, features2, features3, labels):\n",
    "        self.features1 = features1\n",
    "        self.features2 = features2\n",
    "        self.features3 = features3\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.features1[index], self.features2[index], self.features3[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03702557",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "Next, we'll add a few helper functions. These functions will perform common tasks that we'll need later, like displaying results, scaling data, and ensuring our experiments are reproducible.\n",
    "\n",
    "1. display_result\n",
    "\n",
    "This function takes the true labels (y_test) and the model's predicted labels (y_pred) and prints out standard performance metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6414e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result(y_test, y_pred):\n",
    "    print('Accuracy score : ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision score : ', precision_score(y_test, y_pred, average='weighted'))\n",
    "    print('Recall score : ', recall_score(y_test, y_pred, average='weighted'))\n",
    "    print('F1 score : ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a5531",
   "metadata": {},
   "source": [
    "2. scaled_data\n",
    "\n",
    "This function uses Scikit-learn's StandardScaler to normalize the sensor (CSV) data. Scaling is crucial because it ensures that features with larger value ranges don't dominate the learning process. Notice there are two functions with the same name in the original code. In Python, the last definition of a function is the one that gets used. We will add both for completeness, but just know that the first one is effectively overwritten by the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adc0678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test, X_val):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    return X_train_scaled, X_test_scaled, X_val_scaled\n",
    "\n",
    "def scaled_data(X_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    return X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d51f72",
   "metadata": {},
   "source": [
    "3. set_seed\n",
    "\n",
    "This is a very important function for reproducibility. Machine learning involves a lot of randomness (e.g., initializing model weights, shuffling data). By setting a \"seed,\" we ensure that the sequence of random numbers is the same every time we run the code, which means we'll get the exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f4630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    # Sets the environment variable for Python's hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # Sets the seed for NumPy's random number generator\n",
    "    np.random.seed(seed)\n",
    "    # Sets the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    # Sets the seed for PyTorch's random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    # If using a GPU, sets the seed for all CUDA devices\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "    # Ensures deterministic behavior in cuDNN (CUDA Deep Neural Network library)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf1683",
   "metadata": {},
   "source": [
    "### loading and preprocessing the data.\n",
    "\n",
    "The function loadClientsData is designed for a federated learning scenario. It reads data from separate files for each participant (or \"client\"), cleans it, aligns the different data types (sensor vs. image), and splits it into training and testing sets for each client.\n",
    "\n",
    "Because this function is quite long, we'll build it in a few parts.\n",
    "\n",
    "#### Part 1: Initializing and Processing Training Data\n",
    "First, we'll define the function, list the subject IDs we want to load, and create empty dictionaries to store each client's data. Then, we'll start a loop to process each subject one by one. Inside the loop, we'll begin by loading and cleaning the training data.\n",
    "\n",
    "This involves:\n",
    "\n",
    "Reading the sensor data from a CSV file.\n",
    "\n",
    "Removing rows with missing values and any duplicate rows.\n",
    "\n",
    "Dropping columns that we don't need (like the 'Infrared' sensor readings).\n",
    "\n",
    "Loading the corresponding image, label, and timestamp data from .npy files.\n",
    "\n",
    "#### Part 2: Aligning and Preparing Training Data\n",
    "After loading the raw data, we face a common problem: the datasets don't perfectly match. Because we dropped rows with missing values from the sensor (CSV) data, there are now timestamps in our image data that no longer have a corresponding entry in the sensor data.\n",
    "\n",
    "We need to align them by removing the image samples that don't have a matching sensor reading.\n",
    "\n",
    "After alignment, we'll prepare the data for the model:\n",
    "\n",
    "Set the seed for reproducibility.\n",
    "\n",
    "Separate features from labels.\n",
    "\n",
    "One-hot encode the labels, converting them into a format suitable for the model's output layer (e.g., class 3 becomes [0, 0, 0, 1, 0, ...]).\n",
    "\n",
    "Scale the numeric sensor data and the image pixel values.\n",
    "\n",
    "Reshape the images to the format expected by the convolutional layers.\n",
    "\n",
    "#### Part 3: Processing the Test Data and Finalizing the Function\n",
    "The logic here is identical to what we just did for the training data:\n",
    "\n",
    "Load the test sensor data (_test.csv) and test image data (_test.npy).\n",
    "\n",
    "Clean the sensor data by removing missing values and unnecessary columns.\n",
    "\n",
    "Align the test image data with the cleaned test sensor data.\n",
    "\n",
    "Prepare the aligned test data (one-hot encode labels, scale features, reshape images).\n",
    "\n",
    "Store all the processed training and test arrays into our dictionaries.\n",
    "\n",
    "Increment the clint_index and repeat the process for the next subject.\n",
    "\n",
    "After the loop finishes, the function returns all the dictionaries containing the data for every client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f6095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadClientsData():\n",
    "    subs = [1, 3, 4, 7, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "    X_train_csv_scaled_splits = {}\n",
    "    X_test_csv_scaled_splits = {}\n",
    "    Y_train_csv_splits = {}\n",
    "    Y_test_csv_splits = {}\n",
    "    X_train_1_scaled_splits = {}\n",
    "    X_test_1_scaled_splits = {}\n",
    "    Y_train_1_splits = {}\n",
    "    Y_test_1_splits = {}\n",
    "    X_train_2_scaled_splits = {}\n",
    "    X_test_2_scaled_splits = {}\n",
    "    Y_train_2_splits = {}\n",
    "    Y_test_2_splits = {}\n",
    "    clint_index = 0\n",
    "    for sub_ in subs:\n",
    "        # --- Load and clean TRAINING sensor data (CSV) ---\n",
    "        SUB_train = pd.read_csv('./dataset/Sensor + Image/{}_sensor_train.csv'.format(sub_), skiprows=1)\n",
    "        SUB_train.head()\n",
    "        \n",
    "        SUB_train.isnull().sum()\n",
    "        NA_cols = SUB_train.columns[SUB_train.isnull().any()]\n",
    "        SUB_train.dropna(inplace=True)\n",
    "        SUB_train.drop_duplicates(inplace=True)\n",
    "        \n",
    "        times_train = SUB_train['Time']\n",
    "        list_DROP = ['Infrared 1',\n",
    "                     'Infrared 2',\n",
    "                     'Infrared 3',\n",
    "                     'Infrared 4',\n",
    "                     'Infrared 5',\n",
    "                     'Infrared 6']\n",
    "        SUB_train.drop(list_DROP, axis=1, inplace=True)\n",
    "        SUB_train.drop(NA_cols, axis=1, inplace=True)  # drop NAN COLS\n",
    "\n",
    "        SUB_train.set_index('Time', inplace=True)\n",
    "        SUB_train.head()\n",
    "\n",
    "        # --- Load TRAINING image data from both cameras ---\n",
    "        cam = '1'\n",
    "        image_train = './dataset/Sensor + Image' + '/' + '{}_image_1_train.npy'.format(sub_)\n",
    "        name_train = './dataset/Sensor + Image' + '/' + '{}_name_1_train.npy'.format(sub_)\n",
    "        label_train = './dataset/Sensor + Image' + '/' + '{}_label_1_train.npy'.format(sub_)\n",
    "\n",
    "        img_1_train = np.load(image_train)\n",
    "        label_1_train = np.load(label_train)\n",
    "        name_1_train = np.load(name_train)\n",
    "\n",
    "        cam = '2'\n",
    "        image_train = './dataset/Sensor + Image' + '/' + '{}_image_2_train.npy'.format(sub_)\n",
    "        name_train = './dataset/Sensor + Image' + '/' + '{}_name_2_train.npy'.format(sub_)\n",
    "        label_train = './dataset/Sensor + Image' + '/' + '{}_label_2_train.npy'.format(sub_)\n",
    "\n",
    "        img_2_train = np.load(image_train)\n",
    "        label_2_train = np.load(label_train)\n",
    "        name_2_train = np.load(name_train)\n",
    "\n",
    "        # --- Align the training data by removing samples not present in the cleaned CSV ---\n",
    "        redundant_1 = list(set(name_1_train) - set(times_train))\n",
    "        redundant_2 = list(set(name_2_train) - set(times_train))\n",
    "        \n",
    "        ind = np.arange(0, len(img_1_train))\n",
    "\n",
    "        red_in1 = ind[np.isin(name_1_train, redundant_1)]\n",
    "        name_1_train = np.delete(name_1_train, red_in1)\n",
    "        img_1_train = np.delete(img_1_train, red_in1, axis=0)\n",
    "        label_1_train = np.delete(label_1_train, red_in1)\n",
    "\n",
    "        red_in2 = ind[np.isin(name_2_train, redundant_2)]\n",
    "        name_2_train = np.delete(name_2_train, red_in2)\n",
    "        img_2_train = np.delete(img_2_train, red_in2, axis=0)\n",
    "        label_2_train = np.delete(label_2_train, red_in2)\n",
    "        \n",
    "        # --- Prepare the final aligned training data ---\n",
    "        data_train = SUB_train.loc[name_1_train].values\n",
    "\n",
    "        set_seed()\n",
    "        X_csv_train, y_csv_train = data_train[:, :-1], data_train[:, -1]\n",
    "        \n",
    "        # Remap label 20 to 0 for consistency\n",
    "        y_csv_train = np.where(y_csv_train == 20, 0, y_csv_train)\n",
    "        label_1_train = np.where(label_1_train == 20, 0, label_1_train)\n",
    "        label_2_train = np.where(label_2_train == 20, 0, label_2_train)\n",
    "\n",
    "        # One-hot encode the labels for PyTorch\n",
    "        Y_csv_train = torch.nn.functional.one_hot(torch.from_numpy(y_csv_train).long(), 12).float()\n",
    "        Y_train_1 = torch.nn.functional.one_hot(torch.from_numpy(label_1_train).long(), 12).float()\n",
    "        Y_train_2 = torch.nn.functional.one_hot(torch.from_numpy(label_2_train).long(), 12).float()\n",
    "\n",
    "        # Scale the sensor data\n",
    "        X_csv_train_scaled = scaled_data(X_csv_train)\n",
    "\n",
    "        X_train_1 = img_1_train\n",
    "        y_train_1 = label_1_train\n",
    "        \n",
    "        X_train_2 = img_2_train\n",
    "        y_train_2 = label_2_train\n",
    "\n",
    "        # Reshape images to (samples, height, width, channels)\n",
    "        shape1, shape2 = 32, 32\n",
    "        X_train_1 = X_train_1.reshape(X_train_1.shape[0], shape1, shape2, 1)\n",
    "        X_train_2 = X_train_2.reshape(X_train_2.shape[0], shape1, shape2, 1)\n",
    "\n",
    "        # Scale image pixel values to be between 0 and 1\n",
    "        X_train_1_scaled = X_train_1 / 255.0\n",
    "        X_train_2_scaled = X_train_2 / 255.0\n",
    "\n",
    "        # --- Load and clean TEST sensor data (CSV) ---\n",
    "        SUB_test = pd.read_csv('./dataset/Sensor + Image/{}_sensor_test.csv'.format(sub_), skiprows=1)\n",
    "        SUB_test.head()\n",
    "        \n",
    "        SUB_test.isnull().sum()\n",
    "        NA_cols = SUB_test.columns[SUB_test.isnull().any()]\n",
    "        SUB_test.dropna(inplace=True)\n",
    "        SUB_test.drop_duplicates(inplace=True)\n",
    "\n",
    "        times_test = SUB_test['Time']\n",
    "        SUB_test.drop(list_DROP, axis=1, inplace=True)\n",
    "        SUB_test.drop(NA_cols, axis=1, inplace=True)\n",
    "\n",
    "        SUB_test.set_index('Time', inplace=True)\n",
    "        SUB_test.head()\n",
    "\n",
    "        # --- Load TEST image data from both cameras ---\n",
    "        image_test = './dataset/Sensor + Image' + '/' + '{}_image_1_test.npy'.format(sub_)\n",
    "        name_test = './dataset/Sensor + Image' + '/' + '{}_name_1_test.npy'.format(sub_)\n",
    "        label_test = './dataset/Sensor + Image' + '/' + '{}_label_1_test.npy'.format(sub_)\n",
    "        img_1_test = np.load(image_test)\n",
    "        label_1_test = np.load(label_test)\n",
    "        name_1_test = np.load(name_test)\n",
    "\n",
    "        image_test = './dataset/Sensor + Image' + '/' + '{}_image_2_test.npy'.format(sub_)\n",
    "        name_test = './dataset/Sensor + Image' + '/' + '{}_name_2_test.npy'.format(sub_)\n",
    "        label_test = './dataset/Sensor + Image' + '/' + '{}_label_2_test.npy'.format(sub_)\n",
    "        img_2_test = np.load(image_test)\n",
    "        label_2_test = np.load(label_test)\n",
    "        name_2_test = np.load(name_test)\n",
    "\n",
    "        # --- Align the test data ---\n",
    "        redundant_1 = list(set(name_1_test) - set(times_test))\n",
    "        redundant_2 = list(set(name_2_test) - set(times_test))\n",
    "        \n",
    "        ind = np.arange(0, len(img_1_test))\n",
    "\n",
    "        red_in1 = ind[np.isin(name_1_test, redundant_1)]\n",
    "        name_1_test = np.delete(name_1_test, red_in1)\n",
    "        img_1_test = np.delete(img_1_test, red_in1, axis=0)\n",
    "        label_1_test = np.delete(label_1_test, red_in1)\n",
    "\n",
    "        red_in2 = ind[np.isin(name_2_test, redundant_2)]\n",
    "        name_2_test = np.delete(name_2_test, red_in2)\n",
    "        img_2_test = np.delete(img_2_test, red_in2, axis=0)\n",
    "        label_2_test = np.delete(label_2_test, red_in2)\n",
    "\n",
    "        # --- Prepare the final aligned test data ---\n",
    "        data_test = SUB_test.loc[name_1_test].values\n",
    "\n",
    "        set_seed()\n",
    "        X_csv_test, y_csv_test = data_test[:, :-1], data_test[:, -1]\n",
    "        y_csv_test = np.where(y_csv_test == 20, 0, y_csv_test)\n",
    "        label_1_test = np.where(label_1_test == 20, 0, label_1_test)\n",
    "        label_2_test = np.where(label_2_test == 20, 0, label_2_test)\n",
    "\n",
    "        Y_csv_test = torch.nn.functional.one_hot(torch.from_numpy(y_csv_test).long(), 12).float()\n",
    "        X_csv_test_scaled = scaled_data(X_csv_test)\n",
    "        \n",
    "        X_test_1 = img_1_test\n",
    "        y_test_1 = label_1_test\n",
    "        Y_test_1 = torch.nn.functional.one_hot(torch.from_numpy(y_test_1).long(), 12).float()\n",
    "\n",
    "        X_test_2 = img_2_test\n",
    "        y_test_2 = label_2_test\n",
    "        Y_test_2 = torch.nn.functional.one_hot(torch.from_numpy(y_test_2).long(), 12).float()\n",
    "\n",
    "        X_test_1 = X_test_1.reshape(X_test_1.shape[0], shape1, shape2, 1)\n",
    "        X_test_2 = X_test_2.reshape(X_test_2.shape[0], shape1, shape2, 1)\n",
    "\n",
    "        X_test_1_scaled = X_test_1 / 255.0\n",
    "        X_test_2_scaled = X_test_2 / 255.0\n",
    "\n",
    "        # --- Store all processed data for the current client ---\n",
    "        X_train_csv_scaled_splits[clint_index] = X_csv_train_scaled\n",
    "        X_test_csv_scaled_splits[clint_index] = X_csv_test_scaled\n",
    "        Y_train_csv_splits[clint_index] = Y_csv_train\n",
    "        Y_test_csv_splits[clint_index] = Y_csv_test\n",
    "        X_train_1_scaled_splits[clint_index] = X_train_1_scaled\n",
    "        X_test_1_scaled_splits[clint_index] = X_test_1_scaled\n",
    "        Y_train_1_splits[clint_index] = Y_train_1\n",
    "        Y_test_1_splits[clint_index] = Y_test_1\n",
    "        X_train_2_scaled_splits[clint_index] = X_train_2_scaled # This line had a bug in the original code\n",
    "        X_test_2_scaled_splits[clint_index] = X_test_2_scaled\n",
    "        Y_train_2_splits[clint_index] = Y_train_2\n",
    "        Y_test_2_splits[clint_index] = Y_test_2\n",
    "        clint_index += 1\n",
    "        \n",
    "    # --- After loop, return all dictionaries ---\n",
    "    return X_train_csv_scaled_splits,X_test_csv_scaled_splits, Y_train_csv_splits,Y_test_csv_splits,X_train_1_scaled_splits,X_test_1_scaled_splits,Y_train_1_splits,Y_test_1_splits,X_train_2_scaled_splits,X_test_2_scaled_splits,Y_train_2_splits,Y_test_2_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e1b50",
   "metadata": {},
   "source": [
    "## Step 2: Client Selection\n",
    "\n",
    "We're making great progress. We've handled all the data loading and preparation. Now, we'll add the functions that form the \"intelligence\" of our federated learning system: client selection.\n",
    "\n",
    "Instead of blindly averaging updates from every client in every round, these methods evaluate each client's performance and contribution. This allows the server to select the most promising or reliable clients to participate in the global model update, potentially leading to faster convergence and a more robust final model.\n",
    "\n",
    "We'll add a series of functions, each calculating a specific metric to judge the clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5173b0f8",
   "metadata": {},
   "source": [
    "### Client Evaluation Metrics\n",
    "Add all the following functions to your script. Each one calculates a different score based on a client's performance.\n",
    "\n",
    "1. Relative Loss Reduction (RF_loss)\n",
    "\n",
    "This measures how much a client's training loss has dropped from the beginning to the end of a local training round, relative to the client with the biggest drop. A higher score means the client is learning effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca768194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_loss_reduction_as_list(client_losses):\n",
    "    \"\"\"\n",
    "    Calculates the relative loss reduction (RF_loss) for each client.\n",
    "    \"\"\"\n",
    "    loss_reduction = {}\n",
    "    for client_id, losses in client_losses.items():\n",
    "        if len(losses) < 2:\n",
    "            raise ValueError(f\"Client {client_id} has less than 2 loss values, cannot calculate RF_loss.\")\n",
    "        loss_start = losses[0]\n",
    "        loss_end = losses[-1]\n",
    "        loss_reduction[client_id] = loss_start - loss_end\n",
    "\n",
    "    max_loss_reduction = max(loss_reduction.values())\n",
    "    if max_loss_reduction == 0:\n",
    "        return [0.0] * len(loss_reduction)  # If no loss reduction, return 0.0 for all clients\n",
    "\n",
    "    rf_losses_list = [\n",
    "        reduction / max_loss_reduction for reduction in loss_reduction.values()\n",
    "    ]\n",
    "    return rf_losses_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f48d0a",
   "metadata": {},
   "source": [
    "2. Relative Training Accuracy (RF_ACC_Train)\n",
    "\n",
    "This measures a client's local training accuracy relative to the client with the highest accuracy. It's a straightforward measure of performance on local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a82d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_train_accuracy(client_acc):\n",
    "    \"\"\"\n",
    "    Calculates the relative training accuracy (RF_Acc_Train) for each client.\n",
    "    \"\"\"\n",
    "    max_acc = max(client_acc.values())\n",
    "    if max_acc == 0:\n",
    "        return [0.0] * len(client_acc)  # If no accuracy, return 0.0 for all clients\n",
    "\n",
    "    rf_accs_train_list = [\n",
    "        acc / max_acc for acc in client_acc.values()\n",
    "    ]\n",
    "    return rf_accs_train_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e977c2",
   "metadata": {},
   "source": [
    "3. Global Validation Accuracy (RF_ACC_Global)\n",
    "\n",
    "This is a more sophisticated metric. It rewards clients for high accuracy on a global test set but penalizes them if their global accuracy is much worse than their local training accuracy (which is a sign of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3b4108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_global_validation_accuracy(train_acc, global_acc):\n",
    "    \"\"\"\n",
    "    Calculates the global validation accuracy (RF_Acc_Global) based on local training accuracies.\n",
    "    \"\"\"\n",
    "    if set(train_acc.keys()) != set(global_acc.keys()):\n",
    "        raise ValueError(\"Client IDs for train and global accuracy do not match.\")\n",
    "\n",
    "    max_global_acc = max(global_acc.values())\n",
    "    if max_global_acc == 0:\n",
    "        max_global_acc = 1  # Avoid division by zero\n",
    "\n",
    "    global_train_diff = {\n",
    "        client_id: train_acc[client_id] - global_acc[client_id]\n",
    "        for client_id in train_acc\n",
    "    }\n",
    "    max_global_train_diff = max(global_train_diff.values())\n",
    "    if max_global_train_diff == 0:\n",
    "        max_global_train_diff = 1  # Avoid division by zero\n",
    "\n",
    "    rf_acc_global_list = [\n",
    "        (global_acc[client_id] / max_global_acc) - (global_train_diff[client_id] / max_global_train_diff)\n",
    "        for client_id in train_acc\n",
    "    ]\n",
    "    return rf_acc_global_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26f9bf",
   "metadata": {},
   "source": [
    "4. Loss Outliers (P_loss)\n",
    "\n",
    "This function flags clients that are potential negative contributors. If a client's final training loss is significantly higher than the average loss of all clients, it gets a high penalty score. Otherwise, its penalty is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35c66dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_outliers(client_losses, lambda_loss=1.5):\n",
    "    \"\"\"\n",
    "    Calculates the loss outlier penalty (P_loss) for each client.\n",
    "    \"\"\"\n",
    "    final_losses = {client_id: losses[-1] for client_id, losses in client_losses.items()}\n",
    "    loss_values = np.array(list(final_losses.values()))\n",
    "\n",
    "    mean_loss = np.mean(loss_values)\n",
    "    std_loss = np.std(loss_values)\n",
    "\n",
    "    threshold = mean_loss + lambda_loss * std_loss\n",
    "\n",
    "    max_loss = np.max(loss_values)\n",
    "\n",
    "    if max_loss == 0:\n",
    "        return [0.0] * len(loss_values)\n",
    "\n",
    "    # Identify outliers\n",
    "    loss_outliers = [\n",
    "        final_loss / max_loss if final_loss > threshold else 0.0\n",
    "        for final_loss in loss_values\n",
    "    ]\n",
    "    return loss_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96a2ac",
   "metadata": {},
   "source": [
    "5. Performance Bias (P_bias)\n",
    "\n",
    "This metric calculates the gap between a client's performance on its own validation data versus its performance on the global validation data. A large gap might indicate that the client's local data is not representative of the overall data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc704c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_bias(val_acc, global_acc):\n",
    "    \"\"\"\n",
    "    Calculates the performance bias penalty (P_bias).\n",
    "    \"\"\"\n",
    "    if set(val_acc.keys()) != set(global_acc.keys()):\n",
    "        raise ValueError(\"Client IDs for validation and global accuracy do not match.\")\n",
    "\n",
    "    performance_bias_list = []\n",
    "    for client_id in val_acc:\n",
    "        val = val_acc[client_id]\n",
    "        global_val = global_acc[client_id]\n",
    "        max_val = max(val, global_val)\n",
    "\n",
    "        if max_val == 0:\n",
    "            performance_bias = 0\n",
    "        else:\n",
    "            performance_bias = abs(val - global_val) / max_val\n",
    "        performance_bias_list.append(performance_bias)\n",
    "\n",
    "    return performance_bias_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfd596",
   "metadata": {},
   "source": [
    "Excellent. Now that we have the functions to score each client, we need the final step: the algorithms that use these scores to select which clients will participate in a given round."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cecd1",
   "metadata": {},
   "source": [
    "### Client Selection Algorithms\n",
    "1. Pareto Optimization\n",
    "\n",
    "This is a powerful technique used when you have multiple, often conflicting, objectives. Instead of combining all metrics into one score, it tries to find a set of clients that represent the best possible trade-offs.\n",
    "\n",
    "A client is considered \"Pareto optimal\" if no other client is better than it across all metrics. The algorithm first finds this set of optimal clients.\n",
    "\n",
    "If there are more optimal clients than needed, it selects a random subset.\n",
    "\n",
    "If there are fewer, it fills the remaining spots by picking the clients with the best-combined performance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f90e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_optimization(\n",
    "    rf_loss, rf_acc_train, rf_acc_val, rf_acc_global, p_loss, p_bias, client_num\n",
    "):\n",
    "    \"\"\"\n",
    "    实现 Pareto 优化，筛选节点。\n",
    "\n",
    "    参数：\n",
    "    - rf_loss (list): 局部训练损失相对下降幅度。\n",
    "    - rf_acc_train (list): 局部训练精度。\n",
    "    - rf_acc_val (list): 局部验证精度。\n",
    "    - rf_acc_global (list): 全局验证精度。\n",
    "    - p_loss (list): 损失异常。\n",
    "    - p_bias (list): 性能偏离。\n",
    "    - client_num (int): 要选出的节点数。\n",
    "\n",
    "    返回：\n",
    "    - selected_clients (list): 选中的 client ID（按输入顺序从 0 开始）。\n",
    "    \"\"\"\n",
    "    # 将输入指标整合为二维数组，便于处理\n",
    "    # data = np.array([rf_loss, rf_acc_train, rf_acc_val, rf_acc_global, -np.array(p_loss), -np.array(p_bias)]).T\n",
    "\n",
    "    # 确保所有数组中的元素都转换为 NumPy 数组\n",
    "    # rf_loss = np.array([x.detach().cpu().numpy() for x in rf_loss])\n",
    "    rf_loss = np.array(list(rf_loss))\n",
    "    rf_acc_train = rf_acc_train.detach().cpu().numpy() if isinstance(rf_acc_train, torch.Tensor) else np.array(\n",
    "        rf_acc_train)\n",
    "    rf_acc_val = rf_acc_val.detach().cpu().numpy() if isinstance(rf_acc_val, torch.Tensor) else np.array(rf_acc_val)\n",
    "    rf_acc_global = rf_acc_global.detach().cpu().numpy() if isinstance(rf_acc_global, torch.Tensor) else np.array(\n",
    "        rf_acc_global)\n",
    "    p_loss = p_loss.detach().cpu().numpy() if isinstance(p_loss, torch.Tensor) else np.array(p_loss)\n",
    "    p_bias = p_bias.detach().cpu().numpy() if isinstance(p_bias, torch.Tensor) else np.array(p_bias)\n",
    "    # rf_acc_train = np.array([x.detach().cpu().numpy() for x in rf_acc_train])\n",
    "    # rf_acc_val = np.array([x.detach().cpu().numpy() for x in rf_acc_val])\n",
    "    # rf_acc_global = np.array([x.detach().cpu().numpy() for x in rf_acc_global])\n",
    "    # p_loss = np.array([x.detach().cpu().numpy() for x in p_loss])\n",
    "    # p_bias = np.array([x.detach().cpu().numpy() for x in p_bias])\n",
    "\n",
    "    # 构造 NumPy 数组并转置\n",
    "    data = np.array([rf_loss, rf_acc_train, rf_acc_val, rf_acc_global, -p_loss, -p_bias]).T\n",
    "\n",
    "    # Pareto 前沿筛选\n",
    "    def is_dominated(point, others):\n",
    "        \"\"\"判断 point 是否被 others 支配\"\"\"\n",
    "        return any(np.all(other >= point) and np.any(other > point) for other in others)\n",
    "\n",
    "    pareto_indices = [\n",
    "        i for i, point in enumerate(data) if not is_dominated(point, np.delete(data, i, axis=0))\n",
    "    ]\n",
    "    pareto_clients = pareto_indices\n",
    "\n",
    "    # 如果前沿节点数多于 client_num，随机选取\n",
    "    if len(pareto_clients) > client_num:\n",
    "        return random.sample(pareto_clients, client_num)\n",
    "\n",
    "    # 如果前沿节点数小于 client_num，基于组合评分补充\n",
    "    remaining_slots = client_num - len(pareto_clients)\n",
    "    pareto_scores = [0.4 * rf_loss[i] + 0.6 * rf_acc_global[i] for i in range(len(rf_loss))]\n",
    "    sorted_indices = np.argsort(pareto_scores)[::-1]  # 按评分从高到低排序\n",
    "\n",
    "    selected_clients = set(pareto_clients)\n",
    "    for i in sorted_indices:\n",
    "        if len(selected_clients) >= client_num:\n",
    "            break\n",
    "        if i not in selected_clients:\n",
    "            selected_clients.add(i)\n",
    "\n",
    "    return list(selected_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebf65c",
   "metadata": {},
   "source": [
    "2. Weighted Sum Method (5RF)\n",
    "\n",
    "This is a more straightforward approach. It calculates a single comprehensive score for each client by taking a weighted sum of all the metrics. Clients with the highest final scores are selected. The weights (0.2, 0.1, 0.3, etc.) determine the importance of each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb04b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_clients_with5RF(rf_loss, rf_acc_train, rf_acc_val, rf_acc_global, p_loss, p_bias, client_num):\n",
    "    rf_loss = np.array(list(rf_loss))\n",
    "    rf_acc_train = np.array(rf_acc_train)\n",
    "    rf_acc_val = np.array(rf_acc_val)\n",
    "    rf_acc_global = np.array(rf_acc_global)\n",
    "    p_loss = np.array(p_loss)\n",
    "    p_bias = np.array(p_bias)\n",
    "\n",
    "    # Calculate a single weighted score for each client\n",
    "    scores = (\n",
    "            0.2 * rf_loss +\n",
    "            0.1 * rf_acc_train +\n",
    "            0.2 * rf_acc_val +\n",
    "            0.3 * rf_acc_global -\n",
    "            0.1 * p_loss -\n",
    "            0.1 * p_bias\n",
    "    )\n",
    "    origin_scores = scores\n",
    "    # Get the indices of the clients with the highest scores\n",
    "    top_client_ids = np.argsort(scores)[::-1][:client_num]  # Sort descending and take the top N\n",
    "    return top_client_ids.tolist(), origin_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee125dce",
   "metadata": {},
   "source": [
    "## Step 2: The AI's Brain (The Model Definition)\n",
    "We have the data pipeline and the client selection logic. Now it's time to build the brain of the operation: the neural network model itself.\n",
    "\n",
    "The model, ModelCSVIMG, is a multi-modal neural network. This means it's designed to accept and process multiple types of data at once. It has three distinct input branches:\n",
    "\n",
    "One for the numerical sensor (CSV) data.\n",
    "\n",
    "One for the images from camera 1.\n",
    "\n",
    "One for the images from camera 2.\n",
    "\n",
    "The features extracted from each branch are then combined (fused) and passed to a final set of layers that perform the classification. The original code contains a few versions of the architecture; we will use the final, most complex one.\n",
    "\n",
    "Add the complete model class to your script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "218bbd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCSVIMG(nn.Module):\n",
    "    def __init__(self, num_csv_features, img_shape1, img_shape2):\n",
    "        super(ModelCSVIMG, self).__init__()\n",
    "\n",
    "        # --- Branch 1: For processing numerical CSV data ---\n",
    "        self.csv_fc_1 = nn.Linear(num_csv_features, 2000)\n",
    "        self.csv_bn_1 = nn.BatchNorm1d(2000)\n",
    "        self.csv_fc_2 = nn.Linear(2000, 600)\n",
    "        self.csv_bn_2 = nn.BatchNorm1d(600)\n",
    "        self.csv_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # --- Branch 2: For processing images from Camera 1 (CNN) ---\n",
    "        self.img1_conv_1 = nn.Conv2d(in_channels=1, out_channels=18, kernel_size=3, stride=1, padding=1)\n",
    "        self.img1_batch_norm = nn.BatchNorm2d(18)\n",
    "        self.img1_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Flattened features from the CNN go into a fully connected layer\n",
    "        self.img1_fc1 = nn.Linear(18 * 16 * 16, 100)\n",
    "        self.img1_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # --- Branch 3: For processing images from Camera 2 (identical to Branch 2) ---\n",
    "        self.img2_conv = nn.Conv2d(in_channels=1, out_channels=18, kernel_size=3, stride=1, padding=1)\n",
    "        self.img2_batch_norm = nn.BatchNorm2d(18)\n",
    "        self.img2_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.img2_fc1 = nn.Linear(18 * 16 * 16, 100)\n",
    "        self.img2_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # --- Fusion and Final Classification Layers ---\n",
    "        # The input size is 600 (from CSV) + 100 (from Image 1) + 100 (from Image 2) = 800\n",
    "        self.fc1 = nn.Linear(800, 1200)\n",
    "        self.dr1 = nn.Dropout(0.2)\n",
    "        # A residual connection is used here: input to fc2 is the original 800 + output of fc1 (1200) = 2000\n",
    "        self.fc2 = nn.Linear(2000, 12) # 12 output classes\n",
    "\n",
    "    def forward(self, x_csv, x_img1, x_img2):\n",
    "        # --- Process CSV data ---\n",
    "        x_csv = F.relu(self.csv_bn_1(self.csv_fc_1(x_csv)))\n",
    "        x_csv = F.relu(self.csv_bn_2(self.csv_fc_2(x_csv)))\n",
    "        x_csv = self.csv_dropout(x_csv)\n",
    "\n",
    "        # --- Process Image 1 data ---\n",
    "        # Reshape image from (batch, height, width, channels) to (batch, channels, height, width)\n",
    "        x_img1 = x_img1.permute(0, 3, 1, 2)\n",
    "        x_img1 = F.relu(self.img1_conv_1(x_img1))\n",
    "        x_img1 = self.img1_batch_norm(x_img1)\n",
    "        x_img1 = self.img1_pool(x_img1)\n",
    "        x_img1 = x_img1.contiguous().view(x_img1.size(0), -1) # Flatten\n",
    "        x_img1 = F.relu(self.img1_fc1(x_img1))\n",
    "        x_img1 = self.img1_dropout(x_img1)\n",
    "\n",
    "        # --- Process Image 2 data ---\n",
    "        x_img2 = x_img2.permute(0, 3, 1, 2)\n",
    "        x_img2 = F.relu(self.img2_conv(x_img2))\n",
    "        x_img2 = self.img2_batch_norm(x_img2)\n",
    "        x_img2 = self.img2_pool(x_img2)\n",
    "        x_img2 = x_img2.contiguous().view(x_img2.size(0), -1) # Flatten\n",
    "        x_img2 = F.relu(self.img2_fc1(x_img2))\n",
    "        x_img2 = self.img2_dropout(x_img2)\n",
    "\n",
    "        # --- Fusion ---\n",
    "        x = torch.cat((x_csv, x_img1, x_img2), dim=1)\n",
    "        residual = x # Keep a copy for the residual connection\n",
    "        \n",
    "        # --- Final layers ---\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dr1(x)\n",
    "        # Concatenate the residual connection\n",
    "        x = torch.cat((residual, x), dim=1)\n",
    "        # Final output with softmax for classification\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f10c36",
   "metadata": {},
   "source": [
    "## Step 3: The Teacher (The Server Class)\n",
    "Alright, we're on the home stretch. We have the data, the selection logic, and the model. Now we need to create the actors for our simulation: the Server and the Client. These two classes will control the entire federated learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e0220",
   "metadata": {},
   "source": [
    "1. The Server Class\n",
    "\n",
    "The Server is the central coordinator. Its job is to:\n",
    "\n",
    "Hold the main global model.\n",
    "\n",
    "Send the global model to the clients.\n",
    "\n",
    "Receive updates from the selected clients.\n",
    "\n",
    "Aggregate these updates to improve the global model.\n",
    "\n",
    "Evaluate the global model's performance on a held-out test set.\n",
    "\n",
    "Here is the code for the Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7c9959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server\n",
    "class Server(object):\n",
    "    def __init__(self, model, epoch_size, eval_dataset, num_clients):\n",
    "        self.global_model = model\n",
    "        self.epoch_size = epoch_size\n",
    "        self.num_clients = num_clients\n",
    "        self.serverTestDataSet = CustomDatasetRes(eval_dataset[0],eval_dataset[1],eval_dataset[2],eval_dataset[3])\n",
    "        self.eval_loader = torch.utils.data.DataLoader(self.serverTestDataSet, batch_size=epoch_size)\n",
    "\n",
    "    def model_aggregate(self, weight_accumulator):\n",
    "        # Averages the weights from the selected clients and updates the global model\n",
    "        for name, data in self.global_model.state_dict().items():\n",
    "            update_per_layer = weight_accumulator[name] * (1/self.num_clients)   # average\n",
    "            if data.type() != update_per_layer.type():\n",
    "                data.add_(update_per_layer.to(torch.int64))\n",
    "            else:\n",
    "                data.add_(update_per_layer)\n",
    "\n",
    "    def model_eval(self):\n",
    "        # Evaluates the global model on the server's test data\n",
    "        self.global_model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        dataset_size = 0\n",
    "        with torch.no_grad(): # Disable gradient calculation for evaluation\n",
    "            for batch_id, batch in enumerate(self.eval_loader):\n",
    "                data1, data2, data3, target = batch\n",
    "                dataset_size += data1.size()[0]\n",
    "\n",
    "                data1 = data1.to(device).float()\n",
    "                data2 = data2.to(device).float()\n",
    "                data3 = data3.to(device).float()\n",
    "                target = target.to(device).float()\n",
    "                \n",
    "                output = self.global_model(data1,data2,data3)\n",
    "                total_loss += nn.functional.cross_entropy(output, target, reduction='sum').item()\n",
    "\n",
    "                pred = output.detach().max(1)[1]\n",
    "                correct += pred.eq(target.detach().max(1)[1].view_as(pred)).cpu().sum().item()\n",
    "\n",
    "        acc = 100.0 * (float(correct) / float(dataset_size))\n",
    "        loss = total_loss / dataset_size\n",
    "        return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32410ea",
   "metadata": {},
   "source": [
    "2. The Client Class and Helper Functions\n",
    "\n",
    "The Client represents an individual participant. Its job is to:\n",
    "\n",
    "Receive the global model from the server.\n",
    "\n",
    "Train this model on its own local data for a few epochs.\n",
    "\n",
    "Calculate the change (the diff) between the original model and its newly trained model.\n",
    "\n",
    "Send this diff back to the server.\n",
    "\n",
    "The client's training process is handled by two helper functions: train_one_epoch and validate.\n",
    "\n",
    "Add the Client class and its two helper functions to your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a5f5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client\n",
    "class Client(object):\n",
    "    def __init__(self, epoch_size, local_epoch_per_round, train_dataset,val_dataset, id = -1):\n",
    "        self.epoch_size = epoch_size\n",
    "        self.local_epoch_per_round = local_epoch_per_round\n",
    "        self.client_id = id\n",
    "        self.train_dataset = CustomDatasetRes(train_dataset[0],train_dataset[1],train_dataset[2],train_dataset[3])\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=epoch_size,shuffle=True)\n",
    "        self.eval_dataset = CustomDatasetRes(val_dataset[0], val_dataset[1], val_dataset[2], val_dataset[3])\n",
    "        self.eval_loader = torch.utils.data.DataLoader(self.eval_dataset, batch_size=epoch_size,shuffle=False)\n",
    "\n",
    "    def local_train(self, global_model):\n",
    "        # Create a local copy of the global model\n",
    "        model = ModelCSVIMG(self.train_dataset.features1.shape[1], 32, 32)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        losses = []\n",
    "        min_loss, max_loss = float('inf'), float('-inf')\n",
    "\n",
    "        for epoch in range(self.local_epoch_per_round):\n",
    "            # Train for one epoch\n",
    "            train_loss, train_acc = train_one_epoch(model, self.train_loader, criterion, optimizer)\n",
    "            \n",
    "            # Track min/max loss and store all epoch losses for evaluation\n",
    "            if train_loss > max_loss: max_loss = train_loss\n",
    "            if train_loss < min_loss: min_loss = train_loss\n",
    "            losses.append(train_loss)\n",
    "\n",
    "        # Validate the model on the local validation set after training\n",
    "        val_loss, val_acc = validate(model, self.eval_loader, criterion)\n",
    "        print(f\"Client {self.client_id} - Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Calculate the difference between the updated local model and the original global model\n",
    "        diff = dict()\n",
    "        for name, data in model.state_dict().items():\n",
    "            diff[name] = (data - global_model.state_dict()[name])\n",
    "            \n",
    "        return model, diff, val_acc, val_loss, min_loss, max_loss, losses, train_acc\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_id, batch in enumerate(train_loader):\n",
    "        data1, data2, data3, target = batch\n",
    "        data1 = data1.to(device).float()\n",
    "        data2 = data2.to(device).float()\n",
    "        data3 = data3.to(device).float()\n",
    "        target = target.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data1, data2, data3)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * data1.size(0)\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.max(1)[1]).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch in enumerate(val_loader):\n",
    "            data1, data2, data3, target = batch\n",
    "            data1 = data1.to(device).float()\n",
    "            data2 = data2.to(device).float()\n",
    "            data3 = data3.to(device).float()\n",
    "            target = target.to(device).float()\n",
    "\n",
    "            output = model(data1, data2, data3)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            running_loss += loss.item() * data1.size(0)\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target.max(1)[1]).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de9c9f",
   "metadata": {},
   "source": [
    "We are almost there! We've built all the major components. Before we assemble everything in the main training loop, we need to add the last few helper functions.\n",
    "\n",
    "These functions are primarily used for a simpler, baseline client selection strategy (referred to as '4RF' in the code) that calculates a single score for each client and picks the best ones.\n",
    "\n",
    "Add these final utility functions to your script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5083d0",
   "metadata": {},
   "source": [
    "1. Normalize\n",
    "\n",
    "A standard function to scale any number to a range between 0 and 1, given a minimum and maximum value. This is useful for combining metrics that have different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3aec0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(value, min_value, max_value):\n",
    "    # Avoid division by zero if min and max are the same\n",
    "    if (max_value - min_value) == 0:\n",
    "        return 0\n",
    "    return (value - min_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259c438",
   "metadata": {},
   "source": [
    "2. Evaluate Model Score\n",
    "\n",
    "This function calculates a simple, combined score for a client. It's a weighted average of their performance on their local training set and a global validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beb2c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(acc, loss, min_loss, max_loss, oneclient_test_acc, oneclient_test_loss,alpha=0.8, beta=0.8):\n",
    "    normalized_loss = normalize(loss, min_loss, max_loss)\n",
    "    # Score based on local training performance\n",
    "    train_score = alpha * acc + (1 - alpha) * (1 - normalized_loss) # Use (1 - loss) so higher is better\n",
    "\n",
    "    # Score based on global validation performance\n",
    "    val_score = beta * oneclient_test_acc + (1 - beta) * (1 - oneclient_test_loss)\n",
    "\n",
    "    # Final combined score\n",
    "    combined_score = (train_score + val_score) / 2\n",
    "    return combined_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d2e60b",
   "metadata": {},
   "source": [
    "3. Get Top Clients\n",
    "\n",
    "A straightforward function that takes a dictionary of clients and their scores, then returns a list of the top num clients with the highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5f5a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_clients(client_dict, num):\n",
    "    # Sort the clients by their score (the dictionary value) in descending order\n",
    "    sorted_clients = sorted(client_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Extract just the IDs (the dictionary key) of the top clients\n",
    "    top_clients = [client[0] for client in sorted_clients[:num]]\n",
    "    return top_clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c9e3e",
   "metadata": {},
   "source": [
    "4. Dynamic Threshold Selection\n",
    "\n",
    "This is another, more advanced selection method included in the script. It selects clients whose scores are above a dynamic threshold (calculated from the mean and standard deviation of all scores). While not used in the final configuration, we include it for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8c10e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_nodes_with_dynamic_threshold(node_scores, max_nodes, std_multiplier=1.0):\n",
    "    \"\"\"\n",
    "    Selects nodes using a dynamic threshold based on score distribution.\n",
    "    \"\"\"\n",
    "    if not node_scores:\n",
    "        return []\n",
    "        \n",
    "    scores = np.array(list(node_scores.values()))\n",
    "    \n",
    "    # Calculate the dynamic threshold\n",
    "    mean_score = np.mean(scores)\n",
    "    std_dev = np.std(scores)\n",
    "    dynamic_threshold = mean_score + std_multiplier * std_dev\n",
    "\n",
    "    # Select nodes above the threshold\n",
    "    selected_nodes = [\n",
    "        node_id for node_id, score in node_scores.items() if score >= dynamic_threshold\n",
    "    ]\n",
    "\n",
    "    # If too many nodes were selected, keep only the best ones\n",
    "    if len(selected_nodes) > max_nodes:\n",
    "        selected_nodes = sorted(\n",
    "            selected_nodes, key=lambda node_id: node_scores[node_id], reverse=True\n",
    "        )[:max_nodes]\n",
    "\n",
    "    # If not enough nodes were selected, add the next best ones to meet the quota\n",
    "    if len(selected_nodes) < max_nodes:\n",
    "        remaining_nodes = [\n",
    "            node_id for node_id in node_scores if node_id not in selected_nodes\n",
    "        ]\n",
    "        remaining_nodes = sorted(\n",
    "            remaining_nodes, key=lambda node_id: node_scores[node_id], reverse=True\n",
    "        )\n",
    "        selected_nodes += remaining_nodes[: max_nodes - len(selected_nodes)]\n",
    "\n",
    "    return selected_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc43cf88",
   "metadata": {},
   "source": [
    "## Step 4: Model Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae88ddb8",
   "metadata": {},
   "source": [
    "Here we go. This is the big one. We'll now write the trainValModelCSVIMG function. This function is the conductor of our orchestra—it brings together the data, the model, the server, and the clients to run the entire federated learning simulation from start to finish.\n",
    "\n",
    "Because it's so long and important, we'll build it in three parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0daed1",
   "metadata": {},
   "source": [
    "### Part 1: Initialization and Starting the Training Loop\n",
    "\n",
    "First, we'll define the function and set everything up. This includes:\n",
    "\n",
    "Creating the global model, the Server, and all the Client objects.\n",
    "\n",
    "Initializing a series of dictionaries to log every possible metric (loss, accuracy, selection scores, etc.) for every client and every round. This is crucial for analyzing the experiment later.\n",
    "\n",
    "Starting the main training loop, which iterates through the communication rounds.\n",
    "\n",
    "Inside the loop, we'll begin the first phase of a round: every client trains locally on the current global model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2cf43",
   "metadata": {},
   "source": [
    "### Part 2: Client Selection, Aggregation, and Global Evaluation\n",
    "In this part of the trainValModelCSVIMG function, the server performs the following steps:\n",
    "\n",
    "Evaluate: It uses all the metrics gathered from the clients to calculate the advanced performance scores (RF_loss, P_bias, etc.).\n",
    "\n",
    "Select: Based on the chosen selection method (svmethod), it picks the top-performing clients for this round.\n",
    "\n",
    "Aggregate: It averages the model updates (diffs) from only the selected clients to create a new, improved global model.\n",
    "\n",
    "Evaluate Globally: It tests the new global model's performance on the entire held-out test set.\n",
    "\n",
    "Save Best Model: If the new global model is the best one seen so far, its state is saved to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85016d00",
   "metadata": {},
   "source": [
    "### Part 3: Final Test and Saving Results\n",
    "Now that the training is finished, we need to do two last things:\n",
    "\n",
    "Load the best model that we saved during training and run a final, definitive test on it. This gives us the final performance numbers for our experiment.\n",
    "\n",
    "Save all the logs we've been collecting into a CSV file. This is essential for creating plots and analyzing the training process, client behavior, and the effectiveness of the selection strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d12fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainValModelCSVIMG(model_name, svmethod, total_client,num_clients,epoch,max_acc,epoch_size,local_epoch_per_round,round_early_stop,\n",
    "                        X_train_csv_scaled_splits, X_test_csv_scaled_splits,\n",
    "                        X_train_1_scaled_splits, X_test_1_scaled_splits,\n",
    "                        X_train_2_scaled_splits, X_test_2_scaled_splits,\n",
    "                        Y_train_csv_splits, Y_test_csv_splits):\n",
    "    # --- 1. Initialization ---\n",
    "    # Instantiate the global model and move it to the GPU if available\n",
    "    model_MLP = ModelCSVIMG(X_train_csv_scaled_splits[0].shape[1], 32, 32)\n",
    "    model_MLP = model_MLP.to(device)\n",
    "\n",
    "    # The last client's test data is reserved for the server's global evaluation\n",
    "    server = Server(model_MLP, epoch_size, [X_test_csv_scaled_splits[total_client-1], X_test_1_scaled_splits[total_client-1], X_test_2_scaled_splits[total_client-1], Y_test_csv_splits[total_client-1]], num_clients)\n",
    "    \n",
    "    # Create a list of all clients\n",
    "    clients = []\n",
    "    for client_index in range(total_client):\n",
    "        clients.append(Client(epoch_size=epoch_size, local_epoch_per_round=local_epoch_per_round,\n",
    "                              train_dataset=[X_train_csv_scaled_splits[client_index], X_train_1_scaled_splits[client_index], X_train_2_scaled_splits[client_index],\n",
    "                                             Y_train_csv_splits[client_index]],\n",
    "                              val_dataset=[X_test_csv_scaled_splits[client_index], X_test_1_scaled_splits[client_index], X_test_2_scaled_splits[client_index],\n",
    "                                           Y_test_csv_splits[client_index]], id=client_index))\n",
    "\n",
    "    # --- Dictionaries for Logging ---\n",
    "    clients_scoresDict = {}\n",
    "    perEpoch_clients_losses = {}\n",
    "    perEpoch_clients_train_acc = {}\n",
    "    perEpoch_clients_local_test_acc = {}\n",
    "    perEpoch_clients_global_test_acc = {}\n",
    "    clients_train_acc = {}\n",
    "    clients_train_loss = {}\n",
    "    clients_test_acc = {}\n",
    "    clients_test_loss = {}\n",
    "    clients_rf_relative_loss_reduction = {}\n",
    "    clients_rf_acc_train = {}\n",
    "    clients_rf_global_validation_accuracy = {}\n",
    "    clients_rf_loss_outliers = {}\n",
    "    clients_rf_performance_bias = {}\n",
    "    clients_epoch_selected = {}\n",
    "\n",
    "    for i in range(total_client + 1):  # +1 for the server/global model\n",
    "        clients_train_acc[i] = []\n",
    "        clients_train_loss[i] = []\n",
    "        clients_test_acc[i] = []\n",
    "        clients_test_loss[i] = []\n",
    "        clients_scoresDict[i] = []\n",
    "        clients_rf_relative_loss_reduction[i] = []\n",
    "        clients_rf_acc_train[i] = []\n",
    "        clients_rf_global_validation_accuracy[i] = []\n",
    "        clients_rf_loss_outliers[i] = []\n",
    "        clients_rf_performance_bias[i] = []\n",
    "        clients_epoch_selected[i] = []\n",
    "\n",
    "    epoch_count = 0\n",
    "    # --- 2. Main Federated Learning Loop ---\n",
    "    for e in range(epoch):\n",
    "        print(f\"--- Round {e+1}/{epoch} ---\")\n",
    "        if epoch_count >= round_early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "            \n",
    "        diff_client = {}\n",
    "        weight_accumulator = {}\n",
    "        for name, params in server.global_model.state_dict().items():\n",
    "            weight_accumulator[name] = torch.zeros_like(params)\n",
    "\n",
    "        # --- Phase 1: All clients perform local training ---\n",
    "        for client_index in range(total_client):\n",
    "            round_client_model, diff, test_acc_client, loss_client, min_loss, max_loss, losses, train_acc = clients[client_index].local_train(server.global_model)\n",
    "            \n",
    "            # Store results for this client\n",
    "            perEpoch_clients_losses[client_index] = losses\n",
    "            perEpoch_clients_train_acc[client_index] = train_acc\n",
    "            perEpoch_clients_local_test_acc[client_index] = test_acc_client\n",
    "            diff_client[client_index] = diff\n",
    "            \n",
    "            # Evaluate this client's trained model on the entire global test set\n",
    "            correct = 0\n",
    "            dataset_size = 0\n",
    "            with torch.no_grad():\n",
    "                for test_data_index in range(total_client): # Loop through all test splits\n",
    "                    test_server_loader = torch.utils.data.DataLoader(\n",
    "                        CustomDatasetRes(X_test_csv_scaled_splits[test_data_index], X_test_1_scaled_splits[test_data_index],\n",
    "                                      X_test_2_scaled_splits[test_data_index], Y_test_csv_splits[test_data_index]),\n",
    "                        batch_size=epoch_size)\n",
    "                    \n",
    "                    round_client_model.eval()\n",
    "                    for batch_id, batch in enumerate(test_server_loader):\n",
    "                        data1, data2, data3, target = batch\n",
    "                        dataset_size += data1.size()[0]\n",
    "                        data1, data2, data3, target = data1.to(device).float(), data2.to(device).float(), data3.to(device).float(), target.to(device).float()\n",
    "                        \n",
    "                        output = round_client_model(data1, data2, data3)\n",
    "                        pred = output.detach().max(1)[1]\n",
    "                        correct += pred.eq(target.detach().max(1)[1].view_as(pred)).cpu().sum().item()\n",
    "\n",
    "            oneclient_global_test_acc = 100.0 * (correct / dataset_size)\n",
    "            perEpoch_clients_global_test_acc[client_index] = oneclient_global_test_acc\n",
    "            \n",
    "            # Log the local and global test accuracies for this round\n",
    "            clients_train_acc[client_index].append(test_acc_client)\n",
    "            clients_train_loss[client_index].append(loss_client)\n",
    "            clients_test_acc[client_index].append(oneclient_global_test_acc)\n",
    "            clients_epoch_selected[client_index].append(0) # Mark as not selected (yet)\n",
    "\n",
    "        # --- Phase 2: Server evaluates, selects, and aggregates ---\n",
    "        # Calculate all the advanced performance metrics for each client\n",
    "        rf_relative_loss_reduction = calculate_relative_loss_reduction_as_list(perEpoch_clients_losses)\n",
    "        rf_acc_train = calculate_relative_train_accuracy(perEpoch_clients_train_acc)\n",
    "        rf_global_validation_accuracy = calculate_global_validation_accuracy(perEpoch_clients_train_acc, perEpoch_clients_global_test_acc)\n",
    "        rf_loss_outliers = calculate_loss_outliers(perEpoch_clients_losses)\n",
    "        rf_performance_bias = calculate_performance_bias(perEpoch_clients_local_test_acc, perEpoch_clients_global_test_acc)\n",
    "        \n",
    "        # Log these calculated metrics\n",
    "        for client_index in range(total_client):\n",
    "            clients_rf_relative_loss_reduction[client_index].append(rf_relative_loss_reduction[client_index])\n",
    "            clients_rf_acc_train[client_index].append(rf_acc_train[client_index])\n",
    "            clients_rf_global_validation_accuracy[client_index].append(rf_global_validation_accuracy[client_index])\n",
    "            clients_rf_loss_outliers[client_index].append(rf_loss_outliers[client_index])\n",
    "            clients_rf_performance_bias[client_index].append(rf_performance_bias[client_index])\n",
    "\n",
    "        # --- Select clients based on the specified method ---\n",
    "        candidates = []\n",
    "        if svmethod == '5RF':\n",
    "            # Note: rf_acc_val is not defined in the code, so we'll pass an empty list for now.\n",
    "            rf_acc_test = [acc[-1] for acc in clients_test_acc.values() if acc]\n",
    "            candidates, scores = get_top_clients_with5RF(rf_relative_loss_reduction, rf_acc_train, rf_acc_test,\n",
    "                                                         rf_global_validation_accuracy, rf_loss_outliers, rf_performance_bias,\n",
    "                                                         num_clients)\n",
    "            for index in range(len(scores)):\n",
    "                clients_scoresDict[index].append(scores[index])\n",
    "        elif svmethod == 'pareto':\n",
    "            rf_acc_test = [acc[-1] for acc in clients_test_acc.values() if acc]\n",
    "            candidates = pareto_optimization(rf_relative_loss_reduction, rf_acc_train, rf_acc_test,\n",
    "                                              rf_global_validation_accuracy, rf_loss_outliers, rf_performance_bias,\n",
    "                                              num_clients)\n",
    "        elif svmethod == 'random':\n",
    "            candidates = np.random.choice(total_client, num_clients, replace=False)\n",
    "\n",
    "        print(f\"Selected clients for aggregation: {candidates}\")\n",
    "\n",
    "        # Mark which clients were selected in the log\n",
    "        for selected_client_index in candidates:\n",
    "            clients_epoch_selected[selected_client_index][-1] = 1\n",
    "        \n",
    "        # --- Aggregate the updates from selected clients ---\n",
    "        for slected_client_index in candidates:\n",
    "            for name, params in server.global_model.state_dict().items():\n",
    "                weight_accumulator[name].add_(diff_client[slected_client_index][name])\n",
    "        \n",
    "        server.model_aggregate(weight_accumulator)\n",
    "\n",
    "        # --- Phase 3: Evaluate the new global model ---\n",
    "        acc, loss = server.model_eval()\n",
    "        \n",
    "        # Log global model performance\n",
    "        clients_test_acc[total_client].append(acc)\n",
    "        clients_test_loss[total_client].append(loss)\n",
    "        \n",
    "        print(f\"Round {e+1} Global Model - Accuracy: {acc:.2f}%, Loss: {loss:.4f}\\n\")\n",
    "\n",
    "        # --- Save the best model and check for early stopping ---\n",
    "        epoch_count += 1\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            print(\"New best model found! Saving model...\")\n",
    "            torch.save(server.global_model.state_dict(),\n",
    "                       f\"./acc_lossFiles/{model_name}_totalClient_{total_client}_NumClient_{num_clients}_epoch_{epoch}_svmethod_{svmethod}.pth\")\n",
    "            epoch_count = 0 # Reset early stopping counter\n",
    "        \n",
    "    # --- After the training loop, perform a final evaluation on the best model ---\n",
    "    print(\"\\n--- Final Evaluation on Best Model ---\")\n",
    "    model = model_MLP\n",
    "    model.load_state_dict(torch.load(\n",
    "        f\"./acc_lossFiles/{model_name}_totalClient_{total_client}_NumClient_{num_clients}_epoch_{epoch}_svmethod_{svmethod}.pth\"))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    y_test, y_predict = [], []\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    dataset_size = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_data_index in range(total_client):\n",
    "            test_server_loader = torch.utils.data.DataLoader(\n",
    "                CustomDatasetRes(X_test_csv_scaled_splits[test_data_index], X_test_1_scaled_splits[test_data_index],\n",
    "                              X_test_2_scaled_splits[test_data_index], Y_test_csv_splits[test_data_index]),\n",
    "                batch_size=epoch_size)\n",
    "            for batch_id, batch in enumerate(test_server_loader):\n",
    "                data1, data2, data3, target = batch\n",
    "                dataset_size += data1.size()[0]\n",
    "                data1, data2, data3, target = data1.to(device).float(), data2.to(device).float(), data3.to(device).float(), target.to(device).float()\n",
    "\n",
    "                output = model(data1, data2, data3)\n",
    "                total_loss += nn.functional.cross_entropy(output, target, reduction='sum').item()\n",
    "                \n",
    "                pred = output.detach().max(1)[1]\n",
    "                correct += pred.eq(target.detach().max(1)[1].view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    acc = 100.0 * (correct / dataset_size)\n",
    "    loss = total_loss / dataset_size\n",
    "\n",
    "    print(f'Final Best Model Test Accuracy: {acc:.2f}%')\n",
    "    print(f'Final Best Model Test Loss: {loss:.4f}')\n",
    "    print(f'Max accuracy achieved during training: {max_acc:.2f}%')\n",
    "\n",
    "    # --- Save all logged data to a CSV file ---\n",
    "    \"\"\"csv_file_name = f\"./acc_lossFiles/{model_name}_totalClient_{total_client}_NumClient_{num_clients}_epoch_{epoch}_svmethod_{svmethod}.csv\"\n",
    "    try:\n",
    "        # 保存到 CSV 文件\n",
    "        with open(csv_file_name, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['server_client_index',server_client_index])\n",
    "            writer.writerow(\n",
    "                ['client', 'Epoch', 'train_Loss', 'train_Accuracy', 'test_Loss', 'test_Accuracy','rf_loss', 'rf_acc_train', 'rf_acc_val', 'rf_acc_global', 'p_loss', 'p_bias', 'selected'])\n",
    "            for i in range(total_client+1):\n",
    "                print('i:',i)\n",
    "                # 添加列名\n",
    "                train_losses = clients_train_loss[i]\n",
    "                train_accs = clients_train_acc[i]\n",
    "                test_loss = clients_test_loss[i]\n",
    "                test_acc = clients_test_acc[i]\n",
    "                rf_loss = clients_rf_relative_loss_reduction[i]\n",
    "                rf_acc_train = clients_rf_acc_train[i]\n",
    "                rf_acc_global = clients_rf_global_validation_accuracy[i]\n",
    "                p_loss = clients_rf_loss_outliers[i]\n",
    "                p_bias = clients_rf_performance_bias[i]\n",
    "                selecteds = clients_epoch_selected[i]\n",
    "                if i == total_client:\n",
    "                    for j in range(len(train_losses)):\n",
    "                        writer.writerow([i, j + 1, train_losses[j], train_accs[j], test_loss[j], test_acc[j], 0,0,0,0,0,0,0])\n",
    "                else:\n",
    "                    for j in range(len(train_losses)):\n",
    "                        writer.writerow([i, j + 1, train_losses[j], train_accs[j], test_loss[j], test_acc[j],rf_loss[j],rf_acc_train[j], rf_acc_global[j],p_loss[j],p_bias[j],selecteds[j]])\n",
    "        print(f\"Results saved to {csv_file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving results: {e}\")\n",
    "        continue\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6e7cb",
   "metadata": {},
   "source": [
    "We've arrived at the final step! We have all the building blocks in place. The only thing left is to set our experimental parameters and create the main execution block that calls our functions and runs the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d287713",
   "metadata": {},
   "source": [
    "## Final Step: The main Function and Execution Block\n",
    "This final piece of code does the following:\n",
    "\n",
    "Sets Hyperparameters: Defines all the key variables for the experiment, like the number of clients, epochs, learning rate, etc. It also defines the different scenarios we want to test (e.g., different client selection methods, different data corruption scenarios).\n",
    "\n",
    "Defines a main() function: This function orchestrates the experiment. It loads the client data, then loops through each experimental scenario. For scenarios involving \"model loss,\" it intentionally corrupts the data for some clients (e.g., replacing their sensor data with random noise) to simulate system failures or unreliable participants.\n",
    "\n",
    "Calls trainValModelCSVIMG: For each scenario, it calls our main training function to run a full federated learning simulation.\n",
    "\n",
    "Executes main(): The standard if __name__ == \"__main__\": line ensures that the main function is called when you run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdc2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Scenario: Corrupted Image Data ---\n",
      "\n",
      "===== STARTING NEW EXPERIMENT: Model=tc1c2ResModelV3DataV3AdamWithImgLost, Selection=pareto =====\n",
      "--- Round 1/2 ---\n",
      "Client 0 - Train Acc: 95.61%, Val Acc: 96.05%\n",
      "Client 1 - Train Acc: 93.31%, Val Acc: 94.28%\n",
      "Client 2 - Train Acc: 96.49%, Val Acc: 95.86%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# This makes the script runnable\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 57\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m svmethod \u001b[38;5;129;01min\u001b[39;00m svmethods:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== STARTING NEW EXPERIMENT: Model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Selection=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msvmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mtrainValModelCSVIMG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epoch_per_round\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mround_early_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mX_train_csv_scaled_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_csv_scaled_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mX_train_1_scaled_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_1_scaled_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mX_train_2_scaled_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_2_scaled_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mY_train_csv_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test_csv_splits\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 93\u001b[0m, in \u001b[0;36mtrainValModelCSVIMG\u001b[0;34m(model_name, svmethod, total_client, num_clients, epoch, max_acc, epoch_size, local_epoch_per_round, round_early_stop, X_train_csv_scaled_splits, X_test_csv_scaled_splits, X_train_1_scaled_splits, X_test_1_scaled_splits, X_train_2_scaled_splits, X_test_2_scaled_splits, Y_train_csv_splits, Y_test_csv_splits)\u001b[0m\n\u001b[1;32m     90\u001b[0m             data1, data2, data3, target \u001b[38;5;241m=\u001b[39m data1\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat(), data2\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat(), data3\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     92\u001b[0m             output \u001b[38;5;241m=\u001b[39m round_client_model(data1, data2, data3)\n\u001b[0;32m---> 93\u001b[0m             pred \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     94\u001b[0m             correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39meq(target\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview_as(pred))\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     96\u001b[0m oneclient_global_test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100.0\u001b[39m \u001b[38;5;241m*\u001b[39m (correct \u001b[38;5;241m/\u001b[39m dataset_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Define Experimental Scenarios and Hyperparameters ---\n",
    "model_names = {'tc1c2ResModelV3DataV3Adam', \n",
    "               'tc1c2ResModelV3DataV3AdamWithSCVLost', \n",
    "               'tc1c2ResModelV3DataV3AdamWithImgLost'}\n",
    "\n",
    "model_names = {'tc1c2ResModelV3DataV3AdamWithImgLost'}\n",
    "svmethods = {'pareto', '5RF', 'random'}\n",
    "svmethods = {'pareto'}  # For testing purposes, you can change this to 'pareto' or 'random'\n",
    "# --- Hyperparameters ---\n",
    "max_acc = 1  # Threshold of accuracy for saving the best model\n",
    "epoch = 2  # Total number of communication rounds\n",
    "epoch_size = 64  # Batch size\n",
    "total_client = 12  # Total number of clients in the pool\n",
    "num_clients = 6  # Number of clients selected per round\n",
    "local_epoch_per_round = 3 # Number of local training epochs for each client per round\n",
    "round_early_stop = 10 # Number of rounds without improvement before stopping\n",
    "server_client_index = random.randint(0, total_client - 1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the data pre-split for each of the 12 clients\n",
    "    X_train_csv_scaled_splits, X_test_csv_scaled_splits, \\\n",
    "    Y_train_csv_splits, Y_test_csv_splits, \\\n",
    "    X_train_1_scaled_splits, X_test_1_scaled_splits, \\\n",
    "    Y_train_1_splits, Y_test_1_splits, \\\n",
    "    X_train_2_scaled_splits, X_test_2_scaled_splits, \\\n",
    "    Y_train_2_splits, Y_test_2_splits = loadClientsData()\n",
    "\n",
    "    # Loop through each experimental scenario\n",
    "    for model_name in model_names:\n",
    "        # --- Simulate Data Corruption Scenarios ---\n",
    "        if model_name == 'tc1c2ResModelV3DataV3AdamWithSCVLost':\n",
    "            print(\"\\n--- Running Scenario: Corrupted CSV Data ---\")\n",
    "            # Replace CSV data with random noise for clients 6 through 11\n",
    "            for index in [6, 7, 8, 9, 10, 11]:\n",
    "                shape_train = X_train_csv_scaled_splits[index].shape\n",
    "                X_train_csv_scaled_splits[index] = np.random.rand(*shape_train)\n",
    "                shape_test = X_test_csv_scaled_splits[index].shape\n",
    "                X_test_csv_scaled_splits[index] = np.random.rand(*shape_test)\n",
    "        elif model_name == 'tc1c2ResModelV3DataV3AdamWithImgLost':\n",
    "            print(\"\\n--- Running Scenario: Corrupted Image Data ---\")\n",
    "            # Replace Image data with random noise for clients 6 through 11\n",
    "            for index in [6, 7, 8, 9, 10, 11]:\n",
    "                shape_train1 = X_train_1_scaled_splits[index].shape\n",
    "                X_train_1_scaled_splits[index] = np.random.rand(*shape_train1)\n",
    "                shape_test1 = X_test_1_scaled_splits[index].shape\n",
    "                X_test_1_scaled_splits[index] = np.random.rand(*shape_test1)\n",
    "                shape_train2 = X_train_2_scaled_splits[index].shape\n",
    "                X_train_2_scaled_splits[index] = np.random.rand(*shape_train2)\n",
    "                shape_test2 = X_test_2_scaled_splits[index].shape\n",
    "                X_test_2_scaled_splits[index] = np.random.rand(*shape_test2)\n",
    "\n",
    "        # Loop through each client selection method\n",
    "        for svmethod in svmethods:\n",
    "            print(f\"\\n===== STARTING NEW EXPERIMENT: Model={model_name}, Selection={svmethod} =====\")\n",
    "            trainValModelCSVIMG(model_name, svmethod, total_client, num_clients, epoch, max_acc, epoch_size, local_epoch_per_round, round_early_stop,\n",
    "                                X_train_csv_scaled_splits, X_test_csv_scaled_splits,\n",
    "                                X_train_1_scaled_splits, X_test_1_scaled_splits,\n",
    "                                X_train_2_scaled_splits, X_test_2_scaled_splits,\n",
    "                                Y_train_csv_splits, Y_test_csv_splits)\n",
    "\n",
    "# This makes the script runnable\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921306f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
