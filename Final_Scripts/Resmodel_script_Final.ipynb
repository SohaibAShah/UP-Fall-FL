{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0221b1e3",
   "metadata": {},
   "source": [
    "## Step 1: The Foundation (Imports and Setup)\n",
    "Every Python script starts with importing the necessary libraries and setting up the environment. This is like laying the foundation for a house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "696a41f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "# -- Basic Setup --\n",
    "# Set the device to use the GPU if available, otherwise use the CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b56c2",
   "metadata": {},
   "source": [
    "### Define dataset loader\n",
    "\n",
    "PyTorch uses a Dataset object to handle data loading. Since our model will take three different kinds of input (sensor data, image 1, and image 2), we need to create a special class that tells PyTorch how to retrieve one sample of each, along with its corresponding label.\n",
    "\n",
    "This class will have three essential methods:\n",
    "\n",
    "__init__: Initializes the dataset by storing our feature and label arrays.\n",
    "\n",
    "__len__: Returns the total number of samples in the dataset.\n",
    "\n",
    "__getitem__: Fetches a single data sample at a given index.\n",
    "\n",
    "Here is the code for it. Add this to your script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ebd8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset loader\n",
    "class CustomDatasetRes(Dataset):\n",
    "    def __init__(self, features1, features2, features3, labels):\n",
    "        self.features1 = features1\n",
    "        self.features2 = features2\n",
    "        self.features3 = features3\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.features1[index], self.features2[index], self.features3[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03702557",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "Next, we'll add a few helper functions. These functions will perform common tasks that we'll need later, like displaying results, scaling data, and ensuring our experiments are reproducible.\n",
    "\n",
    "1. display_result\n",
    "\n",
    "This function takes the true labels (y_test) and the model's predicted labels (y_pred) and prints out standard performance metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e6414e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result(y_test, y_pred):\n",
    "    print('Accuracy score : ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision score : ', precision_score(y_test, y_pred, average='weighted'))\n",
    "    print('Recall score : ', recall_score(y_test, y_pred, average='weighted'))\n",
    "    print('F1 score : ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a5531",
   "metadata": {},
   "source": [
    "2. scaled_data\n",
    "\n",
    "This function uses Scikit-learn's StandardScaler to normalize the sensor (CSV) data. Scaling is crucial because it ensures that features with larger value ranges don't dominate the learning process. Notice there are two functions with the same name in the original code. In Python, the last definition of a function is the one that gets used. We will add both for completeness, but just know that the first one is effectively overwritten by the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adc0678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def scaled_data(X_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    return X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d51f72",
   "metadata": {},
   "source": [
    "3. set_seed\n",
    "\n",
    "This is a very important function for reproducibility. Machine learning involves a lot of randomness (e.g., initializing model weights, shuffling data). By setting a \"seed,\" we ensure that the sequence of random numbers is the same every time we run the code, which means we'll get the exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93f4630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    # Sets the environment variable for Python's hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # Sets the seed for NumPy's random number generator\n",
    "    np.random.seed(seed)\n",
    "    # Sets the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    # Sets the seed for PyTorch's random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    # If using a GPU, sets the seed for all CUDA devices\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "    # Ensures deterministic behavior in cuDNN (CUDA Deep Neural Network library)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf1683",
   "metadata": {},
   "source": [
    "### loading and preprocessing the data.\n",
    "\n",
    "The function loadClientsData is designed for a federated learning scenario. It reads data from separate files for each participant (or \"client\"), cleans it, aligns the different data types (sensor vs. image), and splits it into training and testing sets for each client.\n",
    "\n",
    "Because this function is quite long, we'll build it in a few parts.\n",
    "\n",
    "#### Part 1: Initializing and Processing Training Data\n",
    "First, we'll define the function, list the subject IDs we want to load, and create empty dictionaries to store each client's data. Then, we'll start a loop to process each subject one by one. Inside the loop, we'll begin by loading and cleaning the training data.\n",
    "\n",
    "This involves:\n",
    "\n",
    "Reading the sensor data from a CSV file.\n",
    "\n",
    "Removing rows with missing values and any duplicate rows.\n",
    "\n",
    "Dropping columns that we don't need (like the 'Infrared' sensor readings).\n",
    "\n",
    "Loading the corresponding image, label, and timestamp data from .npy files.\n",
    "\n",
    "#### Part 2: Aligning and Preparing Training Data\n",
    "After loading the raw data, we face a common problem: the datasets don't perfectly match. Because we dropped rows with missing values from the sensor (CSV) data, there are now timestamps in our image data that no longer have a corresponding entry in the sensor data.\n",
    "\n",
    "We need to align them by removing the image samples that don't have a matching sensor reading.\n",
    "\n",
    "After alignment, we'll prepare the data for the model:\n",
    "\n",
    "Set the seed for reproducibility.\n",
    "\n",
    "Separate features from labels.\n",
    "\n",
    "One-hot encode the labels, converting them into a format suitable for the model's output layer (e.g., class 3 becomes [0, 0, 0, 1, 0, ...]).\n",
    "\n",
    "Scale the numeric sensor data and the image pixel values.\n",
    "\n",
    "Reshape the images to the format expected by the convolutional layers.\n",
    "\n",
    "#### Part 3: Processing the Test Data and Finalizing the Function\n",
    "The logic here is identical to what we just did for the training data:\n",
    "\n",
    "Load the test sensor data (_test.csv) and test image data (_test.npy).\n",
    "\n",
    "Clean the sensor data by removing missing values and unnecessary columns.\n",
    "\n",
    "Align the test image data with the cleaned test sensor data.\n",
    "\n",
    "Prepare the aligned test data (one-hot encode labels, scale features, reshape images).\n",
    "\n",
    "Store all the processed training and test arrays into our dictionaries.\n",
    "\n",
    "Increment the clint_index and repeat the process for the next subject.\n",
    "\n",
    "After the loop finishes, the function returns all the dictionaries containing the data for every client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81f6095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadClientsData():\n",
    "    subs = [1, 3, 4, 7, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "    X_train_csv_scaled_splits = {}\n",
    "    X_test_csv_scaled_splits = {}\n",
    "    Y_train_csv_splits = {}\n",
    "    Y_test_csv_splits = {}\n",
    "    X_train_1_scaled_splits = {}\n",
    "    X_test_1_scaled_splits = {}\n",
    "    Y_train_1_splits = {}\n",
    "    Y_test_1_splits = {}\n",
    "    X_train_2_scaled_splits = {}\n",
    "    X_test_2_scaled_splits = {}\n",
    "    Y_train_2_splits = {}\n",
    "    Y_test_2_splits = {}\n",
    "    clint_index = 0\n",
    "    for sub_ in subs:\n",
    "        # --- Load and clean TRAINING sensor data (CSV) ---\n",
    "        SUB_train = pd.read_csv('./dataset/Sensor + Image/{}_sensor_train.csv'.format(sub_), skiprows=1)\n",
    "        SUB_train.head()\n",
    "        \n",
    "        SUB_train.isnull().sum()\n",
    "        NA_cols = SUB_train.columns[SUB_train.isnull().any()]\n",
    "        SUB_train.dropna(inplace=True)\n",
    "        SUB_train.drop_duplicates(inplace=True)\n",
    "        \n",
    "        times_train = SUB_train['Time']\n",
    "        list_DROP = ['Infrared 1',\n",
    "                     'Infrared 2',\n",
    "                     'Infrared 3',\n",
    "                     'Infrared 4',\n",
    "                     'Infrared 5',\n",
    "                     'Infrared 6']\n",
    "        SUB_train.drop(list_DROP, axis=1, inplace=True)\n",
    "        SUB_train.drop(NA_cols, axis=1, inplace=True)  # drop NAN COLS\n",
    "\n",
    "        SUB_train.set_index('Time', inplace=True)\n",
    "        SUB_train.head()\n",
    "\n",
    "        # --- Load TRAINING image data from both cameras ---\n",
    "        cam = '1'\n",
    "        image_train = './dataset/Sensor + Image' + '/' + '{}_image_1_train.npy'.format(sub_)\n",
    "        name_train = './dataset/Sensor + Image' + '/' + '{}_name_1_train.npy'.format(sub_)\n",
    "        label_train = './dataset/Sensor + Image' + '/' + '{}_label_1_train.npy'.format(sub_)\n",
    "\n",
    "        img_1_train = np.load(image_train)\n",
    "        label_1_train = np.load(label_train)\n",
    "        name_1_train = np.load(name_train)\n",
    "\n",
    "        cam = '2'\n",
    "        image_train = './dataset/Sensor + Image' + '/' + '{}_image_2_train.npy'.format(sub_)\n",
    "        name_train = './dataset/Sensor + Image' + '/' + '{}_name_2_train.npy'.format(sub_)\n",
    "        label_train = './dataset/Sensor + Image' + '/' + '{}_label_2_train.npy'.format(sub_)\n",
    "\n",
    "        img_2_train = np.load(image_train)\n",
    "        label_2_train = np.load(label_train)\n",
    "        name_2_train = np.load(name_train)\n",
    "\n",
    "        # --- Align the training data by removing samples not present in the cleaned CSV ---\n",
    "        redundant_1 = list(set(name_1_train) - set(times_train))\n",
    "        redundant_2 = list(set(name_2_train) - set(times_train))\n",
    "        \n",
    "        ind = np.arange(0, len(img_1_train))\n",
    "\n",
    "        red_in1 = ind[np.isin(name_1_train, redundant_1)]\n",
    "        name_1_train = np.delete(name_1_train, red_in1)\n",
    "        img_1_train = np.delete(img_1_train, red_in1, axis=0)\n",
    "        label_1_train = np.delete(label_1_train, red_in1)\n",
    "\n",
    "        red_in2 = ind[np.isin(name_2_train, redundant_2)]\n",
    "        name_2_train = np.delete(name_2_train, red_in2)\n",
    "        img_2_train = np.delete(img_2_train, red_in2, axis=0)\n",
    "        label_2_train = np.delete(label_2_train, red_in2)\n",
    "        \n",
    "        # --- Prepare the final aligned training data ---\n",
    "        data_train = SUB_train.loc[name_1_train].values\n",
    "\n",
    "        set_seed()\n",
    "        X_csv_train, y_csv_train = data_train[:, :-1], data_train[:, -1]\n",
    "        \n",
    "        # Remap label 20 to 0 for consistency\n",
    "        y_csv_train = np.where(y_csv_train == 20, 0, y_csv_train)\n",
    "        label_1_train = np.where(label_1_train == 20, 0, label_1_train)\n",
    "        label_2_train = np.where(label_2_train == 20, 0, label_2_train)\n",
    "\n",
    "        # One-hot encode the labels for PyTorch\n",
    "        Y_csv_train = torch.nn.functional.one_hot(torch.from_numpy(y_csv_train).long(), 12).float()\n",
    "        Y_train_1 = torch.nn.functional.one_hot(torch.from_numpy(label_1_train).long(), 12).float()\n",
    "        Y_train_2 = torch.nn.functional.one_hot(torch.from_numpy(label_2_train).long(), 12).float()\n",
    "\n",
    "        X_train_1 = img_1_train\n",
    "        y_train_1 = label_1_train\n",
    "        \n",
    "        X_train_2 = img_2_train\n",
    "        y_train_2 = label_2_train\n",
    "\n",
    "        # Reshape images to (samples, height, width, channels)\n",
    "        shape1, shape2 = 32, 32\n",
    "        X_train_1 = X_train_1.reshape(X_train_1.shape[0], shape1, shape2, 1)\n",
    "        X_train_2 = X_train_2.reshape(X_train_2.shape[0], shape1, shape2, 1)\n",
    "\n",
    "        # Scale image pixel values to be between 0 and 1\n",
    "        X_train_1_scaled = X_train_1 / 255.0\n",
    "        X_train_2_scaled = X_train_2 / 255.0\n",
    "\n",
    "        # --- Load and clean TEST sensor data (CSV) ---\n",
    "        SUB_test = pd.read_csv('./dataset/Sensor + Image/{}_sensor_test.csv'.format(sub_), skiprows=1)\n",
    "        SUB_test.head()\n",
    "        \n",
    "        SUB_test.isnull().sum()\n",
    "        NA_cols = SUB_test.columns[SUB_test.isnull().any()]\n",
    "        SUB_test.dropna(inplace=True)\n",
    "        SUB_test.drop_duplicates(inplace=True)\n",
    "\n",
    "        times_test = SUB_test['Time']\n",
    "        SUB_test.drop(list_DROP, axis=1, inplace=True)\n",
    "        SUB_test.drop(NA_cols, axis=1, inplace=True)\n",
    "\n",
    "        SUB_test.set_index('Time', inplace=True)\n",
    "        SUB_test.head()\n",
    "\n",
    "        # --- Load TEST image data from both cameras ---\n",
    "        image_test = './dataset/Sensor + Image' + '/' + '{}_image_1_test.npy'.format(sub_)\n",
    "        name_test = './dataset/Sensor + Image' + '/' + '{}_name_1_test.npy'.format(sub_)\n",
    "        label_test = './dataset/Sensor + Image' + '/' + '{}_label_1_test.npy'.format(sub_)\n",
    "        img_1_test = np.load(image_test)\n",
    "        label_1_test = np.load(label_test)\n",
    "        name_1_test = np.load(name_test)\n",
    "\n",
    "        image_test = './dataset/Sensor + Image' + '/' + '{}_image_2_test.npy'.format(sub_)\n",
    "        name_test = './dataset/Sensor + Image' + '/' + '{}_name_2_test.npy'.format(sub_)\n",
    "        label_test = './dataset/Sensor + Image' + '/' + '{}_label_2_test.npy'.format(sub_)\n",
    "        img_2_test = np.load(image_test)\n",
    "        label_2_test = np.load(label_test)\n",
    "        name_2_test = np.load(name_test)\n",
    "\n",
    "        # --- Align the test data ---\n",
    "        redundant_1 = list(set(name_1_test) - set(times_test))\n",
    "        redundant_2 = list(set(name_2_test) - set(times_test))\n",
    "        \n",
    "        ind = np.arange(0, len(img_1_test))\n",
    "\n",
    "        red_in1 = ind[np.isin(name_1_test, redundant_1)]\n",
    "        name_1_test = np.delete(name_1_test, red_in1)\n",
    "        img_1_test = np.delete(img_1_test, red_in1, axis=0)\n",
    "        label_1_test = np.delete(label_1_test, red_in1)\n",
    "\n",
    "        red_in2 = ind[np.isin(name_2_test, redundant_2)]\n",
    "        name_2_test = np.delete(name_2_test, red_in2)\n",
    "        img_2_test = np.delete(img_2_test, red_in2, axis=0)\n",
    "        label_2_test = np.delete(label_2_test, red_in2)\n",
    "\n",
    "        # --- Prepare the final aligned test data ---\n",
    "        data_test = SUB_test.loc[name_1_test].values\n",
    "\n",
    "        set_seed()\n",
    "        X_csv_test, y_csv_test = data_test[:, :-1], data_test[:, -1]\n",
    "        y_csv_test = np.where(y_csv_test == 20, 0, y_csv_test)\n",
    "        label_1_test = np.where(label_1_test == 20, 0, label_1_test)\n",
    "        label_2_test = np.where(label_2_test == 20, 0, label_2_test)\n",
    "\n",
    "        Y_csv_test = torch.nn.functional.one_hot(torch.from_numpy(y_csv_test).long(), 12).float()\n",
    "        # Scale the sensor data\n",
    "        X_csv_train_scaled, X_csv_test_scaled = scale_data(X_csv_train, X_csv_test)\n",
    "\n",
    "        X_test_1 = img_1_test\n",
    "        y_test_1 = label_1_test\n",
    "        Y_test_1 = torch.nn.functional.one_hot(torch.from_numpy(y_test_1).long(), 12).float()\n",
    "\n",
    "        X_test_2 = img_2_test\n",
    "        y_test_2 = label_2_test\n",
    "        Y_test_2 = torch.nn.functional.one_hot(torch.from_numpy(y_test_2).long(), 12).float()\n",
    "\n",
    "        X_test_1 = X_test_1.reshape(X_test_1.shape[0], shape1, shape2, 1)\n",
    "        X_test_2 = X_test_2.reshape(X_test_2.shape[0], shape1, shape2, 1)\n",
    "\n",
    "        X_test_1_scaled = X_test_1 / 255.0\n",
    "        X_test_2_scaled = X_test_2 / 255.0\n",
    "\n",
    "        # --- Store all processed data for the current client ---\n",
    "        X_train_csv_scaled_splits[clint_index] = X_csv_train_scaled\n",
    "        X_test_csv_scaled_splits[clint_index] = X_csv_test_scaled\n",
    "        Y_train_csv_splits[clint_index] = Y_csv_train\n",
    "        Y_test_csv_splits[clint_index] = Y_csv_test\n",
    "        X_train_1_scaled_splits[clint_index] = X_train_1_scaled\n",
    "        X_test_1_scaled_splits[clint_index] = X_test_1_scaled\n",
    "        Y_train_1_splits[clint_index] = Y_train_1\n",
    "        Y_test_1_splits[clint_index] = Y_test_1\n",
    "        X_train_2_scaled_splits[clint_index] = X_train_2_scaled # This line had a bug in the original code\n",
    "        X_test_2_scaled_splits[clint_index] = X_test_2_scaled\n",
    "        Y_train_2_splits[clint_index] = Y_train_2\n",
    "        Y_test_2_splits[clint_index] = Y_test_2\n",
    "        clint_index += 1\n",
    "        \n",
    "    # --- After loop, return all dictionaries ---\n",
    "    return X_train_csv_scaled_splits,X_test_csv_scaled_splits, Y_train_csv_splits,Y_test_csv_splits,X_train_1_scaled_splits,X_test_1_scaled_splits,Y_train_1_splits,Y_test_1_splits,X_train_2_scaled_splits,X_test_2_scaled_splits,Y_train_2_splits,Y_test_2_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e1b50",
   "metadata": {},
   "source": [
    "## Step 2: Client Selection\n",
    "\n",
    "We're making great progress. We've handled all the data loading and preparation. Now, we'll add the functions that form the \"intelligence\" of our federated learning system: client selection.\n",
    "\n",
    "Instead of blindly averaging updates from every client in every round, these methods evaluate each client's performance and contribution. This allows the server to select the most promising or reliable clients to participate in the global model update, potentially leading to faster convergence and a more robust final model.\n",
    "\n",
    "We'll add a series of functions, each calculating a specific metric to judge the clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5173b0f8",
   "metadata": {},
   "source": [
    "### Client Evaluation Metrics\n",
    "Add all the following functions to your script. Each one calculates a different score based on a client's performance.\n",
    "\n",
    "1. Relative Loss Reduction (RF_loss)\n",
    "\n",
    "This measures how much a client's training loss has dropped from the beginning to the end of a local training round, relative to the client with the biggest drop. A higher score means the client is learning effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca768194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_loss_reduction_as_list(client_losses):\n",
    "    \"\"\"\n",
    "    Calculates the relative loss reduction (RF_loss) for each client.\n",
    "    \"\"\"\n",
    "    loss_reduction = {}\n",
    "    for client_id, losses in client_losses.items():\n",
    "        if len(losses) < 2:\n",
    "            raise ValueError(f\"Client {client_id} has less than 2 loss values, cannot calculate RF_loss.\")\n",
    "        loss_start = losses[0]\n",
    "        loss_end = losses[-1]\n",
    "        loss_reduction[client_id] = loss_start - loss_end\n",
    "\n",
    "    max_loss_reduction = max(loss_reduction.values())\n",
    "    if max_loss_reduction == 0:\n",
    "        return [0.0] * len(loss_reduction)  # If no loss reduction, return 0.0 for all clients\n",
    "\n",
    "    rf_losses_list = [\n",
    "        reduction / max_loss_reduction for reduction in loss_reduction.values()\n",
    "    ]\n",
    "    return rf_losses_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f48d0a",
   "metadata": {},
   "source": [
    "2. Relative Training Accuracy (RF_ACC_Train)\n",
    "\n",
    "This measures a client's local training accuracy relative to the client with the highest accuracy. It's a straightforward measure of performance on local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a82d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_train_accuracy(client_acc):\n",
    "    \"\"\"\n",
    "    Calculates the relative training accuracy (RF_Acc_Train) for each client.\n",
    "    \"\"\"\n",
    "    max_acc = max(client_acc.values())\n",
    "    if max_acc == 0:\n",
    "        return [0.0] * len(client_acc)  # If no accuracy, return 0.0 for all clients\n",
    "\n",
    "    rf_accs_train_list = [\n",
    "        acc / max_acc for acc in client_acc.values()\n",
    "    ]\n",
    "    return rf_accs_train_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e977c2",
   "metadata": {},
   "source": [
    "3. Global Validation Accuracy (RF_ACC_Global)\n",
    "\n",
    "This is a more sophisticated metric. It rewards clients for high accuracy on a global test set but penalizes them if their global accuracy is much worse than their local training accuracy (which is a sign of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3b4108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_global_validation_accuracy(train_acc, global_acc):\n",
    "    \"\"\"\n",
    "    Calculates the global validation accuracy (RF_Acc_Global) based on local training accuracies.\n",
    "    \"\"\"\n",
    "    if set(train_acc.keys()) != set(global_acc.keys()):\n",
    "        raise ValueError(\"Client IDs for train and global accuracy do not match.\")\n",
    "\n",
    "    max_global_acc = max(global_acc.values())\n",
    "    if max_global_acc == 0:\n",
    "        max_global_acc = 1  # Avoid division by zero\n",
    "\n",
    "    global_train_diff = {\n",
    "        client_id: train_acc[client_id] - global_acc[client_id]\n",
    "        for client_id in train_acc\n",
    "    }\n",
    "    max_global_train_diff = max(global_train_diff.values())\n",
    "    if max_global_train_diff == 0:\n",
    "        max_global_train_diff = 1  # Avoid division by zero\n",
    "\n",
    "    rf_acc_global_list = [\n",
    "        (global_acc[client_id] / max_global_acc) - (global_train_diff[client_id] / max_global_train_diff)\n",
    "        for client_id in train_acc\n",
    "    ]\n",
    "    return rf_acc_global_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21270341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_validation_accuracy(client_acc):\n",
    "    \"\"\"\n",
    "    Calculates the relative validation accuracy (RF_ACC_Val) for each client.\n",
    "    \"\"\"\n",
    "    # Ensure client_acc is a dictionary, not a list of lists\n",
    "    if not isinstance(client_acc, dict):\n",
    "        raise TypeError(\"Input must be a dictionary of client accuracies.\")\n",
    "        \n",
    "    max_acc = max(client_acc.values())\n",
    "    if max_acc == 0:\n",
    "        return [0.0] * len(client_acc)\n",
    "\n",
    "    return [acc / max_acc for acc in client_acc.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26f9bf",
   "metadata": {},
   "source": [
    "4. Loss Outliers (P_loss)\n",
    "\n",
    "This function flags clients that are potential negative contributors. If a client's final training loss is significantly higher than the average loss of all clients, it gets a high penalty score. Otherwise, its penalty is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35c66dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_outliers(client_losses, lambda_loss=1.5):\n",
    "    \"\"\"\n",
    "    Calculates the loss outlier penalty (P_loss) for each client.\n",
    "    \"\"\"\n",
    "    final_losses = {client_id: losses[-1] for client_id, losses in client_losses.items()}\n",
    "    loss_values = np.array(list(final_losses.values()))\n",
    "\n",
    "    mean_loss = np.mean(loss_values)\n",
    "    std_loss = np.std(loss_values)\n",
    "\n",
    "    threshold = mean_loss + lambda_loss * std_loss\n",
    "\n",
    "    max_loss = np.max(loss_values)\n",
    "\n",
    "    if max_loss == 0:\n",
    "        return [0.0] * len(loss_values)\n",
    "\n",
    "    # Identify outliers\n",
    "    loss_outliers = [\n",
    "        final_loss / max_loss if final_loss > threshold else 0.0\n",
    "        for final_loss in loss_values\n",
    "    ]\n",
    "    return loss_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96a2ac",
   "metadata": {},
   "source": [
    "5. Performance Bias (P_bias)\n",
    "\n",
    "This metric calculates the gap between a client's performance on its own validation data versus its performance on the global validation data. A large gap might indicate that the client's local data is not representative of the overall data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc704c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_bias(val_acc, global_acc):\n",
    "    \"\"\"\n",
    "    Calculates the performance bias penalty (P_bias).\n",
    "    \"\"\"\n",
    "    if set(val_acc.keys()) != set(global_acc.keys()):\n",
    "        raise ValueError(\"Client IDs for validation and global accuracy do not match.\")\n",
    "\n",
    "    performance_bias_list = []\n",
    "    for client_id in val_acc:\n",
    "        val = val_acc[client_id]\n",
    "        global_val = global_acc[client_id]\n",
    "        max_val = max(val, global_val)\n",
    "\n",
    "        if max_val == 0:\n",
    "            performance_bias = 0\n",
    "        else:\n",
    "            performance_bias = abs(val - global_val) / max_val\n",
    "        performance_bias_list.append(performance_bias)\n",
    "\n",
    "    return performance_bias_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfd596",
   "metadata": {},
   "source": [
    "Excellent. Now that we have the functions to score each client, we need the final step: the algorithms that use these scores to select which clients will participate in a given round."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cecd1",
   "metadata": {},
   "source": [
    "### Client Selection Algorithms\n",
    "1. Pareto Optimization\n",
    "\n",
    "This is a powerful technique used when you have multiple, often conflicting, objectives. Instead of combining all metrics into one score, it tries to find a set of clients that represent the best possible trade-offs.\n",
    "\n",
    "A client is considered \"Pareto optimal\" if no other client is better than it across all metrics. The algorithm first finds this set of optimal clients.\n",
    "\n",
    "If there are more optimal clients than needed, it selects a random subset.\n",
    "\n",
    "If there are fewer, it fills the remaining spots by picking the clients with the best-combined performance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25f90e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_optimization(\n",
    "    rf_loss, rf_acc_train, rf_acc_val, rf_acc_global, p_loss, p_bias, client_num,\n",
    "):\n",
    "    \"\"\"\n",
    "    实现 Pareto 优化，筛选节点。\n",
    "\n",
    "    参数：\n",
    "    - rf_loss (list): 局部训练损失相对下降幅度。\n",
    "    - rf_acc_train (list): 局部训练精度。\n",
    "    - rf_acc_val (list): 局部验证精度。\n",
    "    - rf_acc_global (list): 全局验证精度。\n",
    "    - p_loss (list): 损失异常。\n",
    "    - p_bias (list): 性能偏离。\n",
    "    - client_num (int): 要选出的节点数。\n",
    "\n",
    "    返回：\n",
    "    - selected_clients (list): 选中的 client ID（按输入顺序从 0 开始）。\n",
    "    \"\"\"\n",
    "    print(\"=== Pareto Optimization: Start ===\")\n",
    "    print(\"Input rf_loss:\", [f\"{x:.2f}\" for x in rf_loss])\n",
    "    print(\"Input rf_acc_train:\", [f\"{x:.2f}\" for x in rf_acc_train])\n",
    "    print(\"Input rf_acc_val:\", [f\"{x:.2f}\" for x in rf_acc_val])\n",
    "    print(\"Input rf_acc_global:\", [f\"{x:.2f}\" for x in rf_acc_global])\n",
    "    print(\"Input p_loss:\", [f\"{x:.2f}\" for x in p_loss])\n",
    "    print(\"Input p_bias:\", [f\"{x:.2f}\" for x in p_bias])\n",
    "    print(f\"Number of clients to select: {client_num}\")\n",
    "\n",
    "    # Ensure all arrays are numpy arrays\n",
    "    rf_loss = np.array(list(rf_loss))\n",
    "    rf_acc_train = rf_acc_train.detach().cpu().numpy() if isinstance(rf_acc_train, torch.Tensor) else np.array(rf_acc_train)\n",
    "    rf_acc_val = rf_acc_val.detach().cpu().numpy() if isinstance(rf_acc_val, torch.Tensor) else np.array(rf_acc_val)\n",
    "    rf_acc_global = rf_acc_global.detach().cpu().numpy() if isinstance(rf_acc_global, torch.Tensor) else np.array(rf_acc_global)\n",
    "    p_loss = p_loss.detach().cpu().numpy() if isinstance(p_loss, torch.Tensor) else np.array(p_loss)\n",
    "    p_bias = p_bias.detach().cpu().numpy() if isinstance(p_bias, torch.Tensor) else np.array(p_bias)\n",
    "\n",
    "    print(\"Converted all inputs to numpy arrays.\")\n",
    "\n",
    "    # Construct data matrix\n",
    "    data = np.array([rf_loss, rf_acc_train, rf_acc_val, rf_acc_global, -p_loss, -p_bias]).T\n",
    "    print(f\"Constructed data matrix for Pareto: shape={data.shape}\")\n",
    "\n",
    "    # Pareto front selection\n",
    "    def is_dominated(point, others):\n",
    "        \"\"\"判断 point 是否被 others 支配\"\"\"\n",
    "        return any(np.all(other >= point) and np.any(other > point) for other in others)\n",
    "\n",
    "    pareto_indices = [\n",
    "        i for i, point in enumerate(data) if not is_dominated(point, np.delete(data, i, axis=0))\n",
    "    ]\n",
    "    pareto_clients = pareto_indices\n",
    "    print(f\"Pareto front client indices: {pareto_clients}\")\n",
    "\n",
    "    # If more Pareto clients than needed, randomly select\n",
    "    if len(pareto_clients) > client_num:\n",
    "        selected = random.sample(pareto_clients, client_num)\n",
    "        print(f\"More Pareto clients than needed. Randomly selected: {selected}\")\n",
    "        print(\"=== Pareto Optimization: End ===\")\n",
    "        return [int(x) for x in selected]\n",
    "\n",
    "    # If fewer Pareto clients, fill with best scores\n",
    "    remaining_slots = client_num - len(pareto_clients)\n",
    "    pareto_scores = [0.4 * rf_loss[i] + 0.6 * rf_acc_global[i] for i in range(len(rf_loss))]\n",
    "    print(\"Pareto scores for all clients:\", [f\"{x:.2f}\" for x in pareto_scores])\n",
    "    sorted_indices = np.argsort(pareto_scores)[::-1]  # Descending order\n",
    "    print(\"Sorted indices by Pareto score:\", [int(x) for x in sorted_indices])\n",
    "\n",
    "    selected_clients = set(pareto_clients)\n",
    "    print(f\"Initial selected clients (Pareto front): {[int(x) for x in selected_clients]}\")\n",
    "    for i in sorted_indices:\n",
    "        if len(selected_clients) >= client_num:\n",
    "            break\n",
    "        if i not in selected_clients:\n",
    "            selected_clients.add(int(i))\n",
    "            print(f\"Added client {int(i)} to fill remaining slots.\")\n",
    "            # If we have filled all slots, we can stop\n",
    "            if len(selected_clients) >= client_num:\n",
    "                break\n",
    "\n",
    "    print(f\"Final selected clients: {[int(x) for x in selected_clients]}\")\n",
    "    print(\"=== Pareto Optimization: End ===\")\n",
    "    return [int(x) for x in selected_clients]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebf65c",
   "metadata": {},
   "source": [
    "2. Weighted Sum Method (5RF)\n",
    "\n",
    "This is a more straightforward approach. It calculates a single comprehensive score for each client by taking a weighted sum of all the metrics. Clients with the highest final scores are selected. The weights (0.2, 0.1, 0.3, etc.) determine the importance of each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb04b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_clients_with5RF(rf_loss, rf_acc_train, rf_acc_val, rf_acc_global, p_loss, p_bias, client_num):\n",
    "    rf_loss = np.array(list(rf_loss))\n",
    "    rf_acc_train = np.array(rf_acc_train)\n",
    "    rf_acc_val = np.array(rf_acc_val)\n",
    "    rf_acc_global = np.array(rf_acc_global)\n",
    "    p_loss = np.array(p_loss)\n",
    "    p_bias = np.array(p_bias)\n",
    "\n",
    "    # Calculate a single weighted score for each client\n",
    "    scores = (\n",
    "            0.2 * rf_loss +\n",
    "            0.1 * rf_acc_train +\n",
    "            0.2 * rf_acc_val +\n",
    "            0.3 * rf_acc_global -\n",
    "            0.1 * p_loss -\n",
    "            0.1 * p_bias\n",
    "    )\n",
    "    origin_scores = scores\n",
    "    # Get the indices of the clients with the highest scores\n",
    "    top_client_ids = np.argsort(scores)[::-1][:client_num]  # Sort descending and take the top N\n",
    "    return top_client_ids.tolist(), origin_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee125dce",
   "metadata": {},
   "source": [
    "## Step 2: The AI's Brain (The Model Definition)\n",
    "We have the data pipeline and the client selection logic. Now it's time to build the brain of the operation: the neural network model itself.\n",
    "\n",
    "The model, ModelCSVIMG, is a multi-modal neural network. This means it's designed to accept and process multiple types of data at once. It has three distinct input branches:\n",
    "\n",
    "One for the numerical sensor (CSV) data.\n",
    "\n",
    "One for the images from camera 1.\n",
    "\n",
    "One for the images from camera 2.\n",
    "\n",
    "The features extracted from each branch are then combined (fused) and passed to a final set of layers that perform the classification. The original code contains a few versions of the architecture; we will use the final, most complex one.\n",
    "\n",
    "Add the complete model class to your script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "218bbd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCSVIMG(nn.Module):\n",
    "    def __init__(self, num_csv_features, img_shape1, img_shape2):\n",
    "        super(ModelCSVIMG, self).__init__()\n",
    "\n",
    "        # --- Branch 1: For processing numerical CSV data ---\n",
    "        self.csv_fc_1 = nn.Linear(num_csv_features, 2000)\n",
    "        self.csv_bn_1 = nn.BatchNorm1d(2000)\n",
    "        self.csv_fc_2 = nn.Linear(2000, 600)\n",
    "        self.csv_bn_2 = nn.BatchNorm1d(600)\n",
    "        self.csv_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # --- Branch 2: For processing images from Camera 1 (CNN) ---\n",
    "        self.img1_conv_1 = nn.Conv2d(in_channels=1, out_channels=18, kernel_size=3, stride=1, padding=1)\n",
    "        self.img1_batch_norm = nn.BatchNorm2d(18)\n",
    "        self.img1_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Flattened features from the CNN go into a fully connected layer\n",
    "        self.img1_fc1 = nn.Linear(18 * 16 * 16, 100)\n",
    "        self.img1_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # --- Branch 3: For processing images from Camera 2 (identical to Branch 2) ---\n",
    "        self.img2_conv = nn.Conv2d(in_channels=1, out_channels=18, kernel_size=3, stride=1, padding=1)\n",
    "        self.img2_batch_norm = nn.BatchNorm2d(18)\n",
    "        self.img2_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.img2_fc1 = nn.Linear(18 * 16 * 16, 100)\n",
    "        self.img2_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # --- Fusion and Final Classification Layers ---\n",
    "        # The input size is 600 (from CSV) + 100 (from Image 1) + 100 (from Image 2) = 800\n",
    "        self.fc1 = nn.Linear(800, 1200)\n",
    "        self.dr1 = nn.Dropout(0.2)\n",
    "        # A residual connection is used here: input to fc2 is the original 800 + output of fc1 (1200) = 2000\n",
    "        self.fc2 = nn.Linear(2000, 12) # 12 output classes\n",
    "\n",
    "    def forward(self, x_csv, x_img1, x_img2):\n",
    "        # --- Process CSV data ---\n",
    "        x_csv = F.relu(self.csv_bn_1(self.csv_fc_1(x_csv)))\n",
    "        x_csv = F.relu(self.csv_bn_2(self.csv_fc_2(x_csv)))\n",
    "        x_csv = self.csv_dropout(x_csv)\n",
    "\n",
    "        # --- Process Image 1 data ---\n",
    "        # Reshape image from (batch, height, width, channels) to (batch, channels, height, width)\n",
    "        x_img1 = x_img1.permute(0, 3, 1, 2)\n",
    "        x_img1 = F.relu(self.img1_conv_1(x_img1))\n",
    "        x_img1 = self.img1_batch_norm(x_img1)\n",
    "        x_img1 = self.img1_pool(x_img1)\n",
    "        x_img1 = x_img1.contiguous().view(x_img1.size(0), -1) # Flatten\n",
    "        x_img1 = F.relu(self.img1_fc1(x_img1))\n",
    "        x_img1 = self.img1_dropout(x_img1)\n",
    "\n",
    "        # --- Process Image 2 data ---\n",
    "        x_img2 = x_img2.permute(0, 3, 1, 2)\n",
    "        x_img2 = F.relu(self.img2_conv(x_img2))\n",
    "        x_img2 = self.img2_batch_norm(x_img2)\n",
    "        x_img2 = self.img2_pool(x_img2)\n",
    "        x_img2 = x_img2.contiguous().view(x_img2.size(0), -1) # Flatten\n",
    "        x_img2 = F.relu(self.img2_fc1(x_img2))\n",
    "        x_img2 = self.img2_dropout(x_img2)\n",
    "\n",
    "        # --- Fusion ---\n",
    "        x = torch.cat((x_csv, x_img1, x_img2), dim=1)\n",
    "        residual = x # Keep a copy for the residual connection\n",
    "        \n",
    "        # --- Final layers ---\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dr1(x)\n",
    "        # Concatenate the residual connection\n",
    "        x = torch.cat((residual, x), dim=1)\n",
    "        # Final output with softmax for classification\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f10c36",
   "metadata": {},
   "source": [
    "## Step 3: The Teacher (The Server Class)\n",
    "Alright, we're on the home stretch. We have the data, the selection logic, and the model. Now we need to create the actors for our simulation: the Server and the Client. These two classes will control the entire federated learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e0220",
   "metadata": {},
   "source": [
    "1. The Server Class\n",
    "\n",
    "The Server is the central coordinator. Its job is to:\n",
    "\n",
    "Hold the main global model.\n",
    "\n",
    "Send the global model to the clients.\n",
    "\n",
    "Receive updates from the selected clients.\n",
    "\n",
    "Aggregate these updates to improve the global model.\n",
    "\n",
    "Evaluate the global model's performance on a held-out test set.\n",
    "\n",
    "Here is the code for the Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7c9959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server\n",
    "class Server(object):\n",
    "    def __init__(self, model, epoch_size, eval_dataset, num_clients):\n",
    "        self.global_model = model\n",
    "        self.epoch_size = epoch_size\n",
    "        self.num_clients = num_clients\n",
    "        self.serverTestDataSet = CustomDatasetRes(eval_dataset[0],eval_dataset[1],eval_dataset[2],eval_dataset[3])\n",
    "        self.eval_loader = torch.utils.data.DataLoader(self.serverTestDataSet, batch_size=epoch_size)\n",
    "\n",
    "    def model_aggregate(self, weight_accumulator):\n",
    "        # Averages the weights from the selected clients and updates the global model\n",
    "        for name, data in self.global_model.state_dict().items():\n",
    "            update_per_layer = weight_accumulator[name] * (1/self.num_clients)   # average\n",
    "            if data.type() != update_per_layer.type():\n",
    "                data.add_(update_per_layer.to(torch.int64))\n",
    "            else:\n",
    "                data.add_(update_per_layer)\n",
    "\n",
    "    def model_eval(self):\n",
    "        # Evaluates the global model on the server's test data\n",
    "        self.global_model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        dataset_size = 0\n",
    "        with torch.no_grad(): # Disable gradient calculation for evaluation\n",
    "            for batch_id, batch in enumerate(self.eval_loader):\n",
    "                data1, data2, data3, target = batch\n",
    "                dataset_size += data1.size()[0]\n",
    "\n",
    "                data1 = data1.to(device).float()\n",
    "                data2 = data2.to(device).float()\n",
    "                data3 = data3.to(device).float()\n",
    "                target = target.to(device).float()\n",
    "                \n",
    "                output = self.global_model(data1,data2,data3)\n",
    "                total_loss += nn.functional.cross_entropy(output, target, reduction='sum').item()\n",
    "\n",
    "                pred = output.detach().max(1)[1]\n",
    "                correct += pred.eq(target.detach().max(1)[1].view_as(pred)).cpu().sum().item()\n",
    "\n",
    "        acc = 100.0 * (float(correct) / float(dataset_size))\n",
    "        loss = total_loss / dataset_size\n",
    "        return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32410ea",
   "metadata": {},
   "source": [
    "2. The Client Class and Helper Functions\n",
    "\n",
    "The Client represents an individual participant. Its job is to:\n",
    "\n",
    "Receive the global model from the server.\n",
    "\n",
    "Train this model on its own local data for a few epochs.\n",
    "\n",
    "Calculate the change (the diff) between the original model and its newly trained model.\n",
    "\n",
    "Send this diff back to the server.\n",
    "\n",
    "The client's training process is handled by two helper functions: train_one_epoch and validate.\n",
    "\n",
    "Add the Client class and its two helper functions to your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a5f5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client\n",
    "class Client(object):\n",
    "    def __init__(self, epoch_size, local_epoch_per_round, train_dataset,val_dataset, id = -1):\n",
    "        self.epoch_size = epoch_size\n",
    "        self.local_epoch_per_round = local_epoch_per_round\n",
    "        self.client_id = id\n",
    "        self.train_dataset = CustomDatasetRes(train_dataset[0],train_dataset[1],train_dataset[2],train_dataset[3])\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=epoch_size,shuffle=True)\n",
    "        self.eval_dataset = CustomDatasetRes(val_dataset[0], val_dataset[1], val_dataset[2], val_dataset[3])\n",
    "        self.eval_loader = torch.utils.data.DataLoader(self.eval_dataset, batch_size=epoch_size,shuffle=False)\n",
    "\n",
    "    def local_train(self, global_model):\n",
    "        # Create a local copy of the global model\n",
    "        model = ModelCSVIMG(self.train_dataset.features1.shape[1], 32, 32)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        losses = []\n",
    "        min_loss, max_loss = float('inf'), float('-inf')\n",
    "\n",
    "        for epoch in range(self.local_epoch_per_round):\n",
    "            # Train for one epoch\n",
    "            train_loss, train_acc = train_one_epoch(model, self.train_loader, criterion, optimizer)\n",
    "            \n",
    "            # Track min/max loss and store all epoch losses for evaluation\n",
    "            if train_loss > max_loss: max_loss = train_loss\n",
    "            if train_loss < min_loss: min_loss = train_loss\n",
    "            losses.append(train_loss)\n",
    "\n",
    "        # Validate the model on the local validation set after training\n",
    "        val_loss, val_acc = validate(model, self.eval_loader, criterion)\n",
    "        print(f\"Client {self.client_id} - Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Calculate the difference between the updated local model and the original global model\n",
    "        diff = dict()\n",
    "        for name, data in model.state_dict().items():\n",
    "            diff[name] = (data - global_model.state_dict()[name])\n",
    "            \n",
    "        return model, diff, val_acc, val_loss, min_loss, max_loss, losses, train_acc\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_id, batch in enumerate(train_loader):\n",
    "        data1, data2, data3, target = batch\n",
    "        data1 = data1.to(device).float()\n",
    "        data2 = data2.to(device).float()\n",
    "        data3 = data3.to(device).float()\n",
    "        target = target.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data1, data2, data3)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * data1.size(0)\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.max(1)[1]).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch in enumerate(val_loader):\n",
    "            data1, data2, data3, target = batch\n",
    "            data1 = data1.to(device).float()\n",
    "            data2 = data2.to(device).float()\n",
    "            data3 = data3.to(device).float()\n",
    "            target = target.to(device).float()\n",
    "\n",
    "            output = model(data1, data2, data3)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            running_loss += loss.item() * data1.size(0)\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target.max(1)[1]).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de9c9f",
   "metadata": {},
   "source": [
    "We are almost there! We've built all the major components. Before we assemble everything in the main training loop, we need to add the last few helper functions.\n",
    "\n",
    "These functions are primarily used for a simpler, baseline client selection strategy (referred to as '4RF' in the code) that calculates a single score for each client and picks the best ones.\n",
    "\n",
    "Add these final utility functions to your script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5083d0",
   "metadata": {},
   "source": [
    "1. Normalize\n",
    "\n",
    "A standard function to scale any number to a range between 0 and 1, given a minimum and maximum value. This is useful for combining metrics that have different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3aec0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(value, min_value, max_value):\n",
    "    # Avoid division by zero if min and max are the same\n",
    "    if (max_value - min_value) == 0:\n",
    "        return 0\n",
    "    return (value - min_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259c438",
   "metadata": {},
   "source": [
    "2. Evaluate Model Score\n",
    "\n",
    "This function calculates a simple, combined score for a client. It's a weighted average of their performance on their local training set and a global validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "beb2c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(acc, loss, min_loss, max_loss, oneclient_test_acc, oneclient_test_loss,alpha=0.8, beta=0.8):\n",
    "    normalized_loss = normalize(loss, min_loss, max_loss)\n",
    "    # Score based on local training performance\n",
    "    train_score = alpha * acc + (1 - alpha) * (1 - normalized_loss) # Use (1 - loss) so higher is better\n",
    "\n",
    "    # Score based on global validation performance\n",
    "    val_score = beta * oneclient_test_acc + (1 - beta) * (1 - oneclient_test_loss)\n",
    "\n",
    "    # Final combined score\n",
    "    combined_score = (train_score + val_score) / 2\n",
    "    return combined_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d2e60b",
   "metadata": {},
   "source": [
    "3. Get Top Clients\n",
    "\n",
    "A straightforward function that takes a dictionary of clients and their scores, then returns a list of the top num clients with the highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5f5a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_clients(client_dict, num):\n",
    "    # Sort the clients by their score (the dictionary value) in descending order\n",
    "    sorted_clients = sorted(client_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Extract just the IDs (the dictionary key) of the top clients\n",
    "    top_clients = [client[0] for client in sorted_clients[:num]]\n",
    "    return top_clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c9e3e",
   "metadata": {},
   "source": [
    "4. Dynamic Threshold Selection\n",
    "\n",
    "This is another, more advanced selection method included in the script. It selects clients whose scores are above a dynamic threshold (calculated from the mean and standard deviation of all scores). While not used in the final configuration, we include it for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8c10e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_nodes_with_dynamic_threshold(node_scores, max_nodes, std_multiplier=1.0):\n",
    "    \"\"\"\n",
    "    Selects nodes using a dynamic threshold based on score distribution.\n",
    "    \"\"\"\n",
    "    if not node_scores:\n",
    "        return []\n",
    "        \n",
    "    scores = np.array(list(node_scores.values()))\n",
    "    \n",
    "    # Calculate the dynamic threshold\n",
    "    mean_score = np.mean(scores)\n",
    "    std_dev = np.std(scores)\n",
    "    dynamic_threshold = mean_score + std_multiplier * std_dev\n",
    "\n",
    "    # Select nodes above the threshold\n",
    "    selected_nodes = [\n",
    "        node_id for node_id, score in node_scores.items() if score >= dynamic_threshold\n",
    "    ]\n",
    "\n",
    "    # If too many nodes were selected, keep only the best ones\n",
    "    if len(selected_nodes) > max_nodes:\n",
    "        selected_nodes = sorted(\n",
    "            selected_nodes, key=lambda node_id: node_scores[node_id], reverse=True\n",
    "        )[:max_nodes]\n",
    "\n",
    "    # If not enough nodes were selected, add the next best ones to meet the quota\n",
    "    if len(selected_nodes) < max_nodes:\n",
    "        remaining_nodes = [\n",
    "            node_id for node_id in node_scores if node_id not in selected_nodes\n",
    "        ]\n",
    "        remaining_nodes = sorted(\n",
    "            remaining_nodes, key=lambda node_id: node_scores[node_id], reverse=True\n",
    "        )\n",
    "        selected_nodes += remaining_nodes[: max_nodes - len(selected_nodes)]\n",
    "\n",
    "    return selected_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc43cf88",
   "metadata": {},
   "source": [
    "## Step 4: Model Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae88ddb8",
   "metadata": {},
   "source": [
    "Here we go. This is the big one. We'll now write the trainValModelCSVIMG function. This function is the conductor of our orchestra—it brings together the data, the model, the server, and the clients to run the entire federated learning simulation from start to finish.\n",
    "\n",
    "Because it's so long and important, we'll build it in three parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0daed1",
   "metadata": {},
   "source": [
    "### Part 1: Initialization and Starting the Training Loop\n",
    "\n",
    "First, we'll define the function and set everything up. This includes:\n",
    "\n",
    "Creating the global model, the Server, and all the Client objects.\n",
    "\n",
    "Initializing a series of dictionaries to log every possible metric (loss, accuracy, selection scores, etc.) for every client and every round. This is crucial for analyzing the experiment later.\n",
    "\n",
    "Starting the main training loop, which iterates through the communication rounds.\n",
    "\n",
    "Inside the loop, we'll begin the first phase of a round: every client trains locally on the current global model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2cf43",
   "metadata": {},
   "source": [
    "### Part 2: Client Selection, Aggregation, and Global Evaluation\n",
    "In this part of the trainValModelCSVIMG function, the server performs the following steps:\n",
    "\n",
    "Evaluate: It uses all the metrics gathered from the clients to calculate the advanced performance scores (RF_loss, P_bias, etc.).\n",
    "\n",
    "Select: Based on the chosen selection method (svmethod), it picks the top-performing clients for this round.\n",
    "\n",
    "Aggregate: It averages the model updates (diffs) from only the selected clients to create a new, improved global model.\n",
    "\n",
    "Evaluate Globally: It tests the new global model's performance on the entire held-out test set.\n",
    "\n",
    "Save Best Model: If the new global model is the best one seen so far, its state is saved to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85016d00",
   "metadata": {},
   "source": [
    "### Part 3: Final Test and Saving Results\n",
    "Now that the training is finished, we need to do two last things:\n",
    "\n",
    "Load the best model that we saved during training and run a final, definitive test on it. This gives us the final performance numbers for our experiment.\n",
    "\n",
    "Save all the logs we've been collecting into a CSV file. This is essential for creating plots and analyzing the training process, client behavior, and the effectiveness of the selection strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainValModelCSVIMG(model_name, svmethod, total_client,num_clients,epoch,max_acc,epoch_size,local_epoch_per_round,round_early_stop,\n",
    "                        X_train_csv_scaled_splits, X_test_csv_scaled_splits,\n",
    "                        X_train_1_scaled_splits, X_test_1_scaled_splits,\n",
    "                        X_train_2_scaled_splits, X_test_2_scaled_splits,\n",
    "                        Y_train_csv_splits, Y_test_csv_splits):\n",
    "    # --- 1. Initialization ---\n",
    "    # Instantiate the global model and move it to the GPU if available\n",
    "    model_MLP = ModelCSVIMG(X_train_csv_scaled_splits[0].shape[1], 32, 32)\n",
    "    model_MLP = model_MLP.to(device)\n",
    "\n",
    "    # The last client's test data is reserved for the server's global evaluation\n",
    "    server = Server(model_MLP, epoch_size, [X_test_csv_scaled_splits[total_client-1], X_test_1_scaled_splits[total_client-1], X_test_2_scaled_splits[total_client-1], Y_test_csv_splits[total_client-1]], num_clients)\n",
    "    \n",
    "    # Create a list of all clients\n",
    "    clients = []\n",
    "    for client_index in range(total_client):\n",
    "        clients.append(Client(epoch_size=epoch_size, local_epoch_per_round=local_epoch_per_round,\n",
    "                              train_dataset=[X_train_csv_scaled_splits[client_index], X_train_1_scaled_splits[client_index], X_train_2_scaled_splits[client_index],\n",
    "                                             Y_train_csv_splits[client_index]],\n",
    "                              val_dataset=[X_test_csv_scaled_splits[client_index], X_test_1_scaled_splits[client_index], X_test_2_scaled_splits[client_index],\n",
    "                                           Y_test_csv_splits[client_index]], id=client_index))\n",
    "\n",
    "    # --- Dictionaries for Logging ---\n",
    "    clients_scoresDict = {}\n",
    "    perEpoch_clients_losses = {}\n",
    "    perEpoch_clients_train_acc = {}\n",
    "    perEpoch_clients_local_test_acc = {}\n",
    "    perEpoch_clients_global_test_acc = {}\n",
    "    clients_train_acc = {}\n",
    "    clients_train_loss = {}\n",
    "    clients_test_acc = {}\n",
    "    clients_test_loss = {}\n",
    "    clients_rf_relative_loss_reduction = {}\n",
    "    clients_rf_acc_train = {}\n",
    "    clients_rf_global_validation_accuracy = {}\n",
    "    clients_rf_loss_outliers = {}\n",
    "    clients_rf_performance_bias = {}\n",
    "    clients_epoch_selected = {}\n",
    "\n",
    "    for i in range(total_client + 1):  # +1 for the server/global model\n",
    "        clients_train_acc[i] = []\n",
    "        clients_train_loss[i] = []\n",
    "        clients_test_acc[i] = []\n",
    "        clients_test_loss[i] = []\n",
    "        clients_scoresDict[i] = []\n",
    "        clients_rf_relative_loss_reduction[i] = []\n",
    "        clients_rf_acc_train[i] = []\n",
    "        clients_rf_global_validation_accuracy[i] = []\n",
    "        clients_rf_loss_outliers[i] = []\n",
    "        clients_rf_performance_bias[i] = []\n",
    "        clients_epoch_selected[i] = []\n",
    "\n",
    "    epoch_count = 0\n",
    "    # --- 2. Main Federated Learning Loop ---\n",
    "    for e in range(epoch):\n",
    "        print(f\"--- Round {e+1}/{epoch} ---\")\n",
    "        if epoch_count >= round_early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "            \n",
    "        diff_client = {}\n",
    "        weight_accumulator = {}\n",
    "        for name, params in server.global_model.state_dict().items():\n",
    "            weight_accumulator[name] = torch.zeros_like(params)\n",
    "\n",
    "        # --- Phase 1: All clients perform local training ---\n",
    "        for client_index in range(total_client):\n",
    "            round_client_model, diff, test_acc_client, loss_client, min_loss, max_loss, losses, train_acc = clients[client_index].local_train(server.global_model)\n",
    "            \n",
    "            # Store results for this client\n",
    "            perEpoch_clients_losses[client_index] = losses\n",
    "            perEpoch_clients_train_acc[client_index] = train_acc\n",
    "            perEpoch_clients_local_test_acc[client_index] = test_acc_client\n",
    "            diff_client[client_index] = diff\n",
    "            \n",
    "            # Evaluate this client's trained model on the entire global test set\n",
    "            correct = 0\n",
    "            dataset_size = 0\n",
    "            with torch.no_grad():\n",
    "                for test_data_index in range(total_client): # Loop through all test splits\n",
    "                    test_server_loader = torch.utils.data.DataLoader(\n",
    "                        CustomDatasetRes(X_test_csv_scaled_splits[test_data_index], X_test_1_scaled_splits[test_data_index],\n",
    "                                      X_test_2_scaled_splits[test_data_index], Y_test_csv_splits[test_data_index]),\n",
    "                        batch_size=epoch_size)\n",
    "                    \n",
    "                    round_client_model.eval()\n",
    "                    for batch_id, batch in enumerate(test_server_loader):\n",
    "                        data1, data2, data3, target = batch\n",
    "                        dataset_size += data1.size()[0]\n",
    "                        data1, data2, data3, target = data1.to(device).float(), data2.to(device).float(), data3.to(device).float(), target.to(device).float()\n",
    "                        \n",
    "                        output = round_client_model(data1, data2, data3)\n",
    "                        pred = output.detach().max(1)[1]\n",
    "                        correct += pred.eq(target.detach().max(1)[1].view_as(pred)).cpu().sum().item()\n",
    "\n",
    "            oneclient_global_test_acc = 100.0 * (correct / dataset_size)\n",
    "            perEpoch_clients_global_test_acc[client_index] = oneclient_global_test_acc\n",
    "            \n",
    "            # Log the local and global test accuracies for this round\n",
    "            clients_train_acc[client_index].append(test_acc_client)\n",
    "            clients_train_loss[client_index].append(loss_client)\n",
    "            clients_test_acc[client_index].append(oneclient_global_test_acc)\n",
    "            clients_epoch_selected[client_index].append(0) # Mark as not selected (yet)\n",
    "\n",
    "        # --- Phase 2: Server evaluates, selects, and aggregates ---\n",
    "        # Calculate all the advanced performance metrics for each client\n",
    "        rf_relative_loss_reduction = calculate_relative_loss_reduction_as_list(perEpoch_clients_losses)\n",
    "        rf_acc_train = calculate_relative_train_accuracy(perEpoch_clients_train_acc)\n",
    "        rf_global_validation_accuracy = calculate_global_validation_accuracy(perEpoch_clients_train_acc, perEpoch_clients_global_test_acc)\n",
    "        rf_loss_outliers = calculate_loss_outliers(perEpoch_clients_losses)\n",
    "        rf_performance_bias = calculate_performance_bias(perEpoch_clients_local_test_acc, perEpoch_clients_global_test_acc)\n",
    "        \n",
    "        # Log these calculated metrics\n",
    "        for client_index in range(total_client):\n",
    "            clients_rf_relative_loss_reduction[client_index].append(rf_relative_loss_reduction[client_index])\n",
    "            clients_rf_acc_train[client_index].append(rf_acc_train[client_index])\n",
    "            clients_rf_global_validation_accuracy[client_index].append(rf_global_validation_accuracy[client_index])\n",
    "            clients_rf_loss_outliers[client_index].append(rf_loss_outliers[client_index])\n",
    "            clients_rf_performance_bias[client_index].append(rf_performance_bias[client_index])\n",
    "\n",
    "        # --- Select clients based on the specified method ---\n",
    "        candidates = []\n",
    "        if svmethod == '5RF':\n",
    "            # 1. CORRECTLY calculate rf_acc_val using the local validation accuracies.\n",
    "            #    perEpoch_clients_local_test_acc is a dictionary with 12 client entries.\n",
    "            rf_acc_val = calculate_relative_validation_accuracy(perEpoch_clients_local_test_acc)\n",
    "\n",
    "            candidates, scores = get_top_clients_with5RF(rf_relative_loss_reduction, rf_acc_train, rf_acc_val,\n",
    "                                                         rf_global_validation_accuracy, rf_loss_outliers, rf_performance_bias,\n",
    "                                                         num_clients)\n",
    "            for index in range(len(scores)):\n",
    "                clients_scoresDict[index].append(scores[index])\n",
    "\n",
    "        elif svmethod == 'pareto':\n",
    "            # 1. CORRECTLY calculate rf_acc_val using the local validation accuracies.\n",
    "            #    perEpoch_clients_local_test_acc is a dictionary with 12 client entries.\n",
    "            rf_acc_val = calculate_relative_validation_accuracy(perEpoch_clients_local_test_acc)\n",
    "            \n",
    "            # 2. Call pareto_optimization with the correctly-sized lists.\n",
    "            candidates = pareto_optimization(rf_relative_loss_reduction, rf_acc_train, rf_acc_val,\n",
    "                                              rf_global_validation_accuracy, rf_loss_outliers, rf_performance_bias,\n",
    "                                              num_clients)\n",
    "            \n",
    "        elif svmethod == 'random':\n",
    "            candidates = np.random.choice(total_client, num_clients, replace=False)\n",
    "\n",
    "        print(f\"Selected clients for aggregation: {candidates}\")\n",
    "\n",
    "        # Mark which clients were selected in the log\n",
    "        for selected_client_index in candidates:\n",
    "            clients_epoch_selected[selected_client_index][-1] = 1\n",
    "        \n",
    "        # --- Aggregate the updates from selected clients ---\n",
    "        for slected_client_index in candidates:\n",
    "            for name, params in server.global_model.state_dict().items():\n",
    "                weight_accumulator[name].add_(diff_client[slected_client_index][name])\n",
    "        \n",
    "        server.model_aggregate(weight_accumulator)\n",
    "\n",
    "        # --- Phase 3: Evaluate the new global model ---\n",
    "        acc, loss = server.model_eval()\n",
    "        \n",
    "        # Log global model performance\n",
    "        clients_test_acc[total_client].append(acc)\n",
    "        clients_test_loss[total_client].append(loss)\n",
    "        \n",
    "        print(f\"Round {e+1} Global Model - Accuracy: {acc:.2f}%, Loss: {loss:.4f}\\n\")\n",
    "\n",
    "        # --- Save the best model and check for early stopping ---\n",
    "        epoch_count += 1\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            print(\"New best model found! Saving model...\")\n",
    "            torch.save(server.global_model.state_dict(),\n",
    "                       f\"./acc_lossFiles/{model_name}_totalClient_{total_client}_NumClient_{num_clients}_epoch_{epoch}_svmethod_{svmethod}.pth\")\n",
    "            epoch_count = 0 # Reset early stopping counter\n",
    "        \n",
    "    # --- After the training loop, perform a final evaluation on the best model ---\n",
    "    print(\"\\n--- Final Evaluation on Best Model ---\")\n",
    "    model = model_MLP\n",
    "    model.load_state_dict(torch.load(\n",
    "        f\"./acc_lossFiles/{model_name}_totalClient_{total_client}_NumClient_{num_clients}_epoch_{epoch}_svmethod_{svmethod}.pth\"))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    y_test, y_predict = [], []\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    dataset_size = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_data_index in range(total_client):\n",
    "            test_server_loader = torch.utils.data.DataLoader(\n",
    "                CustomDatasetRes(X_test_csv_scaled_splits[test_data_index], X_test_1_scaled_splits[test_data_index],\n",
    "                              X_test_2_scaled_splits[test_data_index], Y_test_csv_splits[test_data_index]),\n",
    "                batch_size=epoch_size)\n",
    "            for batch_id, batch in enumerate(test_server_loader):\n",
    "                data1, data2, data3, target = batch\n",
    "                dataset_size += data1.size()[0]\n",
    "                data1, data2, data3, target = data1.to(device).float(), data2.to(device).float(), data3.to(device).float(), target.to(device).float()\n",
    "\n",
    "                output = model(data1, data2, data3)\n",
    "                total_loss += nn.functional.cross_entropy(output, target, reduction='sum').item()\n",
    "                y_test.append(target.detach().max(1)[1].cpu().numpy())\n",
    "                y_predict.append(output.detach().max(1)[1].cpu().numpy())\n",
    "                pred = output.detach().max(1)[1]\n",
    "                correct += pred.eq(target.detach().max(1)[1].view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    acc = 100.0 * (correct / dataset_size)\n",
    "    loss = total_loss / dataset_size\n",
    "    print(f'Final Best Model Test Accuracy: {acc:.2f}%')\n",
    "    print(f'Final Best Model Test Loss: {loss:.4f}')\n",
    "    print(f'Max accuracy achieved during training: {max_acc:.2f}%')\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # --- Save all logged data to a CSV file (Robust Version) ---\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    csv_file_name = f\"./acc_lossFiles/{model_name}_totalClient_{total_client}_NumClient_{num_clients}_epoch_{epoch}_svmethod_{svmethod}.csv\"\n",
    "    \n",
    "    header = ['client_id', 'client_name', 'Epoch', 'local_val_loss', 'local_val_accuracy', 'global_test_loss', 'global_test_accuracy',\n",
    "              'rf_loss', 'rf_acc_train', 'rf_acc_val', 'rf_acc_global', 'p_loss', 'p_bias', 'selected']\n",
    "    \n",
    "    client_name_map = {i: name for i, name in enumerate(sensor_clients.keys())}\n",
    "    client_name_map[total_client] = 'Global_Model'\n",
    "\n",
    "    all_rows = []\n",
    "    # Loop through each client (and the server entry)\n",
    "    for i in range(total_client + 1):\n",
    "        # Gather all lists for the current client\n",
    "        train_losses = clients_train_loss.get(i, [])\n",
    "        train_accs = clients_train_acc.get(i, [])\n",
    "        test_losses = clients_test_loss.get(i, [])\n",
    "        test_accs = clients_test_acc.get(i, [])\n",
    "        rf_losses = clients_rf_relative_loss_reduction.get(i, [])\n",
    "        rf_acc_trains = clients_rf_acc_train.get(i, [])\n",
    "        rf_acc_vals = clients_rf_acc_val.get(i, [])\n",
    "        rf_acc_globals = clients_rf_global_validation_accuracy.get(i, [])\n",
    "        p_losses = clients_rf_loss_outliers.get(i, [])\n",
    "        p_biases = clients_rf_performance_bias.get(i, [])\n",
    "        selecteds = clients_epoch_selected.get(i, [])\n",
    "\n",
    "        # **FIX 1: Find the maximum length across ALL metric lists for this client**\n",
    "        max_epochs = max(len(lst) for lst in [\n",
    "            train_losses, train_accs, test_losses, test_accs, rf_losses, \n",
    "            rf_acc_trains, rf_acc_vals, rf_acc_globals, p_losses, p_biases, selecteds\n",
    "        ])\n",
    "\n",
    "        # Loop up to the maximum number of epochs found\n",
    "        for j in range(max_epochs):\n",
    "            # **FIX 2: Safely get each value, providing '' if the index is out of bounds**\n",
    "            row_data = {\n",
    "                'client_id': i,\n",
    "                'client_name': client_name_map.get(i, ''),\n",
    "                'Epoch': j + 1,\n",
    "                'local_val_loss': train_losses[j] if j < len(train_losses) else '',\n",
    "                'local_val_accuracy': train_accs[j] if j < len(train_accs) else '',\n",
    "                'global_test_loss': test_losses[j] if j < len(test_losses) else '',\n",
    "                'global_test_accuracy': test_accs[j] if j < len(test_accs) else '',\n",
    "                'rf_loss': rf_losses[j] if j < len(rf_losses) else '',\n",
    "                'rf_acc_train': rf_acc_trains[j] if j < len(rf_acc_trains) else '',\n",
    "                'rf_acc_val': rf_acc_vals[j] if j < len(rf_acc_vals) else '',\n",
    "                'rf_acc_global': rf_acc_globals[j] if j < len(rf_acc_globals) else '',\n",
    "                'p_loss': p_losses[j] if j < len(p_losses) else '',\n",
    "                'p_bias': p_biases[j] if j < len(p_biases) else '',\n",
    "                'selected': selecteds[j] if j < len(selecteds) else ''\n",
    "            }\n",
    "            all_rows.append(row_data)\n",
    "\n",
    "    results_df = pd.DataFrame(all_rows, columns=header)\n",
    "    results_df.to_csv(csv_file_name, index=False)\n",
    "    print(f\"✅ Results successfully and safely saved to {csv_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6e7cb",
   "metadata": {},
   "source": [
    "We've arrived at the final step! We have all the building blocks in place. The only thing left is to set our experimental parameters and create the main execution block that calls our functions and runs the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d287713",
   "metadata": {},
   "source": [
    "## Final Step: The main Function and Execution Block\n",
    "This final piece of code does the following:\n",
    "\n",
    "Sets Hyperparameters: Defines all the key variables for the experiment, like the number of clients, epochs, learning rate, etc. It also defines the different scenarios we want to test (e.g., different client selection methods, different data corruption scenarios).\n",
    "\n",
    "Defines a main() function: This function orchestrates the experiment. It loads the client data, then loops through each experimental scenario. For scenarios involving \"model loss,\" it intentionally corrupts the data for some clients (e.g., replacing their sensor data with random noise) to simulate system failures or unreliable participants.\n",
    "\n",
    "Calls trainValModelCSVIMG: For each scenario, it calls our main training function to run a full federated learning simulation.\n",
    "\n",
    "Executes main(): The standard if __name__ == \"__main__\": line ensures that the main function is called when you run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13fdc2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Scenario: Corrupted CSV Data ---\n",
      "\n",
      "===== STARTING NEW EXPERIMENT: Model=tc1c2ResModelV3DataV3AdamWithSCVLost, Selection=pareto =====\n",
      "--- Round 1/10 ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# This makes the script runnable\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 57\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m svmethod \u001b[38;5;129;01min\u001b[39;00m svmethods:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== STARTING NEW EXPERIMENT: Model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Selection=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msvmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mtrainValModelCSVIMG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epoch_per_round\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mround_early_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mX_train_csv_scaled_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_csv_scaled_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mX_train_1_scaled_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_1_scaled_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mX_train_2_scaled_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_2_scaled_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mY_train_csv_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test_csv_splits\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 68\u001b[0m, in \u001b[0;36mtrainValModelCSVIMG\u001b[0;34m(model_name, svmethod, total_client, num_clients, epoch, max_acc, epoch_size, local_epoch_per_round, round_early_stop, X_train_csv_scaled_splits, X_test_csv_scaled_splits, X_train_1_scaled_splits, X_test_1_scaled_splits, X_train_2_scaled_splits, X_test_2_scaled_splits, Y_train_csv_splits, Y_test_csv_splits)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# --- Phase 1: All clients perform local training ---\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_client):\n\u001b[0;32m---> 68\u001b[0m     round_client_model, diff, test_acc_client, loss_client, min_loss, max_loss, losses, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mclients\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Store results for this client\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     perEpoch_clients_losses[client_index] \u001b[38;5;241m=\u001b[39m losses\n",
      "Cell \u001b[0;32mIn[40], line 26\u001b[0m, in \u001b[0;36mClient.local_train\u001b[0;34m(self, global_model)\u001b[0m\n\u001b[1;32m     22\u001b[0m min_loss, max_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_epoch_per_round):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Track min/max loss and store all epoch losses for evaluation\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_loss \u001b[38;5;241m>\u001b[39m max_loss: max_loss \u001b[38;5;241m=\u001b[39m train_loss\n",
      "Cell \u001b[0;32mIn[40], line 62\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 62\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m data1\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     65\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/optim/adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    237\u001b[0m         group,\n\u001b[1;32m    238\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         state_steps,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/optim/optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/optim/adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 933\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/flwr/lib/python3.10/site-packages/torch/optim/adam.py:757\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    755\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 757\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    760\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Experimental Scenarios and Hyperparameters ---\n",
    "model_names = {'tc1c2ResModelV3DataV3Adam', \n",
    "               'tc1c2ResModelV3DataV3AdamWithSCVLost', \n",
    "               'tc1c2ResModelV3DataV3AdamWithImgLost'}\n",
    "\n",
    "#model_names = {'tc1c2ResModelV3DataV3Adam'}\n",
    "#svmethods = {'pareto', '5RF', 'random'}\n",
    "svmethods = {'pareto'}  # For testing purposes, you can change this to 'pareto' or 'random'\n",
    "# --- Hyperparameters ---\n",
    "max_acc = 1  # Threshold of accuracy for saving the best model\n",
    "epoch = 10  # Total number of communication rounds\n",
    "epoch_size = 64  # Batch size\n",
    "total_client = 12  # Total number of clients in the pool\n",
    "num_clients = 6  # Number of clients selected per round\n",
    "local_epoch_per_round = 3  # Number of local training epochs for each client per round\n",
    "round_early_stop = 10 # Number of rounds without improvement before stopping\n",
    "server_client_index = random.randint(0, total_client - 1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the data pre-split for each of the 12 clients\n",
    "    X_train_csv_scaled_splits, X_test_csv_scaled_splits, \\\n",
    "    Y_train_csv_splits, Y_test_csv_splits, \\\n",
    "    X_train_1_scaled_splits, X_test_1_scaled_splits, \\\n",
    "    Y_train_1_splits, Y_test_1_splits, \\\n",
    "    X_train_2_scaled_splits, X_test_2_scaled_splits, \\\n",
    "    Y_train_2_splits, Y_test_2_splits = loadClientsData()\n",
    "\n",
    "    # Loop through each experimental scenario\n",
    "    for model_name in model_names:\n",
    "        # --- Simulate Data Corruption Scenarios ---\n",
    "        if model_name == 'tc1c2ResModelV3DataV3AdamWithSCVLost':\n",
    "            print(\"\\n--- Running Scenario: Corrupted CSV Data ---\")\n",
    "            # Replace CSV data with random noise for clients 6 through 11\n",
    "            for index in [6, 7, 8, 9, 10, 11]:\n",
    "                shape_train = X_train_csv_scaled_splits[index].shape\n",
    "                X_train_csv_scaled_splits[index] = np.random.rand(*shape_train)\n",
    "                shape_test = X_test_csv_scaled_splits[index].shape\n",
    "                X_test_csv_scaled_splits[index] = np.random.rand(*shape_test)\n",
    "        elif model_name == 'tc1c2ResModelV3DataV3AdamWithImgLost':\n",
    "            print(\"\\n--- Running Scenario: Corrupted Image Data ---\")\n",
    "            # Replace Image data with random noise for clients 6 through 11\n",
    "            for index in [6, 7, 8, 9, 10, 11]:\n",
    "                shape_train1 = X_train_1_scaled_splits[index].shape\n",
    "                X_train_1_scaled_splits[index] = np.random.rand(*shape_train1)\n",
    "                shape_test1 = X_test_1_scaled_splits[index].shape\n",
    "                X_test_1_scaled_splits[index] = np.random.rand(*shape_test1)\n",
    "                shape_train2 = X_train_2_scaled_splits[index].shape\n",
    "                X_train_2_scaled_splits[index] = np.random.rand(*shape_train2)\n",
    "                shape_test2 = X_test_2_scaled_splits[index].shape\n",
    "                X_test_2_scaled_splits[index] = np.random.rand(*shape_test2)\n",
    "\n",
    "        # Loop through each client selection method\n",
    "        for svmethod in svmethods:\n",
    "            print(f\"\\n===== STARTING NEW EXPERIMENT: Model={model_name}, Selection={svmethod} =====\")\n",
    "            trainValModelCSVIMG(model_name, svmethod, total_client, num_clients, epoch, max_acc, epoch_size, local_epoch_per_round, round_early_stop,\n",
    "                                X_train_csv_scaled_splits, X_test_csv_scaled_splits,\n",
    "                                X_train_1_scaled_splits, X_test_1_scaled_splits,\n",
    "                                X_train_2_scaled_splits, X_test_2_scaled_splits,\n",
    "                                Y_train_csv_splits, Y_test_csv_splits)\n",
    "\n",
    "# This makes the script runnable\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921306f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
