{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0221b1e3",
   "metadata": {},
   "source": [
    "## Step 1: The Foundation (Imports and Setup)\n",
    "Every Python script starts with importing the necessary libraries and setting up the environment. This is like laying the foundation for a house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696a41f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "# -- Basic Setup --\n",
    "# Set the device to use the GPU if available, otherwise use the CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b56c2",
   "metadata": {},
   "source": [
    "### Define dataset loader\n",
    "\n",
    "PyTorch uses a Dataset object to handle data loading. Since our model will take three different kinds of input (sensor data, image 1, and image 2), we need to create a special class that tells PyTorch how to retrieve one sample of each, along with its corresponding label.\n",
    "\n",
    "This class will have three essential methods:\n",
    "\n",
    "__init__: Initializes the dataset by storing our feature and label arrays.\n",
    "\n",
    "__len__: Returns the total number of samples in the dataset.\n",
    "\n",
    "__getitem__: Fetches a single data sample at a given index.\n",
    "\n",
    "Here is the code for it. Add this to your script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ebd8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset loader\n",
    "class CustomDatasetRes(Dataset):\n",
    "    def __init__(self, features1, features2, features3, labels):\n",
    "        self.features1 = features1\n",
    "        self.features2 = features2\n",
    "        self.features3 = features3\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.features1[index], self.features2[index], self.features3[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03702557",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "Next, we'll add a few helper functions. These functions will perform common tasks that we'll need later, like displaying results, scaling data, and ensuring our experiments are reproducible.\n",
    "\n",
    "1. display_result\n",
    "\n",
    "This function takes the true labels (y_test) and the model's predicted labels (y_pred) and prints out standard performance metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e6414e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result(y_test, y_pred):\n",
    "    print('Accuracy score : ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision score : ', precision_score(y_test, y_pred, average='weighted'))\n",
    "    print('Recall score : ', recall_score(y_test, y_pred, average='weighted'))\n",
    "    print('F1 score : ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a5531",
   "metadata": {},
   "source": [
    "2. scaled_data\n",
    "\n",
    "This function uses Scikit-learn's StandardScaler to normalize the sensor (CSV) data. Scaling is crucial because it ensures that features with larger value ranges don't dominate the learning process. Notice there are two functions with the same name in the original code. In Python, the last definition of a function is the one that gets used. We will add both for completeness, but just know that the first one is effectively overwritten by the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc0678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test, X_val):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    return X_train_scaled, X_test_scaled, X_val_scaled\n",
    "\n",
    "def scaled_data(X_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    return X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d51f72",
   "metadata": {},
   "source": [
    "3. set_seed\n",
    "\n",
    "This is a very important function for reproducibility. Machine learning involves a lot of randomness (e.g., initializing model weights, shuffling data). By setting a \"seed,\" we ensure that the sequence of random numbers is the same every time we run the code, which means we'll get the exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f4630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    # Sets the environment variable for Python's hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # Sets the seed for NumPy's random number generator\n",
    "    np.random.seed(seed)\n",
    "    # Sets the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    # Sets the seed for PyTorch's random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    # If using a GPU, sets the seed for all CUDA devices\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "    # Ensures deterministic behavior in cuDNN (CUDA Deep Neural Network library)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf1683",
   "metadata": {},
   "source": [
    "### loading and preprocessing the data.\n",
    "\n",
    "The function loadClientsData is designed for a federated learning scenario. It reads data from separate files for each participant (or \"client\"), cleans it, aligns the different data types (sensor vs. image), and splits it into training and testing sets for each client.\n",
    "\n",
    "Because this function is quite long, we'll build it in a few parts.\n",
    "\n",
    "#### Part 1: Initializing and Processing Training Data\n",
    "First, we'll define the function, list the subject IDs we want to load, and create empty dictionaries to store each client's data. Then, we'll start a loop to process each subject one by one. Inside the loop, we'll begin by loading and cleaning the training data.\n",
    "\n",
    "This involves:\n",
    "\n",
    "Reading the sensor data from a CSV file.\n",
    "\n",
    "Removing rows with missing values and any duplicate rows.\n",
    "\n",
    "Dropping columns that we don't need (like the 'Infrared' sensor readings).\n",
    "\n",
    "Loading the corresponding image, label, and timestamp data from .npy files.\n",
    "\n",
    "#### Part 2: Aligning and Preparing Training Data\n",
    "After loading the raw data, we face a common problem: the datasets don't perfectly match. Because we dropped rows with missing values from the sensor (CSV) data, there are now timestamps in our image data that no longer have a corresponding entry in the sensor data.\n",
    "\n",
    "We need to align them by removing the image samples that don't have a matching sensor reading.\n",
    "\n",
    "After alignment, we'll prepare the data for the model:\n",
    "\n",
    "Set the seed for reproducibility.\n",
    "\n",
    "Separate features from labels.\n",
    "\n",
    "One-hot encode the labels, converting them into a format suitable for the model's output layer (e.g., class 3 becomes [0, 0, 0, 1, 0, ...]).\n",
    "\n",
    "Scale the numeric sensor data and the image pixel values.\n",
    "\n",
    "Reshape the images to the format expected by the convolutional layers.\n",
    "\n",
    "#### Part 3: Processing the Test Data and Finalizing the Function\n",
    "The logic here is identical to what we just did for the training data:\n",
    "\n",
    "Load the test sensor data (_test.csv) and test image data (_test.npy).\n",
    "\n",
    "Clean the sensor data by removing missing values and unnecessary columns.\n",
    "\n",
    "Align the test image data with the cleaned test sensor data.\n",
    "\n",
    "Prepare the aligned test data (one-hot encode labels, scale features, reshape images).\n",
    "\n",
    "Store all the processed training and test arrays into our dictionaries.\n",
    "\n",
    "Increment the clint_index and repeat the process for the next subject.\n",
    "\n",
    "After the loop finishes, the function returns all the dictionaries containing the data for every client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81f6095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadClientsData():\n",
    "    subs = [1, 3, 4, 7, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "    X_train_csv_scaled_splits = {}\n",
    "    X_test_csv_scaled_splits = {}\n",
    "    Y_train_csv_splits = {}\n",
    "    Y_test_csv_splits = {}\n",
    "    X_train_1_scaled_splits = {}\n",
    "    X_test_1_scaled_splits = {}\n",
    "    Y_train_1_splits = {}\n",
    "    Y_test_1_splits = {}\n",
    "    X_train_2_scaled_splits = {}\n",
    "    X_test_2_scaled_splits = {}\n",
    "    Y_train_2_splits = {}\n",
    "    Y_test_2_splits = {}\n",
    "    clint_index = 0\n",
    "    for sub_ in subs:\n",
    "        # --- Load and clean TRAINING sensor data (CSV) ---\n",
    "        SUB_train = pd.read_csv('./dataset/Sensor + Image/{}_sensor_train.csv'.format(sub_), skiprows=1)\n",
    "        SUB_train.head()\n",
    "        \n",
    "        SUB_train.isnull().sum()\n",
    "        NA_cols = SUB_train.columns[SUB_train.isnull().any()]\n",
    "        SUB_train.dropna(inplace=True)\n",
    "        SUB_train.drop_duplicates(inplace=True)\n",
    "        \n",
    "        times_train = SUB_train['Time']\n",
    "        list_DROP = ['Infrared 1',\n",
    "                     'Infrared 2',\n",
    "                     'Infrared 3',\n",
    "                     'Infrared 4',\n",
    "                     'Infrared 5',\n",
    "                     'Infrared 6']\n",
    "        SUB_train.drop(list_DROP, axis=1, inplace=True)\n",
    "        SUB_train.drop(NA_cols, axis=1, inplace=True)  # drop NAN COLS\n",
    "\n",
    "        SUB_train.set_index('Time', inplace=True)\n",
    "        SUB_train.head()\n",
    "\n",
    "        # --- Load TRAINING image data from both cameras ---\n",
    "        cam = '1'\n",
    "        image_train = './dataset/Sensor + Image' + '/' + '{}_image_1_train.npy'.format(sub_)\n",
    "        name_train = './dataset/Sensor + Image' + '/' + '{}_name_1_train.npy'.format(sub_)\n",
    "        label_train = './dataset/Sensor + Image' + '/' + '{}_label_1_train.npy'.format(sub_)\n",
    "\n",
    "        img_1_train = np.load(image_train)\n",
    "        label_1_train = np.load(label_train)\n",
    "        name_1_train = np.load(name_train)\n",
    "\n",
    "        cam = '2'\n",
    "        image_train = './dataset/Sensor + Image' + '/' + '{}_image_2_train.npy'.format(sub_)\n",
    "        name_train = './dataset/Sensor + Image' + '/' + '{}_name_2_train.npy'.format(sub_)\n",
    "        label_train = './dataset/Sensor + Image' + '/' + '{}_label_2_train.npy'.format(sub_)\n",
    "\n",
    "        img_2_train = np.load(image_train)\n",
    "        label_2_train = np.load(label_train)\n",
    "        name_2_train = np.load(name_train)\n",
    "\n",
    "        # --- Align the training data by removing samples not present in the cleaned CSV ---\n",
    "        redundant_1 = list(set(name_1_train) - set(times_train))\n",
    "        redundant_2 = list(set(name_2_train) - set(times_train))\n",
    "        \n",
    "        ind = np.arange(0, len(img_1_train))\n",
    "\n",
    "        red_in1 = ind[np.isin(name_1_train, redundant_1)]\n",
    "        name_1_train = np.delete(name_1_train, red_in1)\n",
    "        img_1_train = np.delete(img_1_train, red_in1, axis=0)\n",
    "        label_1_train = np.delete(label_1_train, red_in1)\n",
    "\n",
    "        red_in2 = ind[np.isin(name_2_train, redundant_2)]\n",
    "        name_2_train = np.delete(name_2_train, red_in2)\n",
    "        img_2_train = np.delete(img_2_train, red_in2, axis=0)\n",
    "        label_2_train = np.delete(label_2_train, red_in2)\n",
    "        \n",
    "        # --- Prepare the final aligned training data ---\n",
    "        data_train = SUB_train.loc[name_1_train].values\n",
    "\n",
    "        set_seed()\n",
    "        X_csv_train, y_csv_train = data_train[:, :-1], data_train[:, -1]\n",
    "        \n",
    "        # Remap label 20 to 0 for consistency\n",
    "        y_csv_train = np.where(y_csv_train == 20, 0, y_csv_train)\n",
    "        label_1_train = np.where(label_1_train == 20, 0, label_1_train)\n",
    "        label_2_train = np.where(label_2_train == 20, 0, label_2_train)\n",
    "\n",
    "        # One-hot encode the labels for PyTorch\n",
    "        Y_csv_train = torch.nn.functional.one_hot(torch.from_numpy(y_csv_train).long(), 12).float()\n",
    "        Y_train_1 = torch.nn.functional.one_hot(torch.from_numpy(y_train_1).long(), 12).float()\n",
    "        Y_train_2 = torch.nn.functional.one_hot(torch.from_numpy(y_train_2).long(), 12).float()\n",
    "\n",
    "        # Scale the sensor data\n",
    "        X_csv_train_scaled = scaled_data(X_csv_train)\n",
    "\n",
    "        X_train_1 = img_1_train\n",
    "        y_train_1 = label_1_train\n",
    "        \n",
    "        X_train_2 = img_2_train\n",
    "        y_train_2 = label_2_train\n",
    "\n",
    "        # Reshape images to (samples, height, width, channels)\n",
    "        shape1, shape2 = 32, 32\n",
    "        X_train_1 = X_train_1.reshape(X_train_1.shape[0], shape1, shape2, 1)\n",
    "        X_train_2 = X_train_2.reshape(X_train_2.shape[0], shape1, shape2, 1)\n",
    "\n",
    "        # Scale image pixel values to be between 0 and 1\n",
    "        X_train_1_scaled = X_train_1 / 255.0\n",
    "        X_train_2_scaled = X_train_2 / 255.0\n",
    "\n",
    "        # --- Load and clean TEST sensor data (CSV) ---\n",
    "        SUB_test = pd.read_csv('./dataset/Sensor + Image/{}_sensor_test.csv'.format(sub_), skiprows=1)\n",
    "        SUB_test.head()\n",
    "        \n",
    "        SUB_test.isnull().sum()\n",
    "        NA_cols = SUB_test.columns[SUB_test.isnull().any()]\n",
    "        SUB_test.dropna(inplace=True)\n",
    "        SUB_test.drop_duplicates(inplace=True)\n",
    "\n",
    "        times_test = SUB_test['Time']\n",
    "        SUB_test.drop(list_DROP, axis=1, inplace=True)\n",
    "        SUB_test.drop(NA_cols, axis=1, inplace=True)\n",
    "\n",
    "        SUB_test.set_index('Time', inplace=True)\n",
    "        SUB_test.head()\n",
    "\n",
    "        # --- Load TEST image data from both cameras ---\n",
    "        image_test = './dataset/Sensor + Image' + '/' + '{}_image_1_test.npy'.format(sub_)\n",
    "        name_test = './dataset/Sensor + Image' + '/' + '{}_name_1_test.npy'.format(sub_)\n",
    "        label_test = './dataset/Sensor + Image' + '/' + '{}_label_1_test.npy'.format(sub_)\n",
    "        img_1_test = np.load(image_test)\n",
    "        label_1_test = np.load(label_test)\n",
    "        name_1_test = np.load(name_test)\n",
    "\n",
    "        image_test = './dataset/Sensor + Image' + '/' + '{}_image_2_test.npy'.format(sub_)\n",
    "        name_test = './dataset/Sensor + Image' + '/' + '{}_name_2_test.npy'.format(sub_)\n",
    "        label_test = './dataset/Sensor + Image' + '/' + '{}_label_2_test.npy'.format(sub_)\n",
    "        img_2_test = np.load(image_test)\n",
    "        label_2_test = np.load(label_test)\n",
    "        name_2_test = np.load(name_test)\n",
    "\n",
    "        # --- Align the test data ---\n",
    "        redundant_1 = list(set(name_1_test) - set(times_test))\n",
    "        redundant_2 = list(set(name_2_test) - set(times_test))\n",
    "        \n",
    "        ind = np.arange(0, len(img_1_test))\n",
    "\n",
    "        red_in1 = ind[np.isin(name_1_test, redundant_1)]\n",
    "        name_1_test = np.delete(name_1_test, red_in1)\n",
    "        img_1_test = np.delete(img_1_test, red_in1, axis=0)\n",
    "        label_1_test = np.delete(label_1_test, red_in1)\n",
    "\n",
    "        red_in2 = ind[np.isin(name_2_test, redundant_2)]\n",
    "        name_2_test = np.delete(name_2_test, red_in2)\n",
    "        img_2_test = np.delete(img_2_test, red_in2, axis=0)\n",
    "        label_2_test = np.delete(label_2_test, red_in2)\n",
    "\n",
    "        # --- Prepare the final aligned test data ---\n",
    "        data_test = SUB_test.loc[name_1_test].values\n",
    "\n",
    "        set_seed()\n",
    "        X_csv_test, y_csv_test = data_test[:, :-1], data_test[:, -1]\n",
    "        y_csv_test = np.where(y_csv_test == 20, 0, y_csv_test)\n",
    "        label_1_test = np.where(label_1_test == 20, 0, label_1_test)\n",
    "        label_2_test = np.where(label_2_test == 20, 0, label_2_test)\n",
    "\n",
    "        Y_csv_test = torch.nn.functional.one_hot(torch.from_numpy(y_csv_test).long(), 12).float()\n",
    "        X_csv_test_scaled = scaled_data(X_csv_test)\n",
    "        \n",
    "        X_test_1 = img_1_test\n",
    "        y_test_1 = label_1_test\n",
    "        Y_test_1 = torch.nn.functional.one_hot(torch.from_numpy(y_test_1).long(), 12).float()\n",
    "\n",
    "        X_test_2 = img_2_test\n",
    "        y_test_2 = label_2_test\n",
    "        Y_test_2 = torch.nn.functional.one_hot(torch.from_numpy(y_test_2).long(), 12).float()\n",
    "\n",
    "        X_test_1 = X_test_1.reshape(X_test_1.shape[0], shape1, shape2, 1)\n",
    "        X_test_2 = X_test_2.reshape(X_test_2.shape[0], shape1, shape2, 1)\n",
    "\n",
    "        X_test_1_scaled = X_test_1 / 255.0\n",
    "        X_test_2_scaled = X_test_2 / 255.0\n",
    "\n",
    "        # --- Store all processed data for the current client ---\n",
    "        X_train_csv_scaled_splits[clint_index] = X_csv_train_scaled\n",
    "        X_test_csv_scaled_splits[clint_index] = X_csv_test_scaled\n",
    "        Y_train_csv_splits[clint_index] = Y_csv_train\n",
    "        Y_test_csv_splits[clint_index] = Y_csv_test\n",
    "        X_train_1_scaled_splits[clint_index] = X_train_1_scaled\n",
    "        X_test_1_scaled_splits[clint_index] = X_test_1_scaled\n",
    "        Y_train_1_splits[clint_index] = Y_train_1\n",
    "        Y_test_1_splits[clint_index] = Y_test_1\n",
    "        X_train_2_scaled_splits[clint_index] = X_train_2_scaled # This line had a bug in the original code\n",
    "        X_test_2_scaled_splits[clint_index] = X_test_2_scaled\n",
    "        Y_train_2_splits[clint_index] = Y_train_2\n",
    "        Y_test_2_splits[clint_index] = Y_test_2\n",
    "        clint_index += 1\n",
    "        \n",
    "    # --- After loop, return all dictionaries ---\n",
    "    return X_train_csv_scaled_splits,X_test_csv_scaled_splits, Y_train_csv_splits,Y_test_csv_splits,X_train_1_scaled_splits,X_test_1_scaled_splits,Y_train_1_splits,Y_test_1_splits,X_train_2_scaled_splits,X_test_2_scaled_splits,Y_train_2_splits,Y_test_2_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e1b50",
   "metadata": {},
   "source": [
    "## Step 2: Client Selection\n",
    "\n",
    "We're making great progress. We've handled all the data loading and preparation. Now, we'll add the functions that form the \"intelligence\" of our federated learning system: client selection.\n",
    "\n",
    "Instead of blindly averaging updates from every client in every round, these methods evaluate each client's performance and contribution. This allows the server to select the most promising or reliable clients to participate in the global model update, potentially leading to faster convergence and a more robust final model.\n",
    "\n",
    "We'll add a series of functions, each calculating a specific metric to judge the clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5173b0f8",
   "metadata": {},
   "source": [
    "### Client Evaluation Metrics\n",
    "Add all the following functions to your script. Each one calculates a different score based on a client's performance.\n",
    "\n",
    "1. Relative Loss Reduction (RF_loss)\n",
    "\n",
    "This measures how much a client's training loss has dropped from the beginning to the end of a local training round, relative to the client with the biggest drop. A higher score means the client is learning effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca768194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_loss_reduction_as_list(client_losses):\n",
    "    \"\"\"\n",
    "    Calculates the relative loss reduction (RF_loss) for each client.\n",
    "    \"\"\"\n",
    "    loss_reduction = {}\n",
    "    for client_id, losses in client_losses.items():\n",
    "        if len(losses) < 2:\n",
    "            raise ValueError(f\"Client {client_id} has less than 2 loss values, cannot calculate RF_loss.\")\n",
    "        loss_start = losses[0]\n",
    "        loss_end = losses[-1]\n",
    "        loss_reduction[client_id] = loss_start - loss_end\n",
    "\n",
    "    max_loss_reduction = max(loss_reduction.values())\n",
    "    if max_loss_reduction == 0:\n",
    "        return [0.0] * len(loss_reduction)  # If no loss reduction, return 0.0 for all clients\n",
    "\n",
    "    rf_losses_list = [\n",
    "        reduction / max_loss_reduction for reduction in loss_reduction.values()\n",
    "    ]\n",
    "    return rf_losses_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f48d0a",
   "metadata": {},
   "source": [
    "2. Relative Training Accuracy (RF_ACC_Train)\n",
    "\n",
    "This measures a client's local training accuracy relative to the client with the highest accuracy. It's a straightforward measure of performance on local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a82d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_train_accuracy(client_acc):\n",
    "    \"\"\"\n",
    "    Calculates the relative training accuracy (RF_Acc_Train) for each client.\n",
    "    \"\"\"\n",
    "    max_acc = max(client_acc.values())\n",
    "    if max_acc == 0:\n",
    "        return [0.0] * len(client_acc)  # If no accuracy, return 0.0 for all clients\n",
    "\n",
    "    rf_accs_train_list = [\n",
    "        acc / max_acc for acc in client_acc.values()\n",
    "    ]\n",
    "    return rf_accs_train_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e977c2",
   "metadata": {},
   "source": [
    "3. Global Validation Accuracy (RF_ACC_Global)\n",
    "\n",
    "This is a more sophisticated metric. It rewards clients for high accuracy on a global test set but penalizes them if their global accuracy is much worse than their local training accuracy (which is a sign of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3b4108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_global_validation_accuracy(train_acc, global_acc):\n",
    "    \"\"\"\n",
    "    Calculates the global validation accuracy (RF_Acc_Global) based on local training accuracies.\n",
    "    \"\"\"\n",
    "    if set(train_acc.keys()) != set(global_acc.keys()):\n",
    "        raise ValueError(\"Client IDs for train and global accuracy do not match.\")\n",
    "\n",
    "    max_global_acc = max(global_acc.values())\n",
    "    if max_global_acc == 0:\n",
    "        max_global_acc = 1  # Avoid division by zero\n",
    "\n",
    "    global_train_diff = {\n",
    "        client_id: train_acc[client_id] - global_acc[client_id]\n",
    "        for client_id in train_acc\n",
    "    }\n",
    "    max_global_train_diff = max(global_train_diff.values())\n",
    "    if max_global_train_diff == 0:\n",
    "        max_global_train_diff = 1  # Avoid division by zero\n",
    "\n",
    "    rf_acc_global_list = [\n",
    "        (global_acc[client_id] / max_global_acc) - (global_train_diff[client_id] / max_global_train_diff)\n",
    "        for client_id in train_acc\n",
    "    ]\n",
    "    return rf_acc_global_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26f9bf",
   "metadata": {},
   "source": [
    "4. Loss Outliers (P_loss)\n",
    "\n",
    "This function flags clients that are potential negative contributors. If a client's final training loss is significantly higher than the average loss of all clients, it gets a high penalty score. Otherwise, its penalty is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35c66dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def claculate_loss_outliers(client_losses, lambda_loss=1.5):\n",
    "    \"\"\"\n",
    "    Calculates the loss outlier penalty (P_loss) for each client.\n",
    "    \"\"\"\n",
    "    final_losses = {client_id: losses[-1] for client_id, losses in client_losses.items()}\n",
    "    loss_values = np.array(list(final_losses.values()))\n",
    "\n",
    "    mean_loss = np.mean(loss_values)\n",
    "    std_loss = np.std(loss_values)\n",
    "\n",
    "    threshold = mean_loss + lambda_loss * std_loss\n",
    "\n",
    "    max_loss = np.max(loss_values)\n",
    "\n",
    "    if max_loss == 0:\n",
    "        return [0.0] * len(loss_values)\n",
    "\n",
    "    # Identify outliers\n",
    "    loss_outliers = [\n",
    "        final_loss / max_loss if final_loss > threshold else 0.0\n",
    "        for final_loss in loss_values\n",
    "    ]\n",
    "    return loss_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96a2ac",
   "metadata": {},
   "source": [
    "5. Performance Bias (P_bias)\n",
    "\n",
    "This metric calculates the gap between a client's performance on its own validation data versus its performance on the global validation data. A large gap might indicate that the client's local data is not representative of the overall data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc704c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_bias(val_acc, global_acc):\n",
    "    \"\"\"\n",
    "    Calculates the performance bias penalty (P_bias).\n",
    "    \"\"\"\n",
    "    if set(val_acc.keys()) != set(global_acc.keys()):\n",
    "        raise ValueError(\"Client IDs for validation and global accuracy do not match.\")\n",
    "\n",
    "    performance_bias_list = []\n",
    "    for client_id in val_acc:\n",
    "        val = val_acc[client_id]\n",
    "        global_val = global_acc[client_id]\n",
    "        max_val = max(val, global_val)\n",
    "\n",
    "        if max_val == 0:\n",
    "            performance_bias = 0\n",
    "        else:\n",
    "            performance_bias = abs(val - global_val) / max_val\n",
    "        performance_bias_list.append(performance_bias)\n",
    "\n",
    "    return performance_bias_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfd596",
   "metadata": {},
   "source": [
    "Excellent. Now that we have the functions to score each client, we need the final step: the algorithms that use these scores to select which clients will participate in a given round."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cecd1",
   "metadata": {},
   "source": [
    "### Client Selection Algorithms\n",
    "1. Pareto Optimization\n",
    "\n",
    "This is a powerful technique used when you have multiple, often conflicting, objectives. Instead of combining all metrics into one score, it tries to find a set of clients that represent the best possible trade-offs.\n",
    "\n",
    "A client is considered \"Pareto optimal\" if no other client is better than it across all metrics. The algorithm first finds this set of optimal clients.\n",
    "\n",
    "If there are more optimal clients than needed, it selects a random subset.\n",
    "\n",
    "If there are fewer, it fills the remaining spots by picking the clients with the best-combined performance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25f90e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_optimization(\n",
    "    rf_loss, rf_acc_train, rf_acc_val, rf_acc_global, p_loss, p_bias, client_num\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements Pareto optimization to select clients.\n",
    "    \"\"\"\n",
    "    # Combine all metrics into a single numpy array for easier processing.\n",
    "    # Note: penalty scores (p_loss, p_bias) are negated because we want to maximize all metrics.\n",
    "    rf_loss = np.array(list(rf_loss))\n",
    "    rf_acc_train = np.array(rf_acc_train)\n",
    "    rf_acc_val = np.array(rf_acc_val)\n",
    "    rf_acc_global = np.array(rf_acc_global)\n",
    "    p_loss = np.array(p_loss)\n",
    "    p_bias = np.array(p_bias)\n",
    "    data = np.array([rf_loss, rf_acc_train, rf_acc_val, rf_acc_global, -p_loss, -p_bias]).T\n",
    "\n",
    "    # --- Pareto Front Selection ---\n",
    "    def is_dominated(point, others):\n",
    "        \"\"\"Checks if a point is dominated by any other point in the set.\"\"\"\n",
    "        return any(np.all(other >= point) and np.any(other > point) for other in others)\n",
    "\n",
    "    pareto_indices = [\n",
    "        i for i, point in enumerate(data)\n",
    "        if not is_dominated(point, np.delete(data, i, axis=0))\n",
    "    ]\n",
    "    pareto_clients = pareto_indices\n",
    "\n",
    "    # --- Handle cases where the front is too large or too small ---\n",
    "    if len(pareto_clients) > client_num:\n",
    "        return random.sample(pareto_clients, client_num)\n",
    "\n",
    "    # If the front is smaller than needed, fill remaining slots based on a combined score.\n",
    "    remaining_slots = client_num - len(pareto_clients)\n",
    "    # This score is a simple weighted sum, emphasizing loss reduction and global accuracy.\n",
    "    pareto_scores = [0.4 * rf_loss[i] + 0.6 * rf_acc_global[i] for i in range(len(rf_loss))]\n",
    "    sorted_indices = np.argsort(pareto_scores)[::-1]  # Sort clients by score in descending order.\n",
    "\n",
    "    selected_clients = set(pareto_clients)\n",
    "    for i in sorted_indices:\n",
    "        if len(selected_clients) >= client_num:\n",
    "            break\n",
    "        if i not in selected_clients:\n",
    "            selected_clients.add(i)\n",
    "\n",
    "    return list(selected_clients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebf65c",
   "metadata": {},
   "source": [
    "2. Weighted Sum Method (5RF)\n",
    "\n",
    "This is a more straightforward approach. It calculates a single comprehensive score for each client by taking a weighted sum of all the metrics. Clients with the highest final scores are selected. The weights (0.2, 0.1, 0.3, etc.) determine the importance of each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb04b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_clients_with5RF(rf_loss, rf_acc_train, rf_acc_val, rf_acc_global, p_loss, p_bias, client_num):\n",
    "    rf_loss = np.array(list(rf_loss))\n",
    "    rf_acc_train = np.array(rf_acc_train)\n",
    "    rf_acc_val = np.array(rf_acc_val)\n",
    "    rf_acc_global = np.array(rf_acc_global)\n",
    "    p_loss = np.array(p_loss)\n",
    "    p_bias = np.array(p_bias)\n",
    "\n",
    "    # Calculate a single weighted score for each client\n",
    "    scores = (\n",
    "            0.2 * rf_loss +\n",
    "            0.1 * rf_acc_train +\n",
    "            0.2 * rf_acc_val +\n",
    "            0.3 * rf_acc_global -\n",
    "            0.1 * p_loss -\n",
    "            0.1 * p_bias\n",
    "    )\n",
    "    origin_scores = scores\n",
    "    # Get the indices of the clients with the highest scores\n",
    "    top_client_ids = np.argsort(scores)[::-1][:client_num]  # Sort descending and take the top N\n",
    "    return top_client_ids.tolist(), origin_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee125dce",
   "metadata": {},
   "source": [
    "## Step 2: The AI's Brain (The Model Definition)\n",
    "We have the data pipeline and the client selection logic. Now it's time to build the brain of the operation: the neural network model itself.\n",
    "\n",
    "The model, ModelCSVIMG, is a multi-modal neural network. This means it's designed to accept and process multiple types of data at once. It has three distinct input branches:\n",
    "\n",
    "One for the numerical sensor (CSV) data.\n",
    "\n",
    "One for the images from camera 1.\n",
    "\n",
    "One for the images from camera 2.\n",
    "\n",
    "The features extracted from each branch are then combined (fused) and passed to a final set of layers that perform the classification. The original code contains a few versions of the architecture; we will use the final, most complex one.\n",
    "\n",
    "Add the complete model class to your script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "218bbd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCSVIMG(nn.Module):\n",
    "    def __init__(self, num_csv_features, img_shape1, img_shape2):\n",
    "        super(ModelCSVIMG, self).__init__()\n",
    "\n",
    "        # --- Branch 1: For processing numerical CSV data ---\n",
    "        self.csv_fc_1 = nn.Linear(num_csv_features, 2000)\n",
    "        self.csv_bn_1 = nn.BatchNorm1d(2000)\n",
    "        self.csv_fc_2 = nn.Linear(2000, 600)\n",
    "        self.csv_bn_2 = nn.BatchNorm1d(600)\n",
    "        self.csv_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # --- Branch 2: For processing images from Camera 1 (CNN) ---\n",
    "        self.img1_conv_1 = nn.Conv2d(in_channels=1, out_channels=18, kernel_size=3, stride=1, padding=1)\n",
    "        self.img1_batch_norm = nn.BatchNorm2d(18)\n",
    "        self.img1_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Flattened features from the CNN go into a fully connected layer\n",
    "        self.img1_fc1 = nn.Linear(18 * 16 * 16, 100)\n",
    "        self.img1_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # --- Branch 3: For processing images from Camera 2 (identical to Branch 2) ---\n",
    "        self.img2_conv = nn.Conv2d(in_channels=1, out_channels=18, kernel_size=3, stride=1, padding=1)\n",
    "        self.img2_batch_norm = nn.BatchNorm2d(18)\n",
    "        self.img2_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.img2_fc1 = nn.Linear(18 * 16 * 16, 100)\n",
    "        self.img2_dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # --- Fusion and Final Classification Layers ---\n",
    "        # The input size is 600 (from CSV) + 100 (from Image 1) + 100 (from Image 2) = 800\n",
    "        self.fc1 = nn.Linear(800, 1200)\n",
    "        self.dr1 = nn.Dropout(0.2)\n",
    "        # A residual connection is used here: input to fc2 is the original 800 + output of fc1 (1200) = 2000\n",
    "        self.fc2 = nn.Linear(2000, 12) # 12 output classes\n",
    "\n",
    "    def forward(self, x_csv, x_img1, x_img2):\n",
    "        # --- Process CSV data ---\n",
    "        x_csv = F.relu(self.csv_bn_1(self.csv_fc_1(x_csv)))\n",
    "        x_csv = F.relu(self.csv_bn_2(self.csv_fc_2(x_csv)))\n",
    "        x_csv = self.csv_dropout(x_csv)\n",
    "\n",
    "        # --- Process Image 1 data ---\n",
    "        # Reshape image from (batch, height, width, channels) to (batch, channels, height, width)\n",
    "        x_img1 = x_img1.permute(0, 3, 1, 2)\n",
    "        x_img1 = F.relu(self.img1_conv_1(x_img1))\n",
    "        x_img1 = self.img1_batch_norm(x_img1)\n",
    "        x_img1 = self.img1_pool(x_img1)\n",
    "        x_img1 = x_img1.contiguous().view(x_img1.size(0), -1) # Flatten\n",
    "        x_img1 = F.relu(self.img1_fc1(x_img1))\n",
    "        x_img1 = self.img1_dropout(x_img1)\n",
    "\n",
    "        # --- Process Image 2 data ---\n",
    "        x_img2 = x_img2.permute(0, 3, 1, 2)\n",
    "        x_img2 = F.relu(self.img2_conv(x_img2))\n",
    "        x_img2 = self.img2_batch_norm(x_img2)\n",
    "        x_img2 = self.img2_pool(x_img2)\n",
    "        x_img2 = x_img2.contiguous().view(x_img2.size(0), -1) # Flatten\n",
    "        x_img2 = F.relu(self.img2_fc1(x_img2))\n",
    "        x_img2 = self.img2_dropout(x_img2)\n",
    "\n",
    "        # --- Fusion ---\n",
    "        x = torch.cat((x_csv, x_img1, x_img2), dim=1)\n",
    "        residual = x # Keep a copy for the residual connection\n",
    "        \n",
    "        # --- Final layers ---\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dr1(x)\n",
    "        # Concatenate the residual connection\n",
    "        x = torch.cat((residual, x), dim=1)\n",
    "        # Final output with softmax for classification\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f10c36",
   "metadata": {},
   "source": [
    "## Step 3: The Teacher (The Server Class)\n",
    "Alright, we're on the home stretch. We have the data, the selection logic, and the model. Now we need to create the actors for our simulation: the Server and the Client. These two classes will control the entire federated learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e0220",
   "metadata": {},
   "source": [
    "1. The Server Class\n",
    "\n",
    "The Server is the central coordinator. Its job is to:\n",
    "\n",
    "Hold the main global model.\n",
    "\n",
    "Send the global model to the clients.\n",
    "\n",
    "Receive updates from the selected clients.\n",
    "\n",
    "Aggregate these updates to improve the global model.\n",
    "\n",
    "Evaluate the global model's performance on a held-out test set.\n",
    "\n",
    "Here is the code for the Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7c9959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server\n",
    "class Server(object):\n",
    "    def __init__(self, model, epoch_size, eval_dataset, num_clients):\n",
    "        self.global_model = model\n",
    "        self.epoch_size = epoch_size\n",
    "        self.num_clients = num_clients\n",
    "        self.serverTestDataSet = CostumDataset(eval_dataset[0],eval_dataset[1],eval_dataset[2],eval_dataset[3])\n",
    "        self.eval_loader = torch.utils.data.DataLoader(self.serverTestDataSet, batch_size=epoch_size)\n",
    "\n",
    "    def model_aggregate(self, weight_accumulator):\n",
    "        # Averages the weights from the selected clients and updates the global model\n",
    "        for name, data in self.global_model.state_dict().items():\n",
    "            update_per_layer = weight_accumulator[name] * (1 / self.num_clients)  # FedAvg\n",
    "            \n",
    "            # Ensure data types match before adding\n",
    "            if data.type() != update_per_layer.type():\n",
    "                data.add_(update_per_layer.to(data.type()))\n",
    "            else:\n",
    "                data.add_(update_per_layer)\n",
    "\n",
    "    def model_eval(self):\n",
    "        # Evaluates the global model on the server's test data\n",
    "        self.global_model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        dataset_size = 0\n",
    "        with torch.no_grad(): # Disable gradient calculation for evaluation\n",
    "            for batch_id, batch in enumerate(self.eval_loader):\n",
    "                data1, data2, data3, target = batch\n",
    "                dataset_size += data1.size()[0]\n",
    "\n",
    "                data1 = data1.to(device).float()\n",
    "                data2 = data2.to(device).float()\n",
    "                data3 = data3.to(device).float()\n",
    "                target = target.to(device).float()\n",
    "                \n",
    "                output = self.global_model(data1,data2,data3)\n",
    "                total_loss += nn.functional.cross_entropy(output, target, reduction='sum').item()\n",
    "\n",
    "                pred = output.detach().max(1)[1]\n",
    "                correct += pred.eq(target.detach().max(1)[1].view_as(pred)).cpu().sum().item()\n",
    "\n",
    "        acc = 100.0 * (float(correct) / float(dataset_size))\n",
    "        loss = total_loss / dataset_size\n",
    "        return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32410ea",
   "metadata": {},
   "source": [
    "2. The Client Class and Helper Functions\n",
    "\n",
    "The Client represents an individual participant. Its job is to:\n",
    "\n",
    "Receive the global model from the server.\n",
    "\n",
    "Train this model on its own local data for a few epochs.\n",
    "\n",
    "Calculate the change (the diff) between the original model and its newly trained model.\n",
    "\n",
    "Send this diff back to the server.\n",
    "\n",
    "The client's training process is handled by two helper functions: train_one_epoch and validate.\n",
    "\n",
    "Add the Client class and its two helper functions to your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client\n",
    "class Client(object):\n",
    "    def __init__(self, epoch_size, local_epoch_per_round, train_dataset,val_dataset, id = -1):\n",
    "        self.epoch_size = epoch_size\n",
    "        self.local_epoch_per_round = local_epoch_per_round\n",
    "        self.client_id = id\n",
    "        self.train_dataset = CustomDatasetRes(train_dataset[0],train_dataset[1],train_dataset[2],train_dataset[3])\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=epoch_size,shuffle=True)\n",
    "        self.eval_dataset = CustomDatasetRes(val_dataset[0], val_dataset[1], val_dataset[2], val_dataset[3])\n",
    "        self.eval_loader = torch.utils.data.DataLoader(self.eval_dataset, batch_size=epoch_size,shuffle=False)\n",
    "\n",
    "    def local_train(self, global_model):\n",
    "        # Create a local copy of the global model\n",
    "        model = ModelCSVIMG(self.train_dataset.features1.shape[1], 32, 32)\n",
    "        model = model.to(device)\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        losses = []\n",
    "        min_loss, max_loss = float('inf'), float('-inf')\n",
    "\n",
    "        for epoch in range(self.local_epoch_per_round):\n",
    "            # Train for one epoch\n",
    "            train_loss, train_acc = train_one_epoch(model, self.train_loader, criterion, optimizer)\n",
    "            \n",
    "            # Track min/max loss and store all epoch losses for evaluation\n",
    "            if train_loss > max_loss: max_loss = train_loss\n",
    "            if train_loss < min_loss: min_loss = train_loss\n",
    "            losses.append(train_loss)\n",
    "\n",
    "        # Validate the model on the local validation set after training\n",
    "        val_loss, val_acc = validate(model, self.eval_loader, criterion)\n",
    "        print(f\"Client {self.client_id} - Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Calculate the difference between the updated local model and the original global model\n",
    "        diff = dict()\n",
    "        for name, data in model.state_dict().items():\n",
    "            diff[name] = (data - global_model.state_dict()[name])\n",
    "            \n",
    "        return model, diff, val_acc, val_loss, min_loss, max_loss, losses, train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aec0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
